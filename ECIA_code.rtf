{\rtf1\ansi\ansicpg949\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red245\green245\blue245;\red131\green0\blue165;\red144\green1\blue18;
\red19\green85\blue52;\red15\green112\blue1;\red0\green0\blue255;\red86\green65\blue25;\red0\green0\blue109;
\red31\green99\blue128;}
{\*\expandedcolortbl;;\cssrgb\c96863\c96863\c96863;\cssrgb\c59216\c13725\c70588;\cssrgb\c63922\c8235\c8235;
\cssrgb\c6667\c40000\c26667;\cssrgb\c0\c50196\c0;\cssrgb\c0\c0\c100000;\cssrgb\c41569\c32157\c12941;\cssrgb\c0\c6275\c50196;
\cssrgb\c14510\c46275\c57647;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11480\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf0 \cb2 \expnd0\expndtw0\kerning0
  \cf3 import\cf0  numpy \cf3 as\cf0  np\cb1 \
\cb2   \cf3 import\cf0  matplotlib.pyplot \cf3 as\cf0  plt\cb1 \
\cb2   \cf3 import\cf0  seaborn \cf3 as\cf0  sns\cb1 \
\cb2   \cf3 import\cf0  pandas \cf3 as\cf0  pd\cb1 \
\cb2   \cf3 import\cf0  os\cb1 \
\cb2   \cf3 from\cf0  scipy \cf3 import\cf0  stats\cb1 \
\cb2   \cf3 import\cf0  pickle\cb1 \
\cb2   \cf3 from\cf0  tqdm \cf3 import\cf0  tqdm\cb1 \
\cb2   \cf3 import\cf0  warnings\cb1 \
\cb2   \cf3 from\cf0  collections \cf3 import\cf0  deque\cb1 \
\cb2   \cf3 import\cf0  random\cb1 \
\cb2   warnings.filterwarnings(\cf4 'ignore'\cf0 )\cb1 \
\cb2   \cf3 import\cf0  pandas \cf3 as\cf0  pd\cb1 \
\
\cf5 \cb2 3\cf0 \cb1 \
\cb2   \cf6 # Create result directories\cf0 \cb1 \
\cb2   os.makedirs(\cf4 "content/Results"\cf0 , exist_ok=\cf7 True\cf0 )\cb1 \
\cb2   os.makedirs(\cf4 "content/Results"\cf0 , exist_ok=\cf7 True\cf0 )\cb1 \
\
\cf7 \cb2 class\cf0  MetaAnalysisManager:\cb1 \
\cb2     \cf4 """Manages meta-analysis using Fibonacci sequence master seeds"""\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf6 # Fibonacci sequence (for meta-analysis)\cf0 \cb1 \
\cb2         \cf9 self\cf0 .SEEDS = [\cf5 34\cf0 , \cf5 55\cf0 , \cf5 89\cf0 , \cf5 144\cf0 , \cf5 233\cf0 , \cf5 377\cf0 , \cf5 610\cf0 , \cf5 987\cf0 , \cf5 1597\cf0 , \cf5 2584\cf0 , \cf5 4181\cf0 , \cf5 6765\cf0 ]\cb1 \
\cb2         \cf9 self\cf0 .N_RUNS_PER_SEED = \cf5 300\cf0 \cb1 \
\cb2         \cf9 self\cf0 .RANDOMSHIFT_SEEDS = [\cf5 34\cf0 , \cf5 55\cf0 , \cf5 89\cf0 ]\cb1 \
\cb2         \cf9 self\cf0 .RANDOMSHIFT_N_RUNS_PER_SEED = \cf5 300\cf0 \cb1 \
\cb2         \cf9 self\cf0 .TOTAL_EXPERIMENTS = \cf8 len\cf0 (\cf9 self\cf0 .SEEDS) * \cf9 self\cf0 .N_RUNS_PER_SEED\cb1 \
\
\
\cb2         \cf6 # Logging and tracking\cf0 \cb1 \
\cb2         \cf9 self\cf0 .seed_logs = \{\}\cb1 \
\cb2         \cf9 self\cf0 .meta_results = \{\}\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 get_seeds_and_runs\cf0 (\cf9 self\cf0 , \cf9 env_name\cf0 ):\cb1 \
\cb2         \cf4 """Get environment-specific seeds and runs"""\cf0 \cb1 \
\cb2         \cf3 return\cf0  \cf9 self\cf0 .SEEDS, \cf9 self\cf0 .N_RUNS_PER_SEED\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 generate_experiment_seeds\cf0 (\cf9 self\cf0 , \cf9 master_seed\cf0 , \cf9 env_name\cf0 =\cf7 None\cf0 ):\cb1 \
\cb2         \cf4 """Generate experiment seeds from a Fibonacci master seed"""\cf0 \cb1 \
\cb2         np.random.seed(master_seed)\cb1 \
\cb2         experiment_seeds = []\cb1 \
\
\cb2         n_runs = \cf9 self\cf0 .N_RUNS_PER_SEED  \cf6 # 300\cf0 \cb1 \
\
\cb2         \cf6 # Generate experiment seeds\cf0 \cb1 \
\cb2         \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (n_runs):\cb1 \
\cb2             seed = np.random.randint(\cf5 0\cf0 , \cf5 1000000\cf0 )  \cf6 # Wide range to avoid collisions\cf0 \cb1 \
\cb2             experiment_seeds.append(seed)\cb1 \
\
\cb2         \cf9 self\cf0 .seed_logs[master_seed] = \{\cb1 \
\cb2             \cf4 'first_10_seeds'\cf0 : experiment_seeds[:\cf5 10\cf0 ],\cb1 \
\cb2             \cf4 'total_seeds'\cf0 : \cf8 len\cf0 (experiment_seeds),\cb1 \
\cb2             \cf4 'seed_range'\cf0 : [\cf8 min\cf0 (experiment_seeds), \cf8 max\cf0 (experiment_seeds)]\cb1 \
\cb2         \}\cb1 \
\
\cb2         \cf3 return\cf0  experiment_seeds\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 save_configuration\cf0 (\cf9 self\cf0 , \cf9 filepath\cf0 =\cf4 "content/Results/config.txt"\cf0 ):\cb1 \
\cb2         \cf4 """Save complete meta-analysis configuration"""\cf0 \cb1 \
\cb2         os.makedirs(os.path.dirname(filepath), exist_ok=\cf7 True\cf0 )\cb1 \
\
\cb2         \cf3 with\cf0  \cf8 open\cf0 (filepath, \cf4 'w'\cf0 ) \cf3 as\cf0  f:\cb1 \
\cb2             f.write(\cf4 "Meta-Analysis Configuration\\n"\cf0 )\cb1 \
\cb2             f.write(\cf4 "="\cf0  * \cf5 50\cf0  + \cf4 "\\n\\n"\cf0 )\cb1 \
\
\cb2             \cf6 # Regular environments\cf0 \cb1 \
\cb2             f.write(\cf4 "REGULAR ENVIRONMENTS (EnvA, EnvB, EnvC):\\n"\cf0 )\cb1 \
\cb2             f.write(\cf7 f\cf4 "Master Seeds: \cf0 \{\cf9 self\cf0 .SEEDS\}\cf4 \\n"\cf0 )\cb1 \
\cb2             f.write(\cf7 f\cf4 "Runs per Master Seed: \cf0 \{\cf9 self\cf0 .N_RUNS_PER_SEED\}\cf4 \\n"\cf0 )\cb1 \
\cb2             f.write(\cf7 f\cf4 "Total Experiments: \cf0 \{\cf8 len\cf0 (\cf9 self\cf0 .SEEDS) * \cf9 self\cf0 .N_RUNS_PER_SEED\}\cf4 \\n\\n"\cf0 )\cb1 \
\
\cb2             \cf6 # RandomShift environment\cf0 \cb1 \
\cb2             f.write(\cf4 "CROSS-DATASET ENVIRONMENT (RandomShift):\\n"\cf0 )\cb1 \
\cb2             f.write(\cf7 f\cf4 "Master Seeds: \cf0 \{\cf9 self\cf0 .RANDOMSHIFT_SEEDS\}\cf4 \\n"\cf0 )\cb1 \
\cb2             f.write(\cf7 f\cf4 "Runs per Master Seed: \cf0 \{\cf9 self\cf0 .RANDOMSHIFT_N_RUNS_PER_SEED\}\cf4 \\n"\cf0 )\cb1 \
\cb2             f.write(\cf7 f\cf4 "Total Experiments: \cf0 \{\cf8 len\cf0 (\cf9 self\cf0 .RANDOMSHIFT_SEEDS) * \cf9 self\cf0 .RANDOMSHIFT_N_RUNS_PER_SEED\}\cf4 \\n\\n"\cf0 )\cb1 \
\
\cb2             f.write(\cf4 "Seed Generation Details:\\n"\cf0 )\cb1 \
\cb2             f.write(\cf4 "-"\cf0  * \cf5 30\cf0  + \cf4 "\\n"\cf0 )\cb1 \
\cb2             \cf3 for\cf0  master_seed, log \cf7 in\cf0  \cf9 self\cf0 .seed_logs.items():\cb1 \
\cb2                 f.write(\cf7 f\cf4 "Master Seed \cf0 \{master_seed\}\cf4 :\\n"\cf0 )\cb1 \
\cb2                 f.write(\cf7 f\cf4 "  First 10 experiment seeds: \cf0 \{log[\cf4 'first_10_seeds'\cf0 ]\}\cf4 \\n"\cf0 )\cb1 \
\cb2                 f.write(\cf7 f\cf4 "  Seed range: \cf0 \{log[\cf4 'seed_range'\cf0 ]\}\cf4 \\n"\cf0 )\cb1 \
\cb2                 f.write(\cf7 f\cf4 "  Total generated: \cf0 \{log[\cf4 'total_seeds'\cf0 ]\}\cf4 \\n\\n"\cf0 )\cb1 \
\
\
\cf6 \cb2 # Global manager instances\cf0 \cb1 \
\cb2 MANAGER = MetaAnalysisManager()\cb1 \
\
\cb2   \cf7 class\cf0  EnvironmentA:\cb1 \
\cb2       \cf4 """Environment A with deterministic seed control"""\cf0 \cb1 \
\
\cb2       \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 , \cf9 total_trials\cf0 =\cf5 200\cf0 , \cf9 sigma\cf0 =\cf5 0.15\cf0 , \cf9 random_state\cf0 =\cf7 None\cf0 ):\cb1 \
\cb2           \cf9 self\cf0 .total_trials = total_trials\cb1 \
\cb2           \cf9 self\cf0 .trial = \cf5 0\cf0 \cb1 \
\cb2           \cf9 self\cf0 .n_actions = \cf5 5\cf0 \cb1 \
\cb2           \cf9 self\cf0 .sigma = sigma\cb1 \
\cb2           \cf9 self\cf0 .context = \cf7 None\cf0 \cb1 \
\
\cb2           \cf6 # Set up reproducible random state\cf0 \cb1 \
\cb2           \cf3 if\cf0  random_state \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2               \cf9 self\cf0 .rng = np.random.RandomState(random_state)\cb1 \
\cb2           \cf3 else\cf0 :\cb1 \
\cb2               \cf9 self\cf0 .rng = np.random.RandomState()\cb1 \
\
\cb2       \cf7 def\cf0  \cf8 reset\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2           \cf9 self\cf0 .trial = \cf5 0\cf0 \cb1 \
\cb2           \cf9 self\cf0 .update_context()\cb1 \
\cb2           \cf3 return\cf0  \cf7 None\cf0 \cb1 \
\
\cb2       \cf7 def\cf0  \cf8 step\cf0 (\cf9 self\cf0 , \cf9 action\cf0 ):\cb1 \
\cb2           reward = \cf9 self\cf0 .get_reward(action)\cb1 \
\cb2           \cf9 self\cf0 .trial += \cf5 1\cf0 \cb1 \
\cb2           \cf9 self\cf0 .update_context()\cb1 \
\cb2           \cf3 return\cf0  reward, \cf9 self\cf0 .context\cb1 \
\
\cb2       \cf7 def\cf0  \cf8 get_reward\cf0 (\cf9 self\cf0 , \cf9 action\cf0 ):\cb1 \
\cb2           t = \cf9 self\cf0 .trial\cb1 \
\cb2           \cf3 if\cf0  t < \cf5 50\cf0 :\cb1 \
\cb2               means = [\cf5 0.8\cf0 , \cf5 0.2\cf0 , \cf5 0.2\cf0 , \cf5 0.2\cf0 , \cf5 0.2\cf0 ]\cb1 \
\cb2           \cf3 elif\cf0  t < \cf5 100\cf0 :\cb1 \
\cb2               means = [\cf5 0.8\cf0 , \cf5 0.2\cf0 , \cf5 0.3\cf0 , \cf5 0.2\cf0 , \cf5 0.2\cf0 ]\cb1 \
\cb2           \cf3 elif\cf0  t < \cf5 150\cf0 :\cb1 \
\cb2               means = [\cf5 0.2\cf0 , \cf5 0.2\cf0 , \cf5 0.3\cf0 , \cf5 0.2\cf0 , \cf5 0.9\cf0 ]\cb1 \
\cb2           \cf3 else\cf0 :\cb1 \
\cb2               means = [\cf5 0.2\cf0 , \cf5 0.4\cf0 , \cf5 0.3\cf0 , \cf5 0.2\cf0 , \cf5 0.9\cf0 ]\cb1 \
\
\cb2           \cf6 # Use controlled random state for noise\cf0 \cb1 \
\cb2           \cf3 return\cf0  \cf9 self\cf0 .rng.normal(means[action], \cf9 self\cf0 .sigma)\cb1 \
\
\cb2       \cf7 def\cf0  \cf8 update_context\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2           t = \cf9 self\cf0 .trial\cb1 \
\cb2           norm_t = t / \cf9 self\cf0 .total_trials\cb1 \
\cb2           \cf3 if\cf0  t < \cf5 50\cf0 :\cb1 \
\cb2               phase_id = \cf5 0\cf0 \cb1 \
\cb2           \cf3 elif\cf0  t < \cf5 100\cf0 :\cb1 \
\cb2               phase_id = \cf5 1\cf0 \cb1 \
\cb2           \cf3 elif\cf0  t < \cf5 150\cf0 :\cb1 \
\cb2               phase_id = \cf5 2\cf0 \cb1 \
\cb2           \cf3 else\cf0 :\cb1 \
\cb2               phase_id = \cf5 3\cf0 \cb1 \
\cb2           \cf9 self\cf0 .context = np.array([norm_t, phase_id])\cb1 \
\
\cb2   \cf7 class\cf0  EnvironmentB:\cb1 \
\cb2       \cf4 """Environment B with deterministic seed control"""\cf0 \cb1 \
\
\cb2       \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 , \cf9 total_trials\cf0 =\cf5 200\cf0 , \cf9 sigma\cf0 =\cf5 0.05\cf0 , \cf9 random_state\cf0 =\cf7 None\cf0 ):\cb1 \
\cb2           \cf9 self\cf0 .total_trials = total_trials\cb1 \
\cb2           \cf9 self\cf0 .trial = \cf5 0\cf0 \cb1 \
\cb2           \cf9 self\cf0 .n_actions = \cf5 5\cf0 \cb1 \
\cb2           \cf9 self\cf0 .sigma = sigma\cb1 \
\cb2           \cf9 self\cf0 .context = \cf7 None\cf0 \cb1 \
\
\cb2           \cf3 if\cf0  random_state \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2               \cf9 self\cf0 .rng = np.random.RandomState(random_state)\cb1 \
\cb2           \cf3 else\cf0 :\cb1 \
\cb2               \cf9 self\cf0 .rng = np.random.RandomState()\cb1 \
\
\
\cb2       \cf7 def\cf0  \cf8 reset\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2           \cf9 self\cf0 .trial = \cf5 0\cf0 \cb1 \
\cb2           \cf9 self\cf0 .update_context()\cb1 \
\cb2           \cf3 return\cf0  \cf7 None\cf0 \cb1 \
\
\cb2       \cf7 def\cf0  \cf8 step\cf0 (\cf9 self\cf0 , \cf9 action\cf0 ):\cb1 \
\cb2           reward = \cf9 self\cf0 .get_reward(action)\cb1 \
\cb2           \cf9 self\cf0 .trial += \cf5 1\cf0 \cb1 \
\cb2           \cf9 self\cf0 .update_context()\cb1 \
\cb2           \cf3 return\cf0  reward, \cf9 self\cf0 .context\cb1 \
\
\cb2       \cf7 def\cf0  \cf8 get_reward\cf0 (\cf9 self\cf0 , \cf9 action\cf0 ):\cb1 \
\cb2           t = \cf9 self\cf0 .trial\cb1 \
\cb2           base = [\cf5 0.2\cf0 ] * \cf5 5\cf0 \cb1 \
\cb2           phase = t // \cf5 40\cf0 \cb1 \
\
\cb2           \cf3 if\cf0  phase % \cf5 2\cf0  == \cf5 0\cf0 :\cb1 \
\cb2               base[\cf5 0\cf0 ] = \cf5 0.95\cf0 ; base[\cf5 4\cf0 ] = \cf5 0.01\cf0 \cb1 \
\cb2           \cf3 else\cf0 :\cb1 \
\cb2               base[\cf5 0\cf0 ] = \cf5 0.01\cf0 ; base[\cf5 4\cf0 ] = \cf5 0.95\cf0 \cb1 \
\
\cb2           \cf3 for\cf0  i \cf7 in\cf0  [\cf5 1\cf0 , \cf5 2\cf0 , \cf5 3\cf0 ]:\cb1 \
\cb2               base[i] = \cf5 0.05\cf0 \cb1 \
\
\cb2           reward = base[action] + \cf9 self\cf0 .rng.normal(\cf5 0\cf0 , \cf9 self\cf0 .sigma)\cb1 \
\cb2           \cf3 return\cf0  np.clip(reward, \cf5 0\cf0 , \cf5 1\cf0 )\cb1 \
\
\cb2       \cf7 def\cf0  \cf8 update_context\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2           t = \cf9 self\cf0 .trial\cb1 \
\cb2           norm_t = t / \cf9 self\cf0 .total_trials\cb1 \
\cb2           phase = t // \cf5 40\cf0 \cb1 \
\cb2           phase_id = phase % \cf5 2\cf0 \cb1 \
\cb2           \cf9 self\cf0 .context = np.array([norm_t, phase_id])\cb1 \
\
\
\cb2   \cf7 class\cf0  EnvironmentC:\cb1 \
\cb2       \cf4 """Environment C with controlled randomness while maintaining stochastic nature"""\cf0 \cb1 \
\cb2       \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 , \cf9 total_trials\cf0 =\cf5 200\cf0 , \cf9 sigma\cf0 =\cf5 0.15\cf0 ,\cb1 \
\cb2                   \cf9 disturbance_prob\cf0 =\cf5 0.05\cf0 , \cf9 disturbance_strength\cf0 =\cf5 0.7\cf0 ,\cb1 \
\cb2                   \cf9 random_state\cf0 =\cf7 None\cf0 ):\cb1 \
\cb2           \cf9 self\cf0 .total_trials = total_trials\cb1 \
\cb2           \cf9 self\cf0 .n_actions = \cf5 5\cf0 \cb1 \
\cb2           \cf9 self\cf0 .sigma = sigma\cb1 \
\cb2           \cf9 self\cf0 .trial = \cf5 0\cf0 \cb1 \
\cb2           \cf9 self\cf0 .disturbance_prob = disturbance_prob\cb1 \
\cb2           \cf9 self\cf0 .disturbance_strength = disturbance_strength\cb1 \
\
\cb2           \cf6 # Set up controlled random state\cf0 \cb1 \
\cb2           \cf3 if\cf0  random_state \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2               \cf9 self\cf0 .rng = np.random.RandomState(random_state)\cb1 \
\cb2           \cf3 else\cf0 :\cb1 \
\cb2               \cf9 self\cf0 .rng = np.random.RandomState()\cb1 \
\
\cb2           \cf6 # Generate random but reproducible configuration\cf0 \cb1 \
\cb2           \cf9 self\cf0 ._generate_random_configuration()\cb1 \
\
\cb2           \cf9 self\cf0 .active_disturbance = \cf7 False\cf0 \cb1 \
\cb2           \cf9 self\cf0 .disturbance_action = \cf7 None\cf0 \cb1 \
\cb2           \cf9 self\cf0 .disturbance_duration = \cf5 0\cf0 \cb1 \
\cb2           \cf9 self\cf0 .context = \cf7 None\cf0 \cb1 \
\cb2           \cf9 self\cf0 .update_reward_structure()\cb1 \
\
\cb2       \cf7 def\cf0  \cf8 _generate_random_configuration\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2           \cf4 """Generate random configuration using controlled randomness"""\cf0 \cb1 \
\cb2           \cf6 # Generate 3 random change points between 30-180 (but deterministic)\cf0 \cb1 \
\cb2           \cf9 self\cf0 .change_points = \cf8 sorted\cf0 (\cf9 self\cf0 .rng.choice(\cf8 range\cf0 (\cf5 30\cf0 , \cf5 181\cf0 ), size=\cf5 3\cf0 , replace=\cf7 False\cf0 ).tolist())\cb1 \
\
\cb2           \cf6 # Generate different optimal actions and rewards for each segment\cf0 \cb1 \
\cb2           \cf9 self\cf0 .optimal_actions = []\cb1 \
\cb2           \cf9 self\cf0 .optimal_rewards = []\cb1 \
\cb2           used_actions = \cf10 set\cf0 ()\cb1 \
\
\cb2           \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf5 4\cf0 ):  \cf6 # 4 segments total\cf0 \cb1 \
\cb2               \cf6 # Ensure different optimal action for each segment\cf0 \cb1 \
\cb2               \cf3 if\cf0  \cf8 len\cf0 (used_actions) < \cf9 self\cf0 .n_actions:\cb1 \
\cb2                   unused_actions = [a \cf3 for\cf0  a \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions) \cf3 if\cf0  a \cf7 not\cf0  \cf7 in\cf0  used_actions]\cb1 \
\cb2                   \cf3 if\cf0  unused_actions:\cb1 \
\cb2                       opt_action = \cf9 self\cf0 .rng.choice(unused_actions)\cb1 \
\cb2                   \cf3 else\cf0 :\cb1 \
\cb2                       prev_action = \cf9 self\cf0 .optimal_actions[\cf5 -1\cf0 ] \cf3 if\cf0  \cf9 self\cf0 .optimal_actions \cf3 else\cf0  \cf5 -1\cf0 \cb1 \
\cb2                       opt_action = (prev_action + \cf5 1\cf0 ) % \cf9 self\cf0 .n_actions\cb1 \
\cb2               \cf3 else\cf0 :\cb1 \
\cb2                   prev_action = \cf9 self\cf0 .optimal_actions[\cf5 -1\cf0 ] \cf3 if\cf0  \cf9 self\cf0 .optimal_actions \cf3 else\cf0  \cf5 -1\cf0 \cb1 \
\cb2                   available_actions = [a \cf3 for\cf0  a \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions) \cf3 if\cf0  a != prev_action]\cb1 \
\cb2                   opt_action = \cf9 self\cf0 .rng.choice(available_actions)\cb1 \
\
\cb2               used_actions.add(opt_action)\cb1 \
\cb2               \cf9 self\cf0 .optimal_actions.append(opt_action)\cb1 \
\
\cb2               \cf6 # Generate different optimal reward levels\cf0 \cb1 \
\cb2               opt_reward = \cf9 self\cf0 .rng.uniform(\cf5 0.75\cf0 , \cf5 0.9\cf0 )\cb1 \
\cb2               \cf9 self\cf0 .optimal_rewards.append(opt_reward)\cb1 \
\
\
\cb2       \cf7 def\cf0  \cf8 reset\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2           \cf9 self\cf0 .trial = \cf5 0\cf0 \cb1 \
\cb2           \cf9 self\cf0 .active_disturbance = \cf7 False\cf0 \cb1 \
\cb2           \cf9 self\cf0 .disturbance_action = \cf7 None\cf0 \cb1 \
\cb2           \cf9 self\cf0 .disturbance_duration = \cf5 0\cf0 \cb1 \
\cb2           \cf9 self\cf0 .update_reward_structure()\cb1 \
\cb2           \cf3 return\cf0  \cf7 None\cf0 \cb1 \
\
\cb2       \cf7 def\cf0  \cf8 step\cf0 (\cf9 self\cf0 , \cf9 action\cf0 ):\cb1 \
\cb2           reward = \cf9 self\cf0 .rng.normal(\cf9 self\cf0 .means[action], \cf9 self\cf0 .sigma)\cb1 \
\cb2           \cf9 self\cf0 .trial += \cf5 1\cf0 \cb1 \
\cb2           \cf9 self\cf0 .update_reward_structure()\cb1 \
\cb2           \cf3 return\cf0  reward, \cf9 self\cf0 .context\cb1 \
\
\cb2       \cf7 def\cf0  \cf8 update_reward_structure\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2           t = \cf9 self\cf0 .trial\cb1 \
\cb2           norm_t = t / \cf9 self\cf0 .total_trials\cb1 \
\
\cb2           \cf6 # Determine current segment\cf0 \cb1 \
\cb2           phase = \cf5 0\cf0 \cb1 \
\cb2           \cf3 for\cf0  i, cp \cf7 in\cf0  \cf8 enumerate\cf0 (\cf9 self\cf0 .change_points):\cb1 \
\cb2               \cf3 if\cf0  t >= cp:\cb1 \
\cb2                   phase = i + \cf5 1\cf0 \cb1 \
\cb2               \cf3 else\cf0 :\cb1 \
\cb2                   \cf3 break\cf0 \cb1 \
\
\cb2           optimal_action = \cf9 self\cf0 .optimal_actions[phase]\cb1 \
\cb2           optimal_reward = \cf9 self\cf0 .optimal_rewards[phase]\cb1 \
\
\cb2           \cf6 # Set basic reward structure\cf0 \cb1 \
\cb2           means = [\cf5 0.2\cf0 ] * \cf9 self\cf0 .n_actions\cb1 \
\cb2           means[optimal_action] = optimal_reward\cb1 \
\
\cb2           \cf6 # Handle disturbances with controlled randomness\cf0 \cb1 \
\cb2           \cf3 if\cf0  \cf9 self\cf0 .active_disturbance:\cb1 \
\cb2               \cf9 self\cf0 .disturbance_duration -= \cf5 1\cf0 \cb1 \
\cb2               \cf3 if\cf0  \cf9 self\cf0 .disturbance_duration > \cf5 0\cf0  \cf7 and\cf0  \cf9 self\cf0 .disturbance_action != optimal_action:\cb1 \
\cb2                   means[\cf9 self\cf0 .disturbance_action] = \cf9 self\cf0 .disturbance_strength\cb1 \
\cb2               \cf3 else\cf0 :\cb1 \
\cb2                   \cf9 self\cf0 .active_disturbance = \cf7 False\cf0 \cb1 \
\cb2                   \cf9 self\cf0 .disturbance_action = \cf7 None\cf0 \cb1 \
\
\cb2           \cf3 elif\cf0  \cf9 self\cf0 .rng.random() < \cf9 self\cf0 .disturbance_prob:\cb1 \
\cb2               non_optimal_actions = [a \cf3 for\cf0  a \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions) \cf3 if\cf0  a != optimal_action]\cb1 \
\cb2               \cf3 if\cf0  non_optimal_actions:\cb1 \
\cb2                   \cf9 self\cf0 .disturbance_action = \cf9 self\cf0 .rng.choice(non_optimal_actions)\cb1 \
\cb2                   \cf9 self\cf0 .active_disturbance = \cf7 True\cf0 \cb1 \
\cb2                   \cf9 self\cf0 .disturbance_duration = \cf9 self\cf0 .rng.randint(\cf5 1\cf0 , \cf5 3\cf0 )\cb1 \
\cb2                   means[\cf9 self\cf0 .disturbance_action] = \cf9 self\cf0 .disturbance_strength\cb1 \
\
\cb2           \cf9 self\cf0 .means = means\cb1 \
\cb2           \cf9 self\cf0 .context = np.array([norm_t, phase])\cb1 \
\
\
\
\cb2       \cf7 def\cf0  \cf8 get_current_optimal_info\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2           \cf4 """Get current segment's optimal action and reward"""\cf0 \cb1 \
\cb2           phase = \cf5 0\cf0 \cb1 \
\cb2           \cf3 for\cf0  i, cp \cf7 in\cf0  \cf8 enumerate\cf0 (\cf9 self\cf0 .change_points):\cb1 \
\cb2               \cf3 if\cf0  \cf9 self\cf0 .trial >= cp:\cb1 \
\cb2                   phase = i + \cf5 1\cf0 \cb1 \
\cb2               \cf3 else\cf0 :\cb1 \
\cb2                   \cf3 break\cf0 \cb1 \
\
\cb2           \cf3 return\cf0  \{\cb1 \
\cb2               \cf4 'phase'\cf0 : phase,\cb1 \
\cb2               \cf4 'optimal_action'\cf0 : \cf9 self\cf0 .optimal_actions[phase],\cb1 \
\cb2               \cf4 'optimal_reward'\cf0 : \cf9 self\cf0 .optimal_rewards[phase],\cb1 \
\cb2               \cf4 'change_point'\cf0 : \cf9 self\cf0 .change_points[phase\cf5 -1\cf0 ] \cf3 if\cf0  phase > \cf5 0\cf0  \cf3 else\cf0  \cf5 0\cf0 \cb1 \
\cb2           \}\cb1 \
\
\cf6 \cb2 # =====================\cf0 \cb1 \
\cf6 \cb2 # 1. RandomShiftEnvironment\cf0 \cb1 \
\cf6 \cb2 # =====================\cf0 \cb1 \
\
\cf7 \cb2 class\cf0  RandomShiftEnvironment:\cb1 \
\cb2     \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 , \cf9 total_trials\cf0 =\cf5 600\cf0 , \cf9 n_actions\cf0 =\cf5 5\cf0 , \cf9 random_state\cf0 =\cf7 None\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .total_trials = total_trials\cb1 \
\cb2         \cf9 self\cf0 .n_actions = n_actions\cb1 \
\cb2         \cf9 self\cf0 .trial = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .sigma = \cf5 0.12\cf0 \cb1 \
\
\cb2         \cf6 # Master seed for reproducibility\cf0 \cb1 \
\cb2         \cf3 if\cf0  random_state \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .master_rng = np.random.RandomState(random_state)\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .master_rng = np.random.RandomState()\cb1 \
\
\cb2         \cf6 # Enhanced features for ECIA\cf0 \cb1 \
\cb2         \cf9 self\cf0 .reward_history = deque(maxlen=\cf5 50\cf0 )  \cf6 # For pattern analysis\cf0 \cb1 \
\cb2         \cf9 self\cf0 .action_sequence = deque(maxlen=\cf5 10\cf0 )  \cf6 # For sequence rewards\cf0 \cb1 \
\cb2         \cf9 self\cf0 .extreme_event_counter = \cf5 0\cf0 \cb1 \
\
\cb2         \cf6 # Pattern storage for reuse (ECIA advantage)\cf0 \cb1 \
\cb2         \cf9 self\cf0 .stored_patterns = []  \cf6 # Store patterns from phase 1\cf0 \cb1 \
\
\cb2         \cf6 # Multi-layered temporal patterns\cf0 \cb1 \
\cb2         \cf9 self\cf0 .short_term_cycle = \cf5 0\cf0   \cf6 # 10-15 trials\cf0 \cb1 \
\cb2         \cf9 self\cf0 .medium_term_cycle = \cf5 0\cf0   \cf6 # 50-80 trials\cf0 \cb1 \
\cb2         \cf9 self\cf0 .long_term_phase = \cf5 0\cf0    \cf6 # 150+ trials\cf0 \cb1 \
\
\cb2         \cf6 # Generate enhanced segment schedule\cf0 \cb1 \
\cb2         \cf9 self\cf0 .segments = \cf9 self\cf0 ._generate_enhanced_segments()\cb1 \
\cb2         \cf9 self\cf0 .current_segment_idx = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .segment_start_trial = \cf5 0\cf0 \cb1 \
\
\cb2         \cf6 # Current state\cf0 \cb1 \
\cb2         \cf9 self\cf0 .current_rewards = [\cf5 0.2\cf0 ] * \cf9 self\cf0 .n_actions\cb1 \
\cb2         \cf9 self\cf0 .context = \cf7 None\cf0 \cb1 \
\cb2         \cf9 self\cf0 .base_noise_level = \cf5 0.12\cf0 \cb1 \
\
\cb2         \cf6 # Initialize first segment\cf0 \cb1 \
\cb2         \cf9 self\cf0 ._initialize_current_segment()\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _generate_enhanced_segments\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Generate segments with rich complexity patterns and pattern reuse"""\cf0 \cb1 \
\cb2         segments = []\cb1 \
\cb2         current_trial = \cf5 0\cf0 \cb1 \
\
\cb2         \cf6 # Long-term phases (3 phases of ~200 trials each)\cf0 \cb1 \
\cb2         phase_types = [\cf4 'learning'\cf0 , \cf4 'challenging'\cf0 , \cf4 'mastery'\cf0 ]\cb1 \
\cb2         phase_length = \cf9 self\cf0 .total_trials // \cf5 3\cf0 \cb1 \
\
\cb2         \cf3 for\cf0  phase_idx, phase_type \cf7 in\cf0  \cf8 enumerate\cf0 (phase_types):\cb1 \
\cb2             phase_start = phase_idx * phase_length\cb1 \
\cb2             phase_end = \cf8 min\cf0 ((phase_idx + \cf5 1\cf0 ) * phase_length, \cf9 self\cf0 .total_trials)\cb1 \
\
\cb2             \cf6 # Generate segments within each phase\cf0 \cb1 \
\cb2             phase_trials = phase_start\cb1 \
\cb2             \cf3 while\cf0  phase_trials < phase_end:\cb1 \
\cb2                 \cf6 # Segment length based on phase type\cf0 \cb1 \
\cb2                 \cf3 if\cf0  phase_type == \cf4 'learning'\cf0 :\cb1 \
\cb2                     segment_length = \cf9 self\cf0 .master_rng.randint(\cf5 80\cf0 , \cf5 120\cf0 )\cb1 \
\cb2                 \cf3 elif\cf0  phase_type == \cf4 'challenging'\cf0 :\cb1 \
\cb2                     segment_length = \cf9 self\cf0 .master_rng.randint(\cf5 60\cf0 , \cf5 100\cf0 )\cb1 \
\cb2                 \cf3 else\cf0 :  \cf6 # mastery\cf0 \cb1 \
\cb2                     segment_length = \cf9 self\cf0 .master_rng.randint(\cf5 40\cf0 , \cf5 80\cf0 )\cb1 \
\
\cb2                 segment_end = \cf8 min\cf0 (phase_trials + segment_length, phase_end)\cb1 \
\
\cb2                 segment = \{\cb1 \
\cb2                     \cf4 'start'\cf0 : phase_trials,\cb1 \
\cb2                     \cf4 'end'\cf0 : segment_end,\cb1 \
\cb2                     \cf4 'length'\cf0 : segment_end - phase_trials,\cb1 \
\cb2                     \cf4 'phase_type'\cf0 : phase_type,\cb1 \
\cb2                     \cf4 'phase_idx'\cf0 : phase_idx,\cb1 \
\cb2                     \cf4 'config'\cf0 : \cf9 self\cf0 ._generate_segment_config(phase_type, phase_idx, segment_end - phase_trials)\cb1 \
\cb2                 \}\cb1 \
\
\cb2                 segments.append(segment)\cb1 \
\cb2                 phase_trials = segment_end\cb1 \
\
\cb2         \cf3 return\cf0  segments\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _generate_segment_config\cf0 (\cf9 self\cf0 , \cf9 phase_type\cf0 , \cf9 phase_idx\cf0 , \cf9 segment_length\cf0 ):\cb1 \
\cb2         \cf4 """Generate enhanced segment configuration with pattern reuse"""\cf0 \cb1 \
\cb2         config = \{\cb1 \
\cb2             \cf4 'phase_type'\cf0 : phase_type,\cb1 \
\cb2             \cf4 'reward_structure'\cf0 : \cf4 'layered'\cf0 ,\cb1 \
\cb2             \cf4 'is_reused_pattern'\cf0 : \cf7 False\cf0 \cb1 \
\cb2         \}\cb1 \
\
\cb2         \cf6 # Pattern reuse logic (only after phase 1)\cf0 \cb1 \
\cb2         \cf3 if\cf0  phase_idx > \cf5 0\cf0  \cf7 and\cf0  \cf9 self\cf0 .stored_patterns \cf7 and\cf0  \cf9 self\cf0 .master_rng.random() < \cf5 0.25\cf0 :\cb1 \
\cb2             \cf6 # Reuse pattern from phase 1\cf0 \cb1 \
\cb2             base_pattern = \cf9 self\cf0 .master_rng.choice(\cf9 self\cf0 .stored_patterns)\cb1 \
\
\cb2             \cf6 # 60% identical, 40% similar variation\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf9 self\cf0 .master_rng.random() < \cf5 0.6\cf0 :\cb1 \
\cb2                 \cf6 # Completely identical pattern\cf0 \cb1 \
\cb2                 config.update(base_pattern)\cb1 \
\cb2                 config[\cf4 'is_reused_pattern'\cf0 ] = \cf7 True\cf0 \cb1 \
\cb2                 config[\cf4 'reuse_type'\cf0 ] = \cf4 'identical'\cf0 \cb1 \
\cb2             \cf3 else\cf0 :\cb1 \
\cb2                 \cf6 # Similar pattern with variations\cf0 \cb1 \
\cb2                 config.update(\cf9 self\cf0 ._create_similar_pattern(base_pattern))\cb1 \
\cb2                 config[\cf4 'is_reused_pattern'\cf0 ] = \cf7 True\cf0 \cb1 \
\cb2                 config[\cf4 'reuse_type'\cf0 ] = \cf4 'similar'\cf0 \cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf6 # Generate new pattern\cf0 \cb1 \
\cb2             config.update(\cf9 self\cf0 ._generate_new_pattern_config(phase_type, segment_length))\cb1 \
\
\cb2             \cf6 # Store pattern if it's phase 1 (learning phase)\cf0 \cb1 \
\cb2             \cf3 if\cf0  phase_idx == \cf5 0\cf0 :\cb1 \
\cb2                 pattern_to_store = config.copy()\cb1 \
\cb2                 \cf9 self\cf0 .stored_patterns.append(pattern_to_store)\cb1 \
\
\cb2         \cf3 return\cf0  config\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _generate_new_pattern_config\cf0 (\cf9 self\cf0 , \cf9 phase_type\cf0 , \cf9 segment_length\cf0 ):\cb1 \
\cb2         \cf4 """Generate new pattern configuration"""\cf0 \cb1 \
\cb2         \cf6 # Generate layered reward structure with uneven gaps\cf0 \cb1 \
\cb2         actions = \cf10 list\cf0 (\cf8 range\cf0 (\cf9 self\cf0 .n_actions))\cb1 \
\cb2         \cf9 self\cf0 .master_rng.shuffle(actions)\cb1 \
\
\cb2         config = \{\}\cb1 \
\
\cb2         \cf3 if\cf0  phase_type == \cf4 'learning'\cf0 :\cb1 \
\cb2             \cf6 # Clear hierarchy with uneven gaps for complex learning\cf0 \cb1 \
\cb2             config.update(\{\cb1 \
\cb2                 \cf4 'excellent_actions'\cf0 : actions[:\cf5 1\cf0 ],      \cf6 # 0.80-0.90\cf0 \cb1 \
\cb2                 \cf4 'good_actions'\cf0 : actions[\cf5 1\cf0 :\cf5 2\cf0 ],          \cf6 # 0.75-0.82 (small gap: 0.065)\cf0 \cb1 \
\cb2                 \cf4 'medium_actions'\cf0 : actions[\cf5 2\cf0 :\cf5 3\cf0 ],        \cf6 # 0.45-0.55 (large gap: 0.285)\cf0 \cb1 \
\cb2                 \cf4 'poor_actions'\cf0 : actions[\cf5 3\cf0 :\cf5 4\cf0 ],          \cf6 # 0.15-0.25 (large gap: 0.30)\cf0 \cb1 \
\cb2                 \cf4 'bad_actions'\cf0 : actions[\cf5 4\cf0 :],            \cf6 # 0.05-0.15 (small gap: 0.10)\cf0 \cb1 \
\cb2                 \cf4 'noise_multiplier'\cf0 : \cf5 0.8\cf0 ,\cb1 \
\cb2                 \cf4 'extreme_event_prob'\cf0 : \cf5 0.02\cf0 ,\cb1 \
\cb2                 \cf4 'sequence_bonus_prob'\cf0 : \cf5 0.1\cf0 ,\cb1 \
\cb2                 \cf4 'emotional_shock_point'\cf0 : segment_length // \cf5 2\cf0 ,  \cb1 \
\cb2                 \cf4 'shock_magnitude'\cf0 : \cf5 0.4\cf0 \cb1 \
\cb2             \})\cb1 \
\cb2         \cf3 elif\cf0  phase_type == \cf4 'challenging'\cf0 :\cb1 \
\cb2             \cf6 # Moderate hierarchy with context-dependent patterns\cf0 \cb1 \
\cb2             config.update(\{\cb1 \
\cb2                 \cf4 'excellent_actions'\cf0 : actions[:\cf5 1\cf0 ],      \cf6 # 0.80-0.85\cf0 \cb1 \
\cb2                 \cf4 'good_actions'\cf0 : actions[\cf5 1\cf0 :\cf5 3\cf0 ],          \cf6 # 0.75-0.80 (small gap)\cf0 \cb1 \
\cb2                 \cf4 'medium_actions'\cf0 : actions[\cf5 3\cf0 :\cf5 4\cf0 ],        \cf6 # 0.40-0.50 (large gap)\cf0 \cb1 \
\cb2                 \cf4 'poor_actions'\cf0 : actions[\cf5 4\cf0 :],           \cf6 # 0.20-0.30 (large gap)\cf0 \cb1 \
\cb2                 \cf4 'bad_actions'\cf0 : [],\cb1 \
\cb2                 \cf4 'noise_multiplier'\cf0 : \cf5 1.0\cf0 ,\cb1 \
\cb2                 \cf4 'extreme_event_prob'\cf0 : \cf5 0.05\cf0 ,\cb1 \
\cb2                 \cf4 'sequence_bonus_prob'\cf0 : \cf5 0.15\cf0 ,\cb1 \
\cb2                 \cf4 'context_dependent_bonus'\cf0 : \cf5 0.15\cf0 , \cb1 \
\cb2                 \cf4 'memory_window'\cf0 : \cf5 20\cf0   \cf6 # Long-term memory dependency\cf0 \cb1 \
\cb2             \})\cb1 \
\cb2         \cf3 else\cf0 :  \cf6 # mastery\cf0 \cb1 \
\cb2             \cf6 # Complex patterns requiring sophisticated learning\cf0 \cb1 \
\cb2             config.update(\{\cb1 \
\cb2                 \cf4 'excellent_actions'\cf0 : actions[:\cf5 2\cf0 ],      \cf6 # 0.75-0.80\cf0 \cb1 \
\cb2                 \cf4 'good_actions'\cf0 : actions[\cf5 2\cf0 :\cf5 3\cf0 ],          \cf6 # 0.70-0.75 (small gap)\cf0 \cb1 \
\cb2                 \cf4 'medium_actions'\cf0 : actions[\cf5 3\cf0 :],         \cf6 # 0.35-0.45 (large gap)\cf0 \cb1 \
\cb2                 \cf4 'poor_actions'\cf0 : [],\cb1 \
\cb2                 \cf4 'bad_actions'\cf0 : [],\cb1 \
\cb2                 \cf4 'noise_multiplier'\cf0 : \cf5 1.2\cf0 ,\cb1 \
\cb2                 \cf4 'extreme_event_prob'\cf0 : \cf5 0.08\cf0 ,\cb1 \
\cb2                 \cf4 'sequence_bonus_prob'\cf0 : \cf5 0.2\cf0 ,\cb1 \
\cb2                 \cf4 'expectation_violation_prob'\cf0 : \cf5 0.06\cf0 ,  \cb1 \
\cb2                 \cf4 'long_term_memory_bonus'\cf0 : \cf5 0.1\cf0  \cb1 \
\cb2             \})\cb1 \
\
\cb2         \cf3 return\cf0  config\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _create_similar_pattern\cf0 (\cf9 self\cf0 , \cf9 base_pattern\cf0 ):\cb1 \
\cb2         \cf4 """Create similar but not identical pattern"""\cf0 \cb1 \
\cb2         similar_pattern = base_pattern.copy()\cb1 \
\
\cb2         \cf6 # Introduce small variations\cf0 \cb1 \
\cb2         variation_type = \cf9 self\cf0 .master_rng.choice([\cf4 'timing'\cf0 , \cf4 'magnitude'\cf0 , \cf4 'order'\cf0 ])\cb1 \
\
\cb2         \cf3 if\cf0  variation_type == \cf4 'timing'\cf0 :\cb1 \
\cb2             \cf6 # Change timing of key events by \'b120%\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf4 'emotional_shock_point'\cf0  \cf7 in\cf0  similar_pattern:\cb1 \
\cb2                 original_point = similar_pattern[\cf4 'emotional_shock_point'\cf0 ]\cb1 \
\cb2                 variation = \cf10 int\cf0 (original_point * \cf5 0.2\cf0 )\cb1 \
\cb2                 similar_pattern[\cf4 'emotional_shock_point'\cf0 ] = original_point + \cf9 self\cf0 .master_rng.randint(-variation, variation+\cf5 1\cf0 )\cb1 \
\
\cb2         \cf3 elif\cf0  variation_type == \cf4 'magnitude'\cf0 :\cb1 \
\cb2             \cf6 # Slightly adjust reward magnitudes\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf4 'shock_magnitude'\cf0  \cf7 in\cf0  similar_pattern:\cb1 \
\cb2                 similar_pattern[\cf4 'shock_magnitude'\cf0 ] *= \cf9 self\cf0 .master_rng.uniform(\cf5 0.8\cf0 , \cf5 1.2\cf0 )\cb1 \
\cb2             \cf3 if\cf0  \cf4 'context_dependent_bonus'\cf0  \cf7 in\cf0  similar_pattern:\cb1 \
\cb2                 similar_pattern[\cf4 'context_dependent_bonus'\cf0 ] *= \cf9 self\cf0 .master_rng.uniform(\cf5 0.8\cf0 , \cf5 1.2\cf0 )\cb1 \
\
\cb2         \cf3 elif\cf0  variation_type == \cf4 'order'\cf0 :\cb1 \
\cb2             \cf6 # Swap positions of some action groups\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf4 'good_actions'\cf0  \cf7 in\cf0  similar_pattern \cf7 and\cf0  \cf4 'medium_actions'\cf0  \cf7 in\cf0  similar_pattern:\cb1 \
\cb2                 \cf3 if\cf0  \cf8 len\cf0 (similar_pattern[\cf4 'good_actions'\cf0 ]) == \cf5 1\cf0  \cf7 and\cf0  \cf8 len\cf0 (similar_pattern[\cf4 'medium_actions'\cf0 ]) == \cf5 1\cf0 :\cb1 \
\cb2                     \cf6 # Swap good and medium\cf0 \cb1 \
\cb2                     temp = similar_pattern[\cf4 'good_actions'\cf0 ][\cf5 0\cf0 ]\cb1 \
\cb2                     similar_pattern[\cf4 'good_actions'\cf0 ][\cf5 0\cf0 ] = similar_pattern[\cf4 'medium_actions'\cf0 ][\cf5 0\cf0 ]\cb1 \
\cb2                     similar_pattern[\cf4 'medium_actions'\cf0 ][\cf5 0\cf0 ] = temp\cb1 \
\
\cb2         similar_pattern[\cf4 'reuse_type'\cf0 ] = \cf4 'similar'\cf0 \cb1 \
\cb2         \cf3 return\cf0  similar_pattern\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _initialize_current_segment\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Initialize current segment"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .current_segment_idx >= \cf8 len\cf0 (\cf9 self\cf0 .segments):\cb1 \
\cb2             \cf3 return\cf0 \cb1 \
\
\cb2         segment = \cf9 self\cf0 .segments[\cf9 self\cf0 .current_segment_idx]\cb1 \
\cb2         \cf9 self\cf0 .segment_start_trial = segment[\cf4 'start'\cf0 ]\cb1 \
\cb2         \cf9 self\cf0 ._update_rewards()\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _update_rewards\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Update reward structure with enhanced features"""\cf0 \cb1 \
\cb2         segment = \cf9 self\cf0 .segments[\cf9 self\cf0 .current_segment_idx]\cb1 \
\cb2         config = segment[\cf4 'config'\cf0 ]\cb1 \
\cb2         trial_in_segment = \cf9 self\cf0 .trial - \cf9 self\cf0 .segment_start_trial\cb1 \
\
\cb2         \cf6 # Base reward structure with uneven gaps\cf0 \cb1 \
\cb2         rewards = [\cf5 0.2\cf0 ] * \cf9 self\cf0 .n_actions\cb1 \
\
\cb2         \cf6 # Apply layered reward structure\cf0 \cb1 \
\cb2         \cf3 for\cf0  action \cf7 in\cf0  config.get(\cf4 'excellent_actions'\cf0 , []):\cb1 \
\cb2             rewards[action] = \cf9 self\cf0 .master_rng.uniform(\cf5 0.80\cf0 , \cf5 0.90\cf0 )\cb1 \
\cb2         \cf3 for\cf0  action \cf7 in\cf0  config.get(\cf4 'good_actions'\cf0 , []):\cb1 \
\cb2             rewards[action] = \cf9 self\cf0 .master_rng.uniform(\cf5 0.75\cf0 , \cf5 0.82\cf0 )  \cf6 # Small gap\cf0 \cb1 \
\cb2         \cf3 for\cf0  action \cf7 in\cf0  config.get(\cf4 'medium_actions'\cf0 , []):\cb1 \
\cb2             rewards[action] = \cf9 self\cf0 .master_rng.uniform(\cf5 0.45\cf0 , \cf5 0.55\cf0 )  \cf6 # Large gap from good\cf0 \cb1 \
\cb2         \cf3 for\cf0  action \cf7 in\cf0  config.get(\cf4 'poor_actions'\cf0 , []):\cb1 \
\cb2             rewards[action] = \cf9 self\cf0 .master_rng.uniform(\cf5 0.15\cf0 , \cf5 0.25\cf0 )  \cf6 # Large gap from medium\cf0 \cb1 \
\cb2         \cf3 for\cf0  action \cf7 in\cf0  config.get(\cf4 'bad_actions'\cf0 , []):\cb1 \
\cb2             rewards[action] = \cf9 self\cf0 .master_rng.uniform(\cf5 0.05\cf0 , \cf5 0.15\cf0 )  \cf6 # Small gap from poor\cf0 \cb1 \
\
\cb2         \cf6 # ECIA-specific features\cf0 \cb1 \
\cb2         \cf9 self\cf0 ._apply_ecia_features(rewards, config, trial_in_segment)\cb1 \
\
\cb2         \cf6 # Apply sequence bonuses/penalties (for all agents)\cf0 \cb1 \
\cb2         \cf9 self\cf0 ._apply_sequence_effects(rewards, config)\cb1 \
\
\cb2         \cf6 # Apply temporal variations\cf0 \cb1 \
\cb2         \cf9 self\cf0 ._apply_temporal_patterns(rewards, trial_in_segment)\cb1 \
\
\cb2         \cf9 self\cf0 .current_rewards = rewards\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _apply_ecia_features\cf0 (\cf9 self\cf0 , \cf9 rewards\cf0 , \cf9 config\cf0 , \cf9 trial_in_segment\cf0 ):\cb1 \
\cb2         \cf4 """Apply ECIA-specific learning opportunities"""\cf0 \cb1 \
\
\cb2         \cf6 # Emotional shock (sudden reversal)\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf4 'emotional_shock_point'\cf0  \cf7 in\cf0  config \cf7 and\cf0  trial_in_segment == config[\cf4 'emotional_shock_point'\cf0 ]:\cb1 \
\cb2             excellent_actions = config.get(\cf4 'excellent_actions'\cf0 , [])\cb1 \
\cb2             poor_actions = config.get(\cf4 'poor_actions'\cf0 , [])\cb1 \
\
\cb2             \cf3 if\cf0  excellent_actions \cf7 and\cf0  poor_actions:\cb1 \
\cb2                 \cf6 # Dramatic reversal: best becomes worst, worst becomes best\cf0 \cb1 \
\cb2                 excellent_action = excellent_actions[\cf5 0\cf0 ]\cb1 \
\cb2                 poor_action = poor_actions[\cf5 0\cf0 ]\cb1 \
\
\cb2                 \cf6 # Swap their rewards dramatically\cf0 \cb1 \
\cb2                 rewards[excellent_action] = \cf5 0.1\cf0   \cf6 # Dramatic drop\cf0 \cb1 \
\cb2                 rewards[poor_action] = \cf5 0.85\cf0        \cf6 # Dramatic rise\cf0 \cb1 \
\
\cb2         \cf6 # Context-dependent bonuses\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf4 'context_dependent_bonus'\cf0  \cf7 in\cf0  config \cf7 and\cf0  \cf8 len\cf0 (\cf9 self\cf0 .reward_history) >= \cf5 5\cf0 :\cb1 \
\cb2             recent_avg = np.mean(\cf10 list\cf0 (\cf9 self\cf0 .reward_history)[\cf5 -5\cf0 :])\cb1 \
\cb2             context_bonus = config[\cf4 'context_dependent_bonus'\cf0 ]\cb1 \
\
\cb2             \cf6 # Bonus based on recent performance context\cf0 \cb1 \
\cb2             \cf3 if\cf0  recent_avg > \cf5 0.6\cf0 :  \cf6 # Good performance context\cf0 \cb1 \
\cb2                 \cf3 for\cf0  action \cf7 in\cf0  config.get(\cf4 'excellent_actions'\cf0 , []):\cb1 \
\cb2                     rewards[action] += context_bonus\cb1 \
\cb2             \cf3 elif\cf0  recent_avg < \cf5 0.4\cf0 :  \cf6 # Poor performance context\cf0 \cb1 \
\cb2                 \cf3 for\cf0  action \cf7 in\cf0  config.get(\cf4 'medium_actions'\cf0 , []):\cb1 \
\cb2                     rewards[action] += context_bonus * \cf5 0.5\cf0 \cb1 \
\
\cb2         \cf6 # Long-term memory bonuses\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf4 'long_term_memory_bonus'\cf0  \cf7 in\cf0  config \cf7 and\cf0  \cf8 len\cf0 (\cf9 self\cf0 .reward_history) >= \cf5 30\cf0 :\cb1 \
\cb2             distant_avg = np.mean(\cf10 list\cf0 (\cf9 self\cf0 .reward_history)[\cf5 -30\cf0 :\cf5 -20\cf0 ])\cb1 \
\cb2             recent_avg = np.mean(\cf10 list\cf0 (\cf9 self\cf0 .reward_history)[\cf5 -10\cf0 :])\cb1 \
\
\cb2             \cf6 # Reward for recognizing long-term patterns\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf8 abs\cf0 (distant_avg - recent_avg) < \cf5 0.1\cf0 :  \cf6 # Similar patterns\cf0 \cb1 \
\cb2                 memory_bonus = config[\cf4 'long_term_memory_bonus'\cf0 ]\cb1 \
\cb2                 \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions):\cb1 \
\cb2                     rewards[i] += memory_bonus\cb1 \
\
\cb2         \cf6 # Expectation violation (surprise learning)\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf4 'expectation_violation_prob'\cf0  \cf7 in\cf0  config:\cb1 \
\cb2             \cf3 if\cf0  \cf9 self\cf0 .master_rng.random() < config[\cf4 'expectation_violation_prob'\cf0 ]:\cb1 \
\cb2                 surprise_action = \cf9 self\cf0 .master_rng.randint(\cf5 0\cf0 , \cf9 self\cf0 .n_actions)\cb1 \
\cb2                 \cf3 if\cf0  \cf9 self\cf0 .master_rng.random() < \cf5 0.7\cf0 :\cb1 \
\cb2                     rewards[surprise_action] = \cf8 min\cf0 (\cf5 0.95\cf0 , rewards[surprise_action] + \cf5 0.3\cf0 )\cb1 \
\cb2                 \cf3 else\cf0 :\cb1 \
\cb2                     rewards[surprise_action] = \cf8 max\cf0 (\cf5 0.05\cf0 , rewards[surprise_action] - \cf5 0.3\cf0 )\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _apply_sequence_effects\cf0 (\cf9 self\cf0 , \cf9 rewards\cf0 , \cf9 config\cf0 ):\cb1 \
\cb2         \cf4 """Apply sequence bonuses/penalties (fair for all agents)"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .action_sequence) >= \cf5 3\cf0 :\cb1 \
\cb2             last_actions = \cf10 list\cf0 (\cf9 self\cf0 .action_sequence)[\cf5 -3\cf0 :]\cb1 \
\
\cb2             \cf6 # Diversity bonus\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf8 len\cf0 (\cf10 set\cf0 (last_actions)) == \cf5 3\cf0 :  \cf6 # All different\cf0 \cb1 \
\cb2                 \cf3 if\cf0  \cf9 self\cf0 .master_rng.random() < config.get(\cf4 'sequence_bonus_prob'\cf0 , \cf5 0.1\cf0 ):\cb1 \
\cb2                     \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions):\cb1 \
\cb2                         rewards[i] += \cf5 0.1\cf0 \cb1 \
\
\cb2             \cf6 # Habit penalty\cf0 \cb1 \
\cb2             \cf3 elif\cf0  \cf8 len\cf0 (\cf10 set\cf0 (last_actions)) == \cf5 1\cf0 :  \cf6 # All same\cf0 \cb1 \
\cb2                 repeated_action = last_actions[\cf5 0\cf0 ]\cb1 \
\cb2                 rewards[repeated_action] *= \cf5 0.85\cf0   \cf6 # 15% penalty\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _apply_temporal_patterns\cf0 (\cf9 self\cf0 , \cf9 rewards\cf0 , \cf9 trial_in_segment\cf0 ):\cb1 \
\cb2         \cf4 """Apply multi-layered temporal patterns"""\cf0 \cb1 \
\
\cb2         \cf6 # Short-term cycle (10-15 trials)\cf0 \cb1 \
\cb2         short_cycle_pos = (trial_in_segment % \cf5 12\cf0 ) / \cf5 12.0\cf0 \cb1 \
\cb2         short_modifier = \cf5 0.05\cf0  * np.sin(\cf5 2\cf0  * np.pi * short_cycle_pos)\cb1 \
\
\cb2         \cf6 # Medium-term cycle (50-80 trials)\cf0 \cb1 \
\cb2         medium_cycle_pos = (trial_in_segment % \cf5 65\cf0 ) / \cf5 65.0\cf0 \cb1 \
\cb2         medium_modifier = \cf5 0.03\cf0  * np.cos(\cf5 2\cf0  * np.pi * medium_cycle_pos)\cb1 \
\
\cb2         \cf6 # Apply modifiers\cf0 \cb1 \
\cb2         \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions):\cb1 \
\cb2             rewards[i] += short_modifier + medium_modifier\cb1 \
\cb2             rewards[i] = \cf8 max\cf0 (\cf5 0.05\cf0 , \cf8 min\cf0 (\cf5 0.95\cf0 , rewards[i]))  \cf6 # Clamp\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 _update_context\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Update context (keeping it fair - same as original)"""\cf0 \cb1 \
\cb2         norm_t = \cf9 self\cf0 .trial / \cf9 self\cf0 .total_trials\cb1 \
\cb2         phase_id = \cf9 self\cf0 .current_segment_idx % \cf5 4\cf0   \cf6 # Simple phase cycling\cf0 \cb1 \
\cb2         \cf9 self\cf0 .context = np.array([norm_t, phase_id])\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 step\cf0 (\cf9 self\cf0 , \cf9 action\cf0 ):\cb1 \
\cb2         \cf4 """Execute one step with enhanced features"""\cf0 \cb1 \
\
\cb2         \cf6 # Check if we need to move to next segment\cf0 \cb1 \
\cb2         current_segment = \cf9 self\cf0 .segments[\cf9 self\cf0 .current_segment_idx]\cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .trial >= current_segment[\cf4 'end'\cf0 ] \cf7 and\cf0  \cf9 self\cf0 .current_segment_idx < \cf8 len\cf0 (\cf9 self\cf0 .segments) - \cf5 1\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .current_segment_idx += \cf5 1\cf0 \cb1 \
\cb2             \cf9 self\cf0 ._initialize_current_segment()\cb1 \
\
\cb2         \cf6 # Update rewards for current trial\cf0 \cb1 \
\cb2         \cf9 self\cf0 ._update_rewards()\cb1 \
\
\cb2         \cf6 # Get base reward\cf0 \cb1 \
\cb2         base_reward = \cf9 self\cf0 .current_rewards[action]\cb1 \
\
\cb2         \cf6 # Apply noise (enhanced based on phase)\cf0 \cb1 \
\cb2         segment = \cf9 self\cf0 .segments[\cf9 self\cf0 .current_segment_idx]\cb1 \
\cb2         noise_multiplier = segment[\cf4 'config'\cf0 ].get(\cf4 'noise_multiplier'\cf0 , \cf5 1.0\cf0 )\cb1 \
\cb2         noise = \cf9 self\cf0 .master_rng.normal(\cf5 0\cf0 , \cf9 self\cf0 .sigma * noise_multiplier)\cb1 \
\cb2         final_reward = np.clip(base_reward + noise, \cf5 0\cf0 , \cf5 1\cf0 )\cb1 \
\
\cb2         \cf6 # Update tracking\cf0 \cb1 \
\cb2         \cf9 self\cf0 .reward_history.append(final_reward)\cb1 \
\cb2         \cf9 self\cf0 .action_sequence.append(action)\cb1 \
\
\cb2         \cf6 # Update trial and context\cf0 \cb1 \
\cb2         \cf9 self\cf0 .trial += \cf5 1\cf0 \cb1 \
\cb2         \cf9 self\cf0 ._update_context()\cb1 \
\
\cb2         \cf3 return\cf0  final_reward, \cf9 self\cf0 .context\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 reset\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Reset environment to initial state"""\cf0 \cb1 \
\cb2         \cf9 self\cf0 .trial = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .current_segment_idx = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .segment_start_trial = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .reward_history.clear()\cb1 \
\cb2         \cf9 self\cf0 .action_sequence.clear()\cb1 \
\cb2         \cf9 self\cf0 .extreme_event_counter = \cf5 0\cf0 \cb1 \
\
\cb2         \cf9 self\cf0 ._initialize_current_segment()\cb1 \
\cb2         \cf9 self\cf0 ._update_context()\cb1 \
\cb2         \cf3 return\cf0  \cf7 None\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 get_change_points\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Get actual change points for analysis"""\cf0 \cb1 \
\cb2         \cf3 return\cf0  [segment[\cf4 'start'\cf0 ] \cf3 for\cf0  segment \cf7 in\cf0  \cf9 self\cf0 .segments[\cf5 1\cf0 :]]\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 get_pattern_reuse_info\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Get information about pattern reuse for analysis"""\cf0 \cb1 \
\cb2         reuse_info = []\cb1 \
\cb2         \cf3 for\cf0  segment \cf7 in\cf0  \cf9 self\cf0 .segments:\cb1 \
\cb2             \cf3 if\cf0  segment[\cf4 'config'\cf0 ].get(\cf4 'is_reused_pattern'\cf0 , \cf7 False\cf0 ):\cb1 \
\cb2                 reuse_info.append(\{\cb1 \
\cb2                     \cf4 'start'\cf0 : segment[\cf4 'start'\cf0 ],\cb1 \
\cb2                     \cf4 'end'\cf0 : segment[\cf4 'end'\cf0 ],\cb1 \
\cb2                     \cf4 'reuse_type'\cf0 : segment[\cf4 'config'\cf0 ].get(\cf4 'reuse_type'\cf0 , \cf4 'unknown'\cf0 )\cb1 \
\cb2                 \})\cb1 \
\cb2         \cf3 return\cf0  reuse_info\cb1 \
\
\cf3 \cb2 import\cf0  numpy \cf3 as\cf0  np\cb1 \
\cf3 \cb2 from\cf0  collections \cf3 import\cf0  deque\cb1 \
\
\cf7 \cb2 class\cf0  ECIA:\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 , \cf9 n_actions\cf0 =\cf5 5\cf0 , \cf9 epsilon\cf0 =\cf5 0.03\cf0 , \cf9 eta\cf0 =\cf5 0.55\cf0 , \cf9 xi\cf0 =\cf5 0.001\cf0 ,\cb1 \
\cb2                  \cf9 memory_threshold\cf0 =\cf5 0.015\cf0 , \cf9 memory_influence\cf0 =\cf5 0.3\cf0 ,\cb1 \
\cb2                  \cf9 window_size\cf0 =\cf5 30\cf0 , \cf9 min_eta\cf0 =\cf5 0.095\cf0 , \cf9 memory_size\cf0 =\cf5 15\cf0 ,\cb1 \
\cb2                  \cf9 alpha\cf0 =\cf5 0.22\cf0 , \cf9 memory_similarity_threshold\cf0 =\cf5 0.035\cf0 ,\cb1 \
\cb2                  \cf9 top_k\cf0 =\cf5 3\cf0 , \cf9 emotion_decay\cf0 =\cf5 0.96\cf0 , \cf9 random_state\cf0 =\cf7 None\cf0 ):\cb1 \
\
\cb2         \cf3 if\cf0  random_state \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .rng = np.random.RandomState(random_state)\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .rng = np.random.RandomState()\cb1 \
\
\cb2         \cf6 # Basic parameters\cf0 \cb1 \
\cb2         \cf9 self\cf0 .n_actions = n_actions\cb1 \
\cb2         \cf9 self\cf0 .epsilon = epsilon\cb1 \
\cb2         \cf9 self\cf0 .xi = xi\cb1 \
\cb2         \cf9 self\cf0 .alpha = alpha\cb1 \
\cb2         \cf9 self\cf0 .emotion_decay = emotion_decay\cb1 \
\cb2         \cf9 self\cf0 .window_size = window_size\cb1 \
\cb2         \cf9 self\cf0 .min_eta = min_eta\cb1 \
\cb2         \cf9 self\cf0 .top_k = top_k\cb1 \
\
\cb2         \cf6 # Dynamic eta adjustment\cf0 \cb1 \
\cb2         \cf9 self\cf0 .base_eta = eta\cb1 \
\cb2         \cf9 self\cf0 .eta = \cf9 self\cf0 .base_eta\cb1 \
\cb2         \cf9 self\cf0 .eta_adaptation_counter = \cf5 0\cf0 \cb1 \
\
\cb2         \cf6 # Memory system\cf0 \cb1 \
\cb2         \cf9 self\cf0 .base_memory_threshold = memory_threshold\cb1 \
\cb2         \cf9 self\cf0 .base_memory_influence = memory_influence\cb1 \
\cb2         \cf9 self\cf0 .memory_size = memory_size\cb1 \
\cb2         \cf9 self\cf0 .memory_similarity_threshold = memory_similarity_threshold\cb1 \
\cb2         \cf9 self\cf0 .memory_activation_level = \cf5 1.0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .memory_quality_threshold = \cf5 0.15\cf0 \cb1 \
\cb2         \cf9 self\cf0 .memory_usage_history = deque(maxlen=\cf5 20\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .memory_cooldown = \cf5 0\cf0 \cb1 \
\
\cb2         \cf6 # Environment detection\cf0 \cb1 \
\cb2         \cf9 self\cf0 .memory_effectiveness_tracker = deque(maxlen=\cf5 50\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .environment_stability_tracker = deque(maxlen=\cf5 30\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .change_detection_window = deque(maxlen=\cf5 15\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .stable_performance_counter = \cf5 0\cf0 \cb1 \
\
\cb2         \cf6 # Context clustering\cf0 \cb1 \
\cb2         \cf9 self\cf0 .context_clusters = \{\}\cb1 \
\cb2         \cf9 self\cf0 .cluster_performance = \{\}\cb1 \
\cb2         \cf9 self\cf0 .use_context_clustering = \cf7 True\cf0 \cb1 \
\
\cb2         \cf6 # Basic initialization\cf0 \cb1 \
\cb2         \cf9 self\cf0 .q_values = np.zeros(n_actions)\cb1 \
\cb2         \cf9 self\cf0 .emotion = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .action_counts = np.zeros(n_actions)\cb1 \
\cb2         \cf9 self\cf0 .time = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .prev_reward = \cf5 0.5\cf0 \cb1 \
\cb2         \cf9 self\cf0 .context = \cf7 None\cf0 \cb1 \
\
\cb2         \cf6 # Emotion system\cf0 \cb1 \
\cb2         \cf9 self\cf0 .emotion_names = [\cb1 \
\cb2             \cf4 "fear"\cf0 , \cf4 "joy"\cf0 , \cf4 "hope"\cf0 , \cf4 "sadness"\cf0 ,\cb1 \
\cb2             \cf4 "curiosity"\cf0 , \cf4 "anger"\cf0 , \cf4 "pride"\cf0 , \cf4 "shame"\cf0 \cb1 \
\cb2         ]\cb1 \
\cb2         \cf9 self\cf0 .emotion_weight = np.array([\cb1 \
\cb2             \cf5 -0.15\cf0 , \cf5 0.4\cf0 , \cf5 0.3\cf0 , \cf5 -0.2\cf0 , \cf5 0.35\cf0 , \cf5 -0.25\cf0 , \cf5 0.25\cf0 , \cf5 -0.3\cf0 \cb1 \
\cb2         ])\cb1 \
\cb2         \cf9 self\cf0 .max_total_emotion_energy = \cf5 2.5\cf0 \cb1 \
\cb2         \cf9 self\cf0 .emotion_momentum = np.zeros(\cf5 8\cf0 )\cb1 \
\
\cb2         \cf6 # Memory storage\cf0 \cb1 \
\cb2         \cf9 self\cf0 .episodic_memory = []\cb1 \
\
\cb2         \cf6 # Other variables\cf0 \cb1 \
\cb2         \cf9 self\cf0 .performance_tracker = deque(maxlen=\cf5 25\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .recent_context_changes = deque(maxlen=\cf5 10\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .action_history = deque(maxlen=\cf5 20\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .reward_history = deque(maxlen=\cf5 20\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .learning_boost = \cf5 0.2\cf0 \cb1 \
\cb2         \cf9 self\cf0 .successful_emotion_patterns = \{\}\cb1 \
\cb2         \cf9 self\cf0 .neurogenesis_cycle = \cf5 25\cf0 \cb1 \
\cb2         \cf9 self\cf0 .emotion_learning_rates = np.array([\cb1 \
\cb2             \cf5 0.15\cf0 , \cf5 0.25\cf0 , \cf5 0.20\cf0 , \cf5 0.12\cf0 , \cf5 0.30\cf0 , \cf5 0.18\cf0 , \cf5 0.22\cf0 , \cf5 0.28\cf0 \cb1 \
\cb2         ])\cb1 \
\cb2         \cf9 self\cf0 .emotion_action_history = deque(maxlen=\cf5 12\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .context_change_threshold = \cf5 0.1\cf0 \cb1 \
\cb2         \cf9 self\cf0 .habit_strength_factor = \cf5 0.025\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 assess_environment_stability\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Comprehensive environment stability assessment"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .recent_context_changes) < \cf5 5\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.5\cf0 \cb1 \
\
\cb2         \cf6 # Context change stability\cf0 \cb1 \
\cb2         context_changes = \cf10 list\cf0 (\cf9 self\cf0 .recent_context_changes)\cb1 \
\cb2         avg_change = np.mean(context_changes)\cb1 \
\cb2         change_variance = np.var(context_changes)\cb1 \
\cb2         context_stability = \cf5 1.0\cf0  - \cf8 min\cf0 (avg_change + change_variance * \cf5 0.5\cf0 , \cf5 1.0\cf0 )\cb1 \
\
\cb2         \cf6 # Reward stability\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .reward_history) >= \cf5 10\cf0 :\cb1 \
\cb2             recent_rewards = \cf10 list\cf0 (\cf9 self\cf0 .reward_history)[\cf5 -10\cf0 :]\cb1 \
\cb2             reward_stability = \cf5 1.0\cf0  - \cf8 min\cf0 (np.std(recent_rewards), \cf5 1.0\cf0 )\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             reward_stability = \cf5 0.5\cf0 \cb1 \
\
\cb2         \cf6 # Performance trend stability\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .performance_tracker) >= \cf5 8\cf0 :\cb1 \
\cb2             recent_performance = \cf10 list\cf0 (\cf9 self\cf0 .performance_tracker)[\cf5 -8\cf0 :]\cb1 \
\cb2             performance_trend = \cf8 abs\cf0 (np.polyfit(\cf8 range\cf0 (\cf5 8\cf0 ), recent_performance, \cf5 1\cf0 )[\cf5 0\cf0 ])\cb1 \
\cb2             trend_stability = \cf5 1.0\cf0  - \cf8 min\cf0 (performance_trend * \cf5 5\cf0 , \cf5 1.0\cf0 )\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             trend_stability = \cf5 0.5\cf0 \cb1 \
\
\cb2         \cf6 # Overall stability\cf0 \cb1 \
\cb2         stability = \cf5 0.4\cf0  * context_stability + \cf5 0.4\cf0  * reward_stability + \cf5 0.2\cf0  * trend_stability\cb1 \
\cb2         \cf9 self\cf0 .environment_stability_tracker.append(stability)\cb1 \
\cb2         \cf3 return\cf0  np.mean(\cf9 self\cf0 .environment_stability_tracker) \cf3 if\cf0  \cf9 self\cf0 .environment_stability_tracker \cf3 else\cf0  \cf5 0.5\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 evaluate_memory_effectiveness\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Memory system effectiveness evaluation"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .memory_usage_history) < \cf5 10\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.5\cf0 \cb1 \
\
\cb2         memory_used_rewards = [r \cf3 for\cf0  used, r \cf7 in\cf0  \cf9 self\cf0 .memory_usage_history \cf3 if\cf0  used]\cb1 \
\cb2         memory_unused_rewards = [r \cf3 for\cf0  used, r \cf7 in\cf0  \cf9 self\cf0 .memory_usage_history \cf3 if\cf0  \cf7 not\cf0  used]\cb1 \
\
\cb2         \cf3 if\cf0  \cf8 len\cf0 (memory_used_rewards) > \cf5 3\cf0  \cf7 and\cf0  \cf8 len\cf0 (memory_unused_rewards) > \cf5 3\cf0 :\cb1 \
\cb2             used_avg = np.mean(memory_used_rewards)\cb1 \
\cb2             unused_avg = np.mean(memory_unused_rewards)\cb1 \
\cb2             effectiveness = (used_avg - unused_avg + \cf5 1.0\cf0 ) / \cf5 2.0\cf0 \cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             effectiveness = \cf5 0.5\cf0 \cb1 \
\
\cb2         \cf9 self\cf0 .memory_effectiveness_tracker.append(effectiveness)\cb1 \
\cb2         \cf3 return\cf0  np.mean(\cf9 self\cf0 .memory_effectiveness_tracker) \cf3 if\cf0  \cf9 self\cf0 .memory_effectiveness_tracker \cf3 else\cf0  \cf5 0.5\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 detect_environment_change\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Environment change detection"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .reward_history) < \cf5 10\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf7 False\cf0 \cb1 \
\
\cb2         recent_rewards = \cf10 list\cf0 (\cf9 self\cf0 .reward_history)[\cf5 -10\cf0 :]\cb1 \
\cb2         older_rewards = \cf10 list\cf0 (\cf9 self\cf0 .reward_history)[\cf5 -20\cf0 :\cf5 -10\cf0 ] \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .reward_history) >= \cf5 20\cf0  \cf3 else\cf0  recent_rewards\cb1 \
\
\cb2         recent_mean = np.mean(recent_rewards)\cb1 \
\cb2         older_mean = np.mean(older_rewards)\cb1 \
\cb2         performance_drop = older_mean - recent_mean > \cf5 0.2\cf0 \cb1 \
\cb2         performance_volatility = np.std(recent_rewards) > \cf5 0.25\cf0 \cb1 \
\
\cb2         \cf3 return\cf0  performance_drop \cf7 or\cf0  performance_volatility\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 calculate_memory_need\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Memory need score calculation (0~1)"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .reward_history) < \cf5 10\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.5\cf0 \cb1 \
\
\cb2         \cf6 # Performance volatility\cf0 \cb1 \
\cb2         recent_rewards = \cf10 list\cf0 (\cf9 self\cf0 .reward_history)[\cf5 -10\cf0 :]\cb1 \
\cb2         reward_volatility = np.std(recent_rewards)\cb1 \
\cb2         volatility_score = \cf8 min\cf0 (reward_volatility * \cf5 2\cf0 , \cf5 1.0\cf0 )\cb1 \
\
\cb2         \cf6 # Exploration vs exploitation imbalance\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .action_history) >= \cf5 10\cf0 :\cb1 \
\cb2             action_diversity = \cf8 len\cf0 (\cf10 set\cf0 (\cf10 list\cf0 (\cf9 self\cf0 .action_history)[\cf5 -10\cf0 :])) / \cf9 self\cf0 .n_actions\cb1 \
\cb2             exploration_score = \cf8 abs\cf0 (action_diversity - \cf5 0.6\cf0 ) * \cf5 2\cf0 \cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             exploration_score = \cf5 0.5\cf0 \cb1 \
\
\cb2         \cf6 # Q-value variance\cf0 \cb1 \
\cb2         q_variance = np.var(\cf9 self\cf0 .q_values) \cf3 if\cf0  np.\cf8 sum\cf0 (\cf9 self\cf0 .q_values) != \cf5 0\cf0  \cf3 else\cf0  \cf5 0.5\cf0 \cb1 \
\cb2         uncertainty_score = \cf8 min\cf0 (q_variance * \cf5 5\cf0 , \cf5 1.0\cf0 )\cb1 \
\
\cb2         \cf6 # Emotion intensity\cf0 \cb1 \
\cb2         emotion_intensity = np.linalg.norm(\cf9 self\cf0 .emotion) / np.sqrt(\cf5 8\cf0 )\cb1 \
\cb2         need_score = (\cf5 0.3\cf0  * volatility_score + \cf5 0.25\cf0  * exploration_score +\cb1 \
\cb2                      \cf5 0.25\cf0  * uncertainty_score + \cf5 0.2\cf0  * emotion_intensity)\cb1 \
\
\cb2         \cf3 return\cf0  np.clip(need_score, \cf5 0.0\cf0 , \cf5 1.0\cf0 )\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 analyze_performance_trend\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Recent performance trend analysis"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .performance_tracker) < \cf5 10\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.0\cf0 \cb1 \
\
\cb2         recent_performance = \cf10 list\cf0 (\cf9 self\cf0 .performance_tracker)[\cf5 -10\cf0 :]\cb1 \
\cb2         x = np.arange(\cf8 len\cf0 (recent_performance))\cb1 \
\
\cb2         \cf3 if\cf0  \cf8 len\cf0 (recent_performance) >= \cf5 3\cf0 :\cb1 \
\cb2             trend = np.polyfit(x, recent_performance, \cf5 1\cf0 )[\cf5 0\cf0 ]\cb1 \
\cb2             \cf3 return\cf0  trend\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.0\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 adaptive_memory_control\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Simple and clean adaptive memory control - pure experience-based without environment classification"""\cf0 \cb1 \
\cb2         effectiveness = \cf9 self\cf0 .evaluate_memory_effectiveness()\cb1 \
\
\cb2         \cf6 # Core: Judge only based on memory effectiveness\cf0 \cb1 \
\cb2         \cf3 if\cf0  effectiveness > \cf5 0.6\cf0 :\cb1 \
\cb2             \cf6 # Memory is helpful \uc0\u8594  use more\cf0 \cb1 \
\cb2             \cf9 self\cf0 .memory_activation_level = \cf5 0.8\cf0  + \cf5 0.2\cf0  * effectiveness\cb1 \
\cb2         \cf3 elif\cf0  effectiveness > \cf5 0.4\cf0 :\cb1 \
\cb2             \cf6 # Memory is somewhat helpful \uc0\u8594  moderate use\cf0 \cb1 \
\cb2             \cf9 self\cf0 .memory_activation_level = \cf5 0.4\cf0  + \cf5 0.4\cf0  * effectiveness\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf6 # Memory is not very helpful \uc0\u8594  minimize\cf0 \cb1 \
\cb2             \cf9 self\cf0 .memory_activation_level = \cf5 0.1\cf0  + \cf5 0.3\cf0  * effectiveness\cb1 \
\
\cb2         \cf9 self\cf0 .memory_activation_level = np.clip(\cf9 self\cf0 .memory_activation_level, \cf5 0.05\cf0 , \cf5 1.0\cf0 )\cb1 \
\
\cb2         \cf6 # Dynamic parameter adjustment\cf0 \cb1 \
\cb2         \cf9 self\cf0 .current_memory_threshold = \cf9 self\cf0 .base_memory_threshold / \cf8 max\cf0 (\cf5 0.1\cf0 , \cf9 self\cf0 .memory_activation_level)\cb1 \
\cb2         \cf9 self\cf0 .current_memory_influence = \cf9 self\cf0 .base_memory_influence * \cf9 self\cf0 .memory_activation_level\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 identify_context_cluster\cf0 (\cf9 self\cf0 , \cf9 context\cf0 ):\cb1 \
\cb2         \cf4 """Context cluster identification"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  context \cf7 is\cf0  \cf7 None\cf0  \cf7 or\cf0  \cf8 len\cf0 (context) < \cf5 2\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf4 "default"\cf0 \cb1 \
\
\cb2         norm_t = context[\cf5 0\cf0 ] \cf3 if\cf0  \cf8 len\cf0 (context) > \cf5 0\cf0  \cf3 else\cf0  \cf5 0\cf0 \cb1 \
\cb2         phase_id = \cf10 int\cf0 (context[\cf5 1\cf0 ]) \cf3 if\cf0  \cf8 len\cf0 (context) > \cf5 1\cf0  \cf3 else\cf0  \cf5 0\cf0 \cb1 \
\cb2         time_cluster = \cf10 int\cf0 (norm_t * \cf5 4\cf0 )\cb1 \
\
\cb2         \cf3 return\cf0  \cf7 f\cf4 "phase_\cf0 \{phase_id\}\cf4 _time_\cf0 \{time_cluster\}\cf4 "\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_memory_quality_score\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 , \cf9 prediction_error\cf0 ):\cb1 \
\cb2         \cf4 """Memory quality score calculation"""\cf0 \cb1 \
\cb2         error_score = \cf8 min\cf0 (\cf8 abs\cf0 (prediction_error), \cf5 0.5\cf0 ) / \cf5 0.5\cf0 \cb1 \
\cb2         extreme_reward_score = \cf8 abs\cf0 (reward - \cf5 0.5\cf0 ) / \cf5 0.5\cf0 \cb1 \
\cb2         emotion_intensity = np.linalg.norm(\cf9 self\cf0 .emotion) / np.sqrt(\cf5 8\cf0 )\cb1 \
\
\cb2         \cf3 return\cf0  \cf5 0.4\cf0  * error_score + \cf5 0.4\cf0  * extreme_reward_score + \cf5 0.2\cf0  * emotion_intensity\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 store_adaptive_memory\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 , \cf9 prediction_error\cf0 ):\cb1 \
\cb2         \cf4 """Universal adaptive memory storage"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .memory_activation_level < \cf5 0.1\cf0  \cf7 or\cf0  \cf9 self\cf0 .context \cf7 is\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf3 return\cf0 \cb1 \
\
\cb2         quality_score = \cf9 self\cf0 .compute_memory_quality_score(action, reward, prediction_error)\cb1 \
\
\cb2         \cf3 if\cf0  quality_score < \cf9 self\cf0 .memory_quality_threshold:\cb1 \
\cb2             \cf3 return\cf0 \cb1 \
\
\cb2         \cf3 if\cf0  \cf8 abs\cf0 (prediction_error) > \cf9 self\cf0 .current_memory_threshold:\cb1 \
\cb2             memory = \{\cb1 \
\cb2                 \cf4 'action'\cf0 : action,\cb1 \
\cb2                 \cf4 'reward'\cf0 : reward,\cb1 \
\cb2                 \cf4 'context'\cf0 : \cf9 self\cf0 .context.copy(),\cb1 \
\cb2                 \cf4 'time'\cf0 : \cf9 self\cf0 .time,\cb1 \
\cb2                 \cf4 'prediction_error'\cf0 : \cf8 abs\cf0 (prediction_error),\cb1 \
\cb2                 \cf4 'quality_score'\cf0 : quality_score,\cb1 \
\cb2                 \cf4 'emotion_state'\cf0 : \cf9 self\cf0 .emotion.copy()\cb1 \
\cb2             \}\cb1 \
\
\cb2             \cf6 # Universal memory storage\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf9 self\cf0 .use_context_clustering \cf7 and\cf0  \cf9 self\cf0 .memory_activation_level > \cf5 0.6\cf0 :\cb1 \
\cb2                 cluster_id = \cf9 self\cf0 .identify_context_cluster(\cf9 self\cf0 .context)\cb1 \
\cb2                 memory[\cf4 'cluster_id'\cf0 ] = cluster_id\cb1 \
\
\cb2                 \cf3 if\cf0  cluster_id \cf7 not\cf0  \cf7 in\cf0  \cf9 self\cf0 .context_clusters:\cb1 \
\cb2                     \cf9 self\cf0 .context_clusters[cluster_id] = []\cb1 \
\cb2                     \cf9 self\cf0 .cluster_performance[cluster_id] = deque(maxlen=\cf5 20\cf0 )\cb1 \
\
\cb2                 \cf9 self\cf0 .context_clusters[cluster_id].append(memory)\cb1 \
\cb2                 \cf9 self\cf0 .cluster_performance[cluster_id].append(reward)\cb1 \
\
\cb2                 \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .context_clusters[cluster_id]) > \cf9 self\cf0 .memory_size // \cf5 6\cf0 :\cb1 \
\cb2                     \cf9 self\cf0 .context_clusters[cluster_id].sort(\cb1 \
\cb2                         key=\cf7 lambda\cf0  x: x[\cf4 'quality_score'\cf0 ], reverse=\cf7 True\cf0 \cb1 \
\cb2                     )\cb1 \
\cb2                     \cf9 self\cf0 .context_clusters[cluster_id] = \cf9 self\cf0 .context_clusters[cluster_id][:\cf9 self\cf0 .memory_size // \cf5 6\cf0 ]\cb1 \
\
\cb2             \cf6 # Store in general memory as well\cf0 \cb1 \
\cb2             \cf9 self\cf0 .episodic_memory.append(memory)\cb1 \
\cb2             \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .episodic_memory) > \cf9 self\cf0 .memory_size:\cb1 \
\cb2                 \cf9 self\cf0 .episodic_memory.sort(key=\cf7 lambda\cf0  x: (\cb1 \
\cb2                     \cf5 0.6\cf0  * x[\cf4 'quality_score'\cf0 ] + \cf5 0.4\cf0  * (x[\cf4 'time'\cf0 ] / \cf8 max\cf0 (\cf5 1\cf0 , \cf9 self\cf0 .time))\cb1 \
\cb2                 ), reverse=\cf7 True\cf0 )\cb1 \
\cb2                 \cf9 self\cf0 .episodic_memory = \cf9 self\cf0 .episodic_memory[:\cf9 self\cf0 .memory_size]\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_adaptive_memory_bias\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Universal adaptive memory bias calculation"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .context \cf7 is\cf0  \cf7 None\cf0  \cf7 or\cf0  \cf9 self\cf0 .memory_activation_level < \cf5 0.1\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .memory_usage_history.append((\cf7 False\cf0 , \cf9 self\cf0 .prev_reward))\cb1 \
\cb2             \cf3 return\cf0  np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\
\cb2         relevant_memories = []\cb1 \
\
\cb2         \cf6 # Universal memory retrieval strategy\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .use_context_clustering \cf7 and\cf0  \cf9 self\cf0 .memory_activation_level > \cf5 0.5\cf0  \cf7 and\cf0  \cf9 self\cf0 .context_clusters:\cb1 \
\cb2             cluster_id = \cf9 self\cf0 .identify_context_cluster(\cf9 self\cf0 .context)\cb1 \
\cb2             cluster_memories = \cf9 self\cf0 .context_clusters.get(cluster_id, [])\cb1 \
\
\cb2             \cf3 if\cf0  cluster_memories:\cb1 \
\cb2                 relevant_memories.extend(cluster_memories[\cf5 -3\cf0 :])\cb1 \
\
\cb2             \cf3 if\cf0  \cf8 len\cf0 (relevant_memories) < \cf5 2\cf0 :\cb1 \
\cb2                 \cf3 for\cf0  cid, memories \cf7 in\cf0  \cf9 self\cf0 .context_clusters.items():\cb1 \
\cb2                     \cf3 if\cf0  \cf8 len\cf0 (memories) > \cf5 0\cf0  \cf7 and\cf0  cid \cf7 in\cf0  \cf9 self\cf0 .cluster_performance:\cb1 \
\cb2                         cluster_perf = np.mean(\cf9 self\cf0 .cluster_performance[cid])\cb1 \
\cb2                         \cf3 if\cf0  cluster_perf > \cf5 0.6\cf0 :\cb1 \
\cb2                             relevant_memories.extend(memories[\cf5 -2\cf0 :])\cb1 \
\cb2                             \cf3 if\cf0  \cf8 len\cf0 (relevant_memories) >= \cf5 4\cf0 :\cb1 \
\cb2                                 \cf3 break\cf0 \cb1 \
\
\cb2         \cf6 # Similarity-based search from general memory\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (relevant_memories) < \cf5 3\cf0 :\cb1 \
\cb2             similarity_memories = []\cb1 \
\cb2             \cf3 for\cf0  memory \cf7 in\cf0  \cf9 self\cf0 .episodic_memory[\cf5 -30\cf0 :]:\cb1 \
\cb2                 \cf3 if\cf0  memory.get(\cf4 'context'\cf0 ) \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2                     context_sim = \cf9 self\cf0 .compute_similarity(\cf9 self\cf0 .context, memory[\cf4 'context'\cf0 ])\cb1 \
\cb2                     \cf3 if\cf0  context_sim > \cf9 self\cf0 .memory_similarity_threshold:\cb1 \
\cb2                         emotion_sim = \cf9 self\cf0 .compute_similarity(\cf9 self\cf0 .emotion, memory[\cf4 'emotion_state'\cf0 ])\cb1 \
\cb2                         combined_sim = \cf5 0.7\cf0  * context_sim + \cf5 0.3\cf0  * emotion_sim\cb1 \
\cb2                         similarity_memories.append((combined_sim, memory))\cb1 \
\
\cb2             \cf3 if\cf0  similarity_memories:\cb1 \
\cb2                 similarity_memories.sort(key=\cf7 lambda\cf0  x: x[\cf5 0\cf0 ], reverse=\cf7 True\cf0 )\cb1 \
\cb2                 additional_memories = [mem \cf3 for\cf0  _, mem \cf7 in\cf0  similarity_memories[:\cf8 max\cf0 (\cf5 1\cf0 , \cf5 4\cf0 -\cf8 len\cf0 (relevant_memories))]]\cb1 \
\cb2                 relevant_memories.extend(additional_memories)\cb1 \
\
\cb2         \cf6 # Use high-quality memories if insufficient\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (relevant_memories) < \cf5 2\cf0 :\cb1 \
\cb2             high_quality_memories = [m \cf3 for\cf0  m \cf7 in\cf0  \cf9 self\cf0 .episodic_memory \cf3 if\cf0  m[\cf4 'quality_score'\cf0 ] > \cf5 0.7\cf0 ]\cb1 \
\cb2             \cf3 if\cf0  high_quality_memories:\cb1 \
\cb2                 relevant_memories.extend(high_quality_memories[\cf5 -2\cf0 :])\cb1 \
\
\cb2         \cf3 if\cf0  \cf7 not\cf0  relevant_memories:\cb1 \
\cb2             \cf9 self\cf0 .memory_usage_history.append((\cf7 False\cf0 , \cf9 self\cf0 .prev_reward))\cb1 \
\cb2             \cf3 return\cf0  np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\
\cb2         \cf6 # Memory bias calculation\cf0 \cb1 \
\cb2         bias = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         total_weight = \cf5 0\cf0 \cb1 \
\
\cb2         \cf3 for\cf0  memory \cf7 in\cf0  relevant_memories[\cf5 -5\cf0 :]:\cb1 \
\cb2             action = memory[\cf4 'action'\cf0 ]\cb1 \
\cb2             reward = memory[\cf4 'reward'\cf0 ]\cb1 \
\cb2             quality = memory[\cf4 'quality_score'\cf0 ]\cb1 \
\cb2             weight = quality * \cf9 self\cf0 .current_memory_influence\cb1 \
\
\cb2             \cf3 if\cf0  reward > \cf5 0.6\cf0 :\cb1 \
\cb2                 bias[action] += reward * weight\cb1 \
\cb2             \cf3 elif\cf0  reward < \cf5 0.4\cf0 :\cb1 \
\cb2                 bias[action] -= (\cf5 0.5\cf0  - reward) * weight * \cf5 0.5\cf0 \cb1 \
\
\cb2             total_weight += weight\cb1 \
\
\cb2         \cf3 if\cf0  total_weight > \cf5 0\cf0 :\cb1 \
\cb2             bias = bias / (\cf5 1.0\cf0  + total_weight * \cf5 0.2\cf0 )\cb1 \
\
\cb2         \cf9 self\cf0 .memory_usage_history.append((\cf7 True\cf0 , \cf9 self\cf0 .prev_reward))\cb1 \
\cb2         \cf3 return\cf0  bias\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 emotional_processing\cf0 (\cf9 self\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf4 """Improved emotional processing"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .prev_reward \cf7 is\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .prev_reward = \cf5 0.5\cf0 \cb1 \
\
\cb2         \cf9 self\cf0 .emotion = \cf9 self\cf0 .emotion_decay * \cf9 self\cf0 .emotion\cb1 \
\cb2         \cf9 self\cf0 .reward_history.append(reward)\cb1 \
\cb2         recent_rewards = \cf10 list\cf0 (\cf9 self\cf0 .reward_history)\cb1 \
\
\cb2         current_emotion_updates = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         intensity_factor = \cf5 0.7\cf0 \cb1 \
\
\cb2         \cf6 # Fear\cf0 \cb1 \
\cb2         \cf3 if\cf0  reward < \cf9 self\cf0 .prev_reward - \cf5 0.15\cf0 :\cb1 \
\cb2             fear_strength = \cf8 min\cf0 (\cf5 0.6\cf0 , \cf5 0.15\cf0  + \cf5 0.4\cf0  * \cf8 abs\cf0 (reward - \cf9 self\cf0 .prev_reward))\cb1 \
\cb2             current_emotion_updates[\cf5 0\cf0 ] = fear_strength * intensity_factor\cb1 \
\
\cb2         \cf6 # Joy\cf0 \cb1 \
\cb2         \cf3 if\cf0  reward > \cf5 0.7\cf0 :\cb1 \
\cb2             joy_strength = \cf8 min\cf0 (\cf5 0.7\cf0 , \cf5 0.2\cf0  + \cf5 0.5\cf0  * reward)\cb1 \
\cb2             current_emotion_updates[\cf5 1\cf0 ] = joy_strength * intensity_factor\cb1 \
\
\cb2         \cf6 # Hope\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (recent_rewards) >= \cf5 4\cf0 :\cb1 \
\cb2             recent_trend = np.polyfit(\cf8 range\cf0 (\cf5 4\cf0 ), recent_rewards[\cf5 -4\cf0 :], \cf5 1\cf0 )[\cf5 0\cf0 ]\cb1 \
\cb2             \cf3 if\cf0  recent_trend > \cf5 0.03\cf0 :\cb1 \
\cb2                 hope_strength = \cf8 min\cf0 (\cf5 0.6\cf0 , \cf5 0.15\cf0  + \cf5 0.6\cf0  * recent_trend * \cf5 10\cf0 )\cb1 \
\cb2                 current_emotion_updates[\cf5 2\cf0 ] = hope_strength * intensity_factor\cb1 \
\
\cb2         \cf6 # Sadness\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (recent_rewards) >= \cf5 6\cf0 :\cb1 \
\cb2             avg_recent = np.mean(recent_rewards[\cf5 -6\cf0 :])\cb1 \
\cb2             \cf3 if\cf0  avg_recent < \cf5 0.4\cf0 :\cb1 \
\cb2                 sadness_strength = \cf8 min\cf0 (\cf5 0.5\cf0 , \cf5 0.1\cf0  + \cf5 0.4\cf0  * (\cf5 0.4\cf0  - avg_recent) / \cf5 0.4\cf0 )\cb1 \
\cb2                 current_emotion_updates[\cf5 3\cf0 ] = sadness_strength * intensity_factor\cb1 \
\
\cb2         \cf6 # Curiosity\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .action_history) > \cf5 0\cf0 :\cb1 \
\cb2             action_diversity = \cf8 len\cf0 (\cf10 set\cf0 (\cf9 self\cf0 .action_history)) / \cf8 min\cf0 (\cf8 len\cf0 (\cf9 self\cf0 .action_history), \cf9 self\cf0 .n_actions)\cb1 \
\cb2             recent_performance = np.mean(recent_rewards[\cf5 -3\cf0 :]) \cf3 if\cf0  \cf8 len\cf0 (recent_rewards) >= \cf5 3\cf0  \cf3 else\cf0  \cf5 0.5\cf0 \cb1 \
\
\cb2             \cf3 if\cf0  recent_performance < \cf5 0.6\cf0  \cf7 or\cf0  action_diversity < \cf5 0.8\cf0 :\cb1 \
\cb2                 curiosity_strength = \cf8 min\cf0 (\cf5 0.7\cf0 , \cf5 0.2\cf0  + \cf5 0.3\cf0  * (\cf5 1\cf0  - action_diversity) +\cb1 \
\cb2                                        \cf5 0.2\cf0  * \cf8 max\cf0 (\cf5 0\cf0 , \cf5 0.6\cf0  - recent_performance))\cb1 \
\cb2                 current_emotion_updates[\cf5 4\cf0 ] = curiosity_strength * intensity_factor\cb1 \
\
\cb2         \cf6 # Anger\cf0 \cb1 \
\cb2         expected_improvement = \cf5 0.05\cf0  * \cf9 self\cf0 .time / \cf5 200\cf0 \cb1 \
\cb2         expected_reward = \cf5 0.5\cf0  + expected_improvement\cb1 \
\cb2         \cf3 if\cf0  reward < expected_reward - \cf5 0.2\cf0 :\cb1 \
\cb2             anger_strength = \cf8 min\cf0 (\cf5 0.5\cf0 , \cf5 0.1\cf0  + \cf5 0.4\cf0  * \cf8 abs\cf0 (reward - expected_reward))\cb1 \
\cb2             current_emotion_updates[\cf5 5\cf0 ] = anger_strength * intensity_factor\cb1 \
\
\cb2         \cf6 # Pride\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (recent_rewards) >= \cf5 5\cf0 :\cb1 \
\cb2             success_rate = \cf8 sum\cf0 (r > \cf5 0.7\cf0  \cf3 for\cf0  r \cf7 in\cf0  recent_rewards[\cf5 -5\cf0 :]) / \cf5 5\cf0 \cb1 \
\cb2             \cf3 if\cf0  success_rate > \cf5 0.6\cf0 :\cb1 \
\cb2                 pride_strength = \cf8 min\cf0 (\cf5 0.6\cf0 , \cf5 0.1\cf0  + \cf5 0.4\cf0  * success_rate)\cb1 \
\cb2                 current_emotion_updates[\cf5 6\cf0 ] = pride_strength * intensity_factor\cb1 \
\
\cb2         \cf6 # Shame\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (recent_rewards) >= \cf5 4\cf0 :\cb1 \
\cb2             failure_rate = \cf8 sum\cf0 (r < \cf5 0.3\cf0  \cf3 for\cf0  r \cf7 in\cf0  recent_rewards[\cf5 -4\cf0 :]) / \cf5 4\cf0 \cb1 \
\cb2             \cf3 if\cf0  failure_rate > \cf5 0.5\cf0 :\cb1 \
\cb2                 shame_strength = \cf8 min\cf0 (\cf5 0.4\cf0 , \cf5 0.1\cf0  + \cf5 0.3\cf0  * failure_rate)\cb1 \
\cb2                 current_emotion_updates[\cf5 7\cf0 ] = shame_strength * intensity_factor\cb1 \
\
\cb2         \cf9 self\cf0 .resolve_emotion_conflicts(current_emotion_updates)\cb1 \
\
\cb2         \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf5 8\cf0 ):\cb1 \
\cb2             \cf9 self\cf0 .emotion_momentum[i] = \cf5 0.7\cf0  * \cf9 self\cf0 .emotion_momentum[i] + \cf5 0.3\cf0  * current_emotion_updates[i]\cb1 \
\cb2             emotion_change = \cf5 0.5\cf0  * current_emotion_updates[i] + \cf5 0.5\cf0  * \cf9 self\cf0 .emotion_momentum[i]\cb1 \
\cb2             \cf9 self\cf0 .emotion[i] = \cf5 0.7\cf0  * \cf9 self\cf0 .emotion[i] + \cf5 0.3\cf0  * emotion_change\cb1 \
\
\cb2         \cf9 self\cf0 .normalize_emotions()\cb1 \
\cb2         \cf9 self\cf0 .prev_reward = reward\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 resolve_emotion_conflicts\cf0 (\cf9 self\cf0 , \cf9 emotion_updates\cf0 ):\cb1 \
\cb2         \cf4 """Emotion conflict resolution"""\cf0 \cb1 \
\cb2         conflicting_pairs = [(\cf5 0\cf0 , \cf5 1\cf0 ), (\cf5 2\cf0 , \cf5 3\cf0 ), (\cf5 6\cf0 , \cf5 7\cf0 )]\cb1 \
\
\cb2         \cf3 for\cf0  idx1, idx2 \cf7 in\cf0  conflicting_pairs:\cb1 \
\cb2             \cf3 if\cf0  emotion_updates[idx1] > \cf5 0.5\cf0  \cf7 and\cf0  emotion_updates[idx2] > \cf5 0.5\cf0 :\cb1 \
\cb2                 avg_strength = (emotion_updates[idx1] + emotion_updates[idx2]) / \cf5 2\cf0 \cb1 \
\cb2                 emotion_updates[idx1] = avg_strength * \cf5 0.8\cf0 \cb1 \
\cb2                 emotion_updates[idx2] = avg_strength * \cf5 0.8\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 normalize_emotions\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Emotion normalization"""\cf0 \cb1 \
\cb2         \cf9 self\cf0 .emotion = np.clip(\cf9 self\cf0 .emotion, \cf5 0.0\cf0 , \cf5 1.0\cf0 )\cb1 \
\cb2         total_emotion_energy = np.\cf8 sum\cf0 (\cf9 self\cf0 .emotion)\cb1 \
\
\cb2         \cf3 if\cf0  total_emotion_energy > \cf9 self\cf0 .max_total_emotion_energy:\cb1 \
\cb2             \cf9 self\cf0 .emotion = \cf9 self\cf0 .emotion * (\cf9 self\cf0 .max_total_emotion_energy / total_emotion_energy)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 adaptive_eta_adjustment\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Dynamic eta adjustment"""\cf0 \cb1 \
\cb2         \cf9 self\cf0 .eta_adaptation_counter += \cf5 1\cf0 \cb1 \
\
\cb2         \cf3 if\cf0  \cf9 self\cf0 .eta_adaptation_counter >= \cf5 20\cf0  \cf7 and\cf0  \cf8 len\cf0 (\cf9 self\cf0 .performance_tracker) >= \cf5 10\cf0 :\cb1 \
\cb2             recent_performance = np.mean(\cf10 list\cf0 (\cf9 self\cf0 .performance_tracker)[\cf5 -10\cf0 :])\cb1 \
\cb2             performance_variance = np.var(\cf10 list\cf0 (\cf9 self\cf0 .performance_tracker)[\cf5 -10\cf0 :])\cb1 \
\
\cb2             \cf3 if\cf0  recent_performance > \cf5 0.75\cf0 :\cb1 \
\cb2                 \cf9 self\cf0 .eta = \cf9 self\cf0 .base_eta * \cf5 0.6\cf0 \cb1 \
\cb2             \cf3 elif\cf0  recent_performance > \cf5 0.6\cf0 :\cb1 \
\cb2                 \cf9 self\cf0 .eta = \cf9 self\cf0 .base_eta * \cf5 0.8\cf0 \cb1 \
\cb2             \cf3 elif\cf0  recent_performance < \cf5 0.4\cf0 :\cb1 \
\cb2                 \cf9 self\cf0 .eta = \cf9 self\cf0 .base_eta * \cf5 1.3\cf0 \cb1 \
\cb2             \cf3 elif\cf0  performance_variance > \cf5 0.05\cf0 :\cb1 \
\cb2                 \cf9 self\cf0 .eta = \cf9 self\cf0 .base_eta * \cf5 1.1\cf0 \cb1 \
\cb2             \cf3 else\cf0 :\cb1 \
\cb2                 \cf9 self\cf0 .eta = \cf9 self\cf0 .base_eta\cb1 \
\
\cb2             \cf9 self\cf0 .eta = np.clip(\cf9 self\cf0 .eta, \cf9 self\cf0 .base_eta * \cf5 0.3\cf0 , \cf9 self\cf0 .base_eta * \cf5 1.5\cf0 )\cb1 \
\cb2             \cf9 self\cf0 .eta_adaptation_counter = \cf5 0\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 select_top_emotions\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Core emotion selection"""\cf0 \cb1 \
\cb2         emotion_indices = np.argsort(\cf9 self\cf0 .emotion)[-\cf9 self\cf0 .top_k:]\cb1 \
\cb2         selective_emotions = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         selective_emotions[emotion_indices] = \cf9 self\cf0 .emotion[emotion_indices]\cb1 \
\cb2         \cf3 return\cf0  selective_emotions\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_direct_emotion_influence\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Direct emotion-action mapping"""\cf0 \cb1 \
\cb2         influences = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         selective_emotions = \cf9 self\cf0 .select_top_emotions()\cb1 \
\
\cb2         \cf6 # Fear\cf0 \cb1 \
\cb2         \cf3 if\cf0  selective_emotions[\cf5 0\cf0 ] > \cf5 0.1\cf0 :\cb1 \
\cb2             fear_level = selective_emotions[\cf5 0\cf0 ]\cb1 \
\cb2             min_q = np.\cf8 min\cf0 (\cf9 self\cf0 .q_values)\cb1 \
\cb2             max_q = np.\cf8 max\cf0 (\cf9 self\cf0 .q_values)\cb1 \
\cb2             q_range = max_q - min_q + \cf5 0.001\cf0 \cb1 \
\
\cb2             \cf3 for\cf0  action \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions):\cb1 \
\cb2                 relative_badness = (max_q - \cf9 self\cf0 .q_values[action]) / q_range\cb1 \
\cb2                 influences[action] -= fear_level * \cf5 0.4\cf0  * relative_badness\cb1 \
\
\cb2         \cf6 # Joy\cf0 \cb1 \
\cb2         \cf3 if\cf0  selective_emotions[\cf5 1\cf0 ] > \cf5 0.1\cf0 :\cb1 \
\cb2             joy_level = selective_emotions[\cf5 1\cf0 ]\cb1 \
\cb2             min_q = np.\cf8 min\cf0 (\cf9 self\cf0 .q_values)\cb1 \
\cb2             max_q = np.\cf8 max\cf0 (\cf9 self\cf0 .q_values)\cb1 \
\cb2             q_range = max_q - min_q + \cf5 0.001\cf0 \cb1 \
\
\cb2             \cf3 for\cf0  action \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions):\cb1 \
\cb2                 relative_goodness = (\cf9 self\cf0 .q_values[action] - min_q) / q_range\cb1 \
\cb2                 influences[action] += joy_level * \cf5 0.4\cf0  * relative_goodness\cb1 \
\
\cb2         \cf6 # Curiosity\cf0 \cb1 \
\cb2         \cf3 if\cf0  selective_emotions[\cf5 4\cf0 ] > \cf5 0.1\cf0 :\cb1 \
\cb2             curiosity_level = selective_emotions[\cf5 4\cf0 ]\cb1 \
\cb2             min_count = np.\cf8 min\cf0 (\cf9 self\cf0 .action_counts)\cb1 \
\cb2             max_count = np.\cf8 max\cf0 (\cf9 self\cf0 .action_counts) + \cf5 1\cf0 \cb1 \
\
\cb2             \cf3 for\cf0  action \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions):\cb1 \
\cb2                 exploration_factor = (max_count - \cf9 self\cf0 .action_counts[action]) / max_count\cb1 \
\cb2                 influences[action] += curiosity_level * \cf5 0.4\cf0  * exploration_factor\cb1 \
\
\cb2         \cf3 return\cf0  influences\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 hippocampal_neurogenesis\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Hippocampal neurogenesis simulation"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .time % \cf9 self\cf0 .neurogenesis_cycle == \cf5 0\cf0 :\cb1 \
\cb2             emotion_intensity = np.linalg.norm(\cf9 self\cf0 .emotion)\cb1 \
\cb2             base_boost = \cf5 0.2\cf0 \cb1 \
\
\cb2             \cf3 if\cf0  emotion_intensity > \cf5 0.7\cf0 :\cb1 \
\cb2                 \cf9 self\cf0 .learning_boost = base_boost * \cf5 1.3\cf0 \cb1 \
\cb2             \cf3 elif\cf0  \cf9 self\cf0 .emotion[\cf5 4\cf0 ] > \cf5 0.6\cf0 :\cb1 \
\cb2                 \cf9 self\cf0 .learning_boost = base_boost * \cf5 1.2\cf0 \cb1 \
\cb2             \cf3 else\cf0 :\cb1 \
\cb2                 \cf9 self\cf0 .learning_boost = base_boost\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .learning_boost = \cf8 max\cf0 (\cf5 0\cf0 , \cf9 self\cf0 .learning_boost - \cf5 0.01\cf0 )\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 prefrontal_modulation\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Prefrontal cortex modulation"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .episodic_memory) < \cf5 5\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.5\cf0 \cb1 \
\
\cb2         recent_rewards = [mem[\cf4 'reward'\cf0 ] \cf3 for\cf0  mem \cf7 in\cf0  \cf9 self\cf0 .episodic_memory[\cf5 -10\cf0 :]]\cb1 \
\cb2         reward_stability = \cf5 1.0\cf0  - np.std(recent_rewards) \cf3 if\cf0  recent_rewards \cf3 else\cf0  \cf5 0.5\cf0 \cb1 \
\
\cb2         context_change = np.mean(\cf9 self\cf0 .recent_context_changes) \cf3 if\cf0  \cf9 self\cf0 .recent_context_changes \cf3 else\cf0  \cf5 0\cf0 \cb1 \
\cb2         emotion_volatility = np.std(\cf9 self\cf0 .emotion) \cf3 if\cf0  np.\cf8 sum\cf0 (\cf9 self\cf0 .emotion) > \cf5 0\cf0  \cf3 else\cf0  \cf5 0\cf0 \cb1 \
\cb2         emotion_stability = \cf5 1.0\cf0  - \cf8 min\cf0 (emotion_volatility, \cf5 1.0\cf0 )\cb1 \
\
\cb2         performance_trend = \cf5 0.5\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .performance_tracker) >= \cf5 5\cf0 :\cb1 \
\cb2             recent_performance = \cf10 list\cf0 (\cf9 self\cf0 .performance_tracker)[\cf5 -5\cf0 :]\cb1 \
\cb2             performance_trend = np.mean(recent_performance)\cb1 \
\
\cb2         stability = (\cf5 0.35\cf0  * reward_stability + \cf5 0.25\cf0  * emotion_stability -\cb1 \
\cb2                     \cf5 0.15\cf0  * context_change + \cf5 0.25\cf0  * performance_trend)\cb1 \
\
\cb2         \cf3 return\cf0  np.clip(stability, \cf5 0.1\cf0 , \cf5 0.9\cf0 )\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_uncertainty_bonus\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Uncertainty-based exploration bonus"""\cf0 \cb1 \
\cb2         uncertainty = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         total_experiences = \cf9 self\cf0 .time + \cf5 1\cf0 \cb1 \
\
\cb2         \cf3 for\cf0  a \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions):\cb1 \
\cb2             action_count = \cf9 self\cf0 .action_counts[a] + \cf5 1\cf0 \cb1 \
\cb2             count_uncertainty = np.sqrt(np.log(total_experiences) / action_count)\cb1 \
\
\cb2             \cf6 # Collect reward history for this action\cf0 \cb1 \
\cb2             action_rewards = [mem[\cf4 'reward'\cf0 ] \cf3 for\cf0  mem \cf7 in\cf0  \cf9 self\cf0 .episodic_memory \cf3 if\cf0  mem[\cf4 'action'\cf0 ] == a]\cb1 \
\
\cb2             \cf6 # Calculate reward standard deviation\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf8 len\cf0 (action_rewards) > \cf5 1\cf0 :\cb1 \
\cb2                 reward_std = np.std(action_rewards)\cb1 \
\cb2             \cf3 else\cf0 :\cb1 \
\cb2                 reward_std = \cf5 0.5\cf0   \cf6 # Default value\cf0 \cb1 \
\
\cb2             \cf6 # Emotion factor\cf0 \cb1 \
\cb2             emotion_factor = \cf5 1.0\cf0 \cb1 \
\cb2             \cf3 if\cf0  \cf9 self\cf0 .emotion[\cf5 4\cf0 ] > \cf5 0.6\cf0 :  \cf6 # Curiosity\cf0 \cb1 \
\cb2                 emotion_factor = \cf5 1.3\cf0 \cb1 \
\cb2             \cf3 elif\cf0  \cf9 self\cf0 .emotion[\cf5 0\cf0 ] > \cf5 0.6\cf0 :  \cf6 # Fear\cf0 \cb1 \
\cb2                 emotion_factor = \cf5 0.7\cf0 \cb1 \
\
\cb2             uncertainty[a] = count_uncertainty * (\cf5 1\cf0  + reward_std) * emotion_factor\cb1 \
\
\cb2         \cf3 return\cf0  uncertainty * \cf5 0.3\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 build_context\cf0 (\cf9 self\cf0 , \cf9 norm_t\cf0 , \cf9 phase_id\cf0 ):\cb1 \
\cb2         \cf4 """Enhanced context construction"""\cf0 \cb1 \
\cb2         grid_patterns = []\cb1 \
\
\cb2         \cf3 for\cf0  scale \cf7 in\cf0  [\cf5 1.0\cf0 , \cf5 3.0\cf0 , \cf5 6.0\cf0 ]:\cb1 \
\cb2             \cf3 for\cf0  offset \cf7 in\cf0  [\cf5 0.0\cf0 , \cf5 0.33\cf0 , \cf5 0.67\cf0 ]:\cb1 \
\cb2                 grid_patterns.append(np.sin(\cf5 2\cf0 *np.pi * (norm_t * scale + offset)))\cb1 \
\cb2                 grid_patterns.append(np.cos(\cf5 2\cf0 *np.pi * (norm_t * scale + offset)))\cb1 \
\
\cb2         time_cells = [\cb1 \
\cb2             np.exp(-(norm_t - \cf5 0.25\cf0 )**\cf5 2\cf0  / \cf5 0.15\cf0 ),\cb1 \
\cb2             np.exp(-(norm_t - \cf5 0.5\cf0 )**\cf5 2\cf0  / \cf5 0.15\cf0 ),\cb1 \
\cb2             np.exp(-(norm_t - \cf5 0.75\cf0 )**\cf5 2\cf0  / \cf5 0.15\cf0 )\cb1 \
\cb2         ]\cb1 \
\
\cb2         emotion_context = \cf9 self\cf0 .emotion * \cf5 1.5\cf0 \cb1 \
\
\cb2         \cf3 return\cf0  np.concatenate([grid_patterns, time_cells, [phase_id], emotion_context])\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 update_context\cf0 (\cf9 self\cf0 , \cf9 norm_t\cf0 , \cf9 phase_id\cf0 ):\cb1 \
\cb2         \cf4 """Context update and change tracking"""\cf0 \cb1 \
\cb2         \cf9 self\cf0 .prev_context = \cf9 self\cf0 .context.copy() \cf3 if\cf0  \cf9 self\cf0 .context \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0  \cf3 else\cf0  \cf7 None\cf0 \cb1 \
\cb2         \cf9 self\cf0 .context = \cf9 self\cf0 .build_context(norm_t, phase_id)\cb1 \
\
\cb2         \cf3 if\cf0  \cf9 self\cf0 .prev_context \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2             min_len = \cf8 min\cf0 (\cf8 len\cf0 (\cf9 self\cf0 .context), \cf8 len\cf0 (\cf9 self\cf0 .prev_context))\cb1 \
\cb2             context_change = np.linalg.norm(\cf9 self\cf0 .context[:min_len] - \cf9 self\cf0 .prev_context[:min_len])\cb1 \
\cb2             \cf9 self\cf0 .recent_context_changes.append(context_change)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 select_action\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Integrated action selection"""\cf0 \cb1 \
\cb2         \cf9 self\cf0 .hippocampal_neurogenesis()\cb1 \
\cb2         \cf9 self\cf0 .adaptive_eta_adjustment()\cb1 \
\cb2         \cf9 self\cf0 .adaptive_memory_control()\cb1 \
\
\cb2         pfc_stability = \cf9 self\cf0 .prefrontal_modulation()\cb1 \
\
\cb2         \cf6 # Emotion-based exploration adjustment\cf0 \cb1 \
\cb2         curiosity_boost = \cf9 self\cf0 .emotion[\cf5 4\cf0 ] * \cf5 0.2\cf0 \cb1 \
\cb2         fear_penalty = \cf9 self\cf0 .emotion[\cf5 0\cf0 ] * \cf5 0.25\cf0 \cb1 \
\cb2         joy_exploitation = \cf9 self\cf0 .emotion[\cf5 1\cf0 ] * \cf5 0.15\cf0 \cb1 \
\
\cb2         adaptive_epsilon = \cf9 self\cf0 .epsilon * (\cf5 1\cf0  - pfc_stability)\cb1 \
\cb2         adaptive_epsilon = np.clip(\cb1 \
\cb2             adaptive_epsilon + curiosity_boost - fear_penalty - joy_exploitation,\cb1 \
\cb2             \cf5 0.01\cf0 , \cf5 0.2\cf0 \cb1 \
\cb2         )\cb1 \
\
\cb2         \cf6 # Exploration action\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .rng.rand() < adaptive_epsilon:\cb1 \
\cb2             \cf3 if\cf0  \cf9 self\cf0 .emotion[\cf5 4\cf0 ] > \cf5 0.6\cf0 :\cb1 \
\cb2                 action_probs = \cf5 1.0\cf0  / (\cf9 self\cf0 .action_counts + \cf5 0.1\cf0 )\cb1 \
\cb2                 action_probs = action_probs / np.\cf8 sum\cf0 (action_probs)\cb1 \
\cb2                 \cf3 return\cf0  \cf9 self\cf0 .rng.choice(\cf9 self\cf0 .n_actions, p=action_probs)\cb1 \
\cb2             \cf3 elif\cf0  \cf9 self\cf0 .emotion[\cf5 5\cf0 ] > \cf5 0.6\cf0 :\cb1 \
\cb2                 \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .action_history) > \cf5 0\cf0 :\cb1 \
\cb2                     recent_action = \cf9 self\cf0 .action_history[\cf5 -1\cf0 ]\cb1 \
\cb2                     available_actions = [a \cf3 for\cf0  a \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions) \cf3 if\cf0  a != recent_action]\cb1 \
\cb2                     \cf3 if\cf0  available_actions:\cb1 \
\cb2                         \cf3 return\cf0  \cf9 self\cf0 .rng.choice(available_actions)\cb1 \
\cb2             \cf3 return\cf0  \cf9 self\cf0 .rng.choice(\cf9 self\cf0 .n_actions)\cb1 \
\
\cb2         \cf6 # Exploitation action\cf0 \cb1 \
\cb2         emotion_influences = \cf9 self\cf0 .compute_direct_emotion_influence()\cb1 \
\cb2         memory_bias = \cf9 self\cf0 .compute_adaptive_memory_bias()\cb1 \
\cb2         uncertainty_bonus = \cf9 self\cf0 .compute_uncertainty_bonus()\cb1 \
\
\cb2         final_values = (\cf9 self\cf0 .q_values +\cb1 \
\cb2                        \cf9 self\cf0 .eta * emotion_influences +\cb1 \
\cb2                        memory_bias +\cb1 \
\cb2                        uncertainty_bonus +\cb1 \
\cb2                        \cf9 self\cf0 .xi * \cf9 self\cf0 .rng.randn(\cf9 self\cf0 .n_actions))\cb1 \
\
\cb2         \cf3 return\cf0  np.argmax(final_values)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 update_dopamine_learning\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf4 """Dopamine-based learning"""\cf0 \cb1 \
\cb2         prediction_error = reward - \cf9 self\cf0 .q_values[action]\cb1 \
\cb2         emotion_intensity = np.linalg.norm(\cf9 self\cf0 .emotion)\cb1 \
\
\cb2         base_alpha = \cf9 self\cf0 .alpha\cb1 \
\cb2         emotional_boost = \cf5 1.0\cf0  + emotion_intensity * \cf5 0.3\cf0 \cb1 \
\
\cb2         \cf3 if\cf0  \cf8 abs\cf0 (prediction_error) > \cf5 0.3\cf0 :\cb1 \
\cb2             adaptive_alpha = base_alpha * \cf5 1.3\cf0  * emotional_boost\cb1 \
\cb2         \cf3 elif\cf0  \cf8 abs\cf0 (prediction_error) > \cf5 0.15\cf0 :\cb1 \
\cb2             adaptive_alpha = base_alpha * \cf5 1.1\cf0  * emotional_boost\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             adaptive_alpha = base_alpha * emotional_boost\cb1 \
\
\cb2         adaptive_alpha += \cf9 self\cf0 .learning_boost\cb1 \
\
\cb2         \cf3 if\cf0  \cf9 self\cf0 .emotion[\cf5 4\cf0 ] > \cf5 0.6\cf0 :\cb1 \
\cb2             adaptive_alpha *= \cf5 1.2\cf0 \cb1 \
\
\cb2         \cf3 if\cf0  \cf9 self\cf0 .emotion[\cf5 0\cf0 ] > \cf5 0.7\cf0 :\cb1 \
\cb2             adaptive_alpha *= \cf5 0.85\cf0 \cb1 \
\
\cb2         \cf9 self\cf0 .q_values[action] += adaptive_alpha * prediction_error\cb1 \
\
\cb2         habit_strength = \cf8 min\cf0 (\cf5 0.3\cf0 , \cf9 self\cf0 .habit_strength_factor * \cf9 self\cf0 .action_counts[action])\cb1 \
\cb2         \cf3 if\cf0  reward > \cf5 0.65\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .q_values[action] += habit_strength * reward\cb1 \
\
\cb2         \cf9 self\cf0 .performance_tracker.append(reward)\cb1 \
\cb2         \cf3 return\cf0  prediction_error\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 update\cf0 (\cf9 self\cf0 , *\cf9 args\cf0 , **\cf9 kwargs\cf0 ):\cb1 \
\cb2         \cf4 """Integrated update"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (args) == \cf5 2\cf0 :\cb1 \
\cb2             action, reward = args\cb1 \
\cb2             context = \cf7 None\cf0 \cb1 \
\cb2         \cf3 elif\cf0  \cf8 len\cf0 (args) == \cf5 3\cf0 :\cb1 \
\cb2             context, action, reward = args\cb1 \
\cb2             \cf9 self\cf0 .context = context\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             action, reward = args[\cf5 0\cf0 ], args[\cf5 1\cf0 ]\cb1 \
\cb2             context = \cf7 None\cf0 \cb1 \
\
\cb2         \cf9 self\cf0 .hippocampal_neurogenesis()\cb1 \
\cb2         prediction_error = \cf9 self\cf0 .update_dopamine_learning(action, reward)\cb1 \
\cb2         \cf9 self\cf0 .emotional_processing(reward)\cb1 \
\cb2         \cf9 self\cf0 .store_adaptive_memory(action, reward, prediction_error)\cb1 \
\
\cb2         \cf9 self\cf0 .action_history.append(action)\cb1 \
\cb2         \cf9 self\cf0 .action_counts[action] += \cf5 1\cf0 \cb1 \
\cb2         \cf9 self\cf0 .time += \cf5 1\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_similarity\cf0 (\cf9 self\cf0 , \cf9 c1\cf0 , \cf9 c2\cf0 ):\cb1 \
\cb2         \cf4 """Context similarity calculation"""\cf0 \cb1 \
\cb2         \cf3 if\cf0  \cf7 not\cf0  \cf8 isinstance\cf0 (c1, np.ndarray) \cf7 or\cf0  \cf7 not\cf0  \cf8 isinstance\cf0 (c2, np.ndarray):\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.0\cf0 \cb1 \
\
\cb2         min_len = \cf8 min\cf0 (\cf8 len\cf0 (c1), \cf8 len\cf0 (c2))\cb1 \
\cb2         \cf3 if\cf0  min_len == \cf5 0\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.0\cf0 \cb1 \
\
\cb2         c1_part, c2_part = c1[:min_len], c2[:min_len]\cb1 \
\
\cb2         \cf3 if\cf0  np.linalg.norm(c1_part) == \cf5 0\cf0  \cf7 or\cf0  np.linalg.norm(c2_part) == \cf5 0\cf0 :\cb1 \
\cb2             \cf3 return\cf0  \cf5 0.0\cf0 \cb1 \
\
\cb2         \cf3 return\cf0  np.dot(c1_part, c2_part) / (np.linalg.norm(c1_part) * np.linalg.norm(c2_part))\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 reset\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Complete reset"""\cf0 \cb1 \
\cb2         \cf9 self\cf0 .q_values = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         \cf9 self\cf0 .emotion = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .emotion_momentum = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .action_counts = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         \cf9 self\cf0 .time = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .prev_reward = \cf5 0.5\cf0 \cb1 \
\cb2         \cf9 self\cf0 .context = \cf7 None\cf0 \cb1 \
\cb2         \cf9 self\cf0 .eta = \cf9 self\cf0 .base_eta\cb1 \
\cb2         \cf9 self\cf0 .learning_boost = \cf5 0.2\cf0 \cb1 \
\cb2         \cf9 self\cf0 .successful_emotion_patterns = \{\}\cb1 \
\
\cb2         \cf6 # Universal memory system reset\cf0 \cb1 \
\cb2         \cf9 self\cf0 .memory_effectiveness_tracker.clear()\cb1 \
\cb2         \cf9 self\cf0 .environment_stability_tracker.clear()\cb1 \
\cb2         \cf9 self\cf0 .memory_activation_level = \cf5 1.0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .memory_usage_history.clear()\cb1 \
\cb2         \cf9 self\cf0 .change_detection_window.clear()\cb1 \
\cb2         \cf9 self\cf0 .stable_performance_counter = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .memory_cooldown = \cf5 0\cf0 \cb1 \
\cb2         \cf9 self\cf0 .context_clusters.clear()\cb1 \
\cb2         \cf9 self\cf0 .cluster_performance.clear()\cb1 \
\
\cb2         \cf9 self\cf0 .performance_tracker.clear()\cb1 \
\cb2         \cf9 self\cf0 .recent_context_changes.clear()\cb1 \
\cb2         \cf9 self\cf0 .action_history.clear()\cb1 \
\cb2         \cf9 self\cf0 .reward_history.clear()\cb1 \
\cb2         \cf9 self\cf0 .episodic_memory.clear()\cb1 \
\cb2         \cf9 self\cf0 .emotion_action_history.clear()\cb1 \
\cb2         \cf9 self\cf0 .eta_adaptation_counter = \cf5 0\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 get_memory_status\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Universal memory system status"""\cf0 \cb1 \
\cb2         \cf3 return\cf0  \{\cb1 \
\cb2             \cf4 'memory_activation_level'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .memory_activation_level, \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'environment_stability'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .assess_environment_stability(), \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'memory_effectiveness'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .evaluate_memory_effectiveness(), \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'memory_need_score'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .calculate_memory_need(), \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'current_memory_threshold'\cf0 : \cf8 round\cf0 (\cf8 getattr\cf0 (\cf9 self\cf0 , \cf4 'current_memory_threshold'\cf0 , \cf9 self\cf0 .base_memory_threshold), \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'current_memory_influence'\cf0 : \cf8 round\cf0 (\cf8 getattr\cf0 (\cf9 self\cf0 , \cf4 'current_memory_influence'\cf0 , \cf9 self\cf0 .base_memory_influence), \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'memory_count'\cf0 : \cf8 len\cf0 (\cf9 self\cf0 .episodic_memory),\cb1 \
\cb2             \cf4 'high_quality_memories'\cf0 : \cf8 len\cf0 ([m \cf3 for\cf0  m \cf7 in\cf0  \cf9 self\cf0 .episodic_memory \cf3 if\cf0  m[\cf4 'quality_score'\cf0 ] > \cf5 0.5\cf0 ]),\cb1 \
\cb2             \cf4 'context_clusters'\cf0 : \cf8 len\cf0 (\cf9 self\cf0 .context_clusters),\cb1 \
\cb2             \cf4 'memory_cooldown'\cf0 : \cf9 self\cf0 .memory_cooldown,\cb1 \
\cb2             \cf4 'performance_trend'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .analyze_performance_trend(), \cf5 3\cf0 ) \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .performance_tracker) >= \cf5 10\cf0  \cf3 else\cf0  \cf5 0.0\cf0 \cb1 \
\cb2         \}\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 get_emotion_summary\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Current state summary"""\cf0 \cb1 \
\cb2         selected_emotions = \cf9 self\cf0 .select_top_emotions()\cb1 \
\
\cb2         summary = \{\cb1 \
\cb2             \cf4 'current_eta'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .eta, \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'eta_ratio'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .eta / \cf9 self\cf0 .base_eta, \cf5 2\cf0 ),\cb1 \
\cb2             \cf4 'total_emotion_energy'\cf0 : \cf8 round\cf0 (np.\cf8 sum\cf0 (\cf9 self\cf0 .emotion), \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'selected_emotions'\cf0 : \{\},\cb1 \
\cb2             \cf4 'all_emotions'\cf0 : \{\},\cb1 \
\cb2             \cf4 'memory_status'\cf0 : \cf9 self\cf0 .get_memory_status(),\cb1 \
\cb2             \cf4 'learning_boost'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .learning_boost, \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'pfc_stability'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .prefrontal_modulation(), \cf5 3\cf0 ) \cf3 if\cf0  \cf8 len\cf0 (\cf9 self\cf0 .episodic_memory) >= \cf5 5\cf0  \cf3 else\cf0  \cf5 0.5\cf0 ,\cb1 \
\cb2             \cf4 'neurogenesis_cycle'\cf0 : \cf9 self\cf0 .time % \cf9 self\cf0 .neurogenesis_cycle,\cb1 \
\cb2             \cf4 'successful_patterns'\cf0 : \cf8 len\cf0 (\cf9 self\cf0 .successful_emotion_patterns)\cb1 \
\cb2         \}\cb1 \
\
\cb2         \cf3 for\cf0  i, name \cf7 in\cf0  \cf8 enumerate\cf0 (\cf9 self\cf0 .emotion_names):\cb1 \
\cb2             \cf3 if\cf0  selected_emotions[i] > \cf5 0.1\cf0 :\cb1 \
\cb2                 summary[\cf4 'selected_emotions'\cf0 ][name] = \cf8 round\cf0 (\cf10 float\cf0 (selected_emotions[i]), \cf5 3\cf0 )\cb1 \
\
\cb2         \cf3 for\cf0  i, name \cf7 in\cf0  \cf8 enumerate\cf0 (\cf9 self\cf0 .emotion_names):\cb1 \
\cb2             \cf3 if\cf0  \cf9 self\cf0 .emotion[i] > \cf5 0.1\cf0 :\cb1 \
\cb2                 summary[\cf4 'all_emotions'\cf0 ][name] = \cf8 round\cf0 (\cf10 float\cf0 (\cf9 self\cf0 .emotion[i]), \cf5 3\cf0 )\cb1 \
\
\cb2         \cf3 return\cf0  summary\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 get_detailed_decision_breakdown\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Detailed decision process analysis"""\cf0 \cb1 \
\cb2         breakdown = \{\cb1 \
\cb2             \cf4 'q_values'\cf0 : [\cf8 round\cf0 (q, \cf5 3\cf0 ) \cf3 for\cf0  q \cf7 in\cf0  \cf9 self\cf0 .q_values],\cb1 \
\cb2             \cf4 'emotion_influences'\cf0 : [\cf8 round\cf0 (inf, \cf5 3\cf0 ) \cf3 for\cf0  inf \cf7 in\cf0  \cf9 self\cf0 .compute_direct_emotion_influence()],\cb1 \
\cb2             \cf4 'memory_bias'\cf0 : [\cf8 round\cf0 (mb, \cf5 3\cf0 ) \cf3 for\cf0  mb \cf7 in\cf0  \cf9 self\cf0 .compute_adaptive_memory_bias()],\cb1 \
\cb2             \cf4 'uncertainty_bonus'\cf0 : [\cf8 round\cf0 (ub, \cf5 3\cf0 ) \cf3 for\cf0  ub \cf7 in\cf0  \cf9 self\cf0 .compute_uncertainty_bonus()],\cb1 \
\cb2             \cf4 'pfc_stability'\cf0 : \cf8 round\cf0 (\cf9 self\cf0 .prefrontal_modulation(), \cf5 3\cf0 ),\cb1 \
\cb2             \cf4 'current_emotions'\cf0 : \{name: \cf8 round\cf0 (\cf10 float\cf0 (\cf9 self\cf0 .emotion[i]), \cf5 3\cf0 )\cb1 \
\cb2                                \cf3 for\cf0  i, name \cf7 in\cf0  \cf8 enumerate\cf0 (\cf9 self\cf0 .emotion_names)\cb1 \
\cb2                                \cf3 if\cf0  \cf9 self\cf0 .emotion[i] > \cf5 0.1\cf0 \},\cb1 \
\cb2             \cf4 'memory_status'\cf0 : \cf9 self\cf0 .get_memory_status()\cb1 \
\cb2         \}\cb1 \
\cb2         \cf3 return\cf0  breakdown\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 get_dominant_emotion\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf4 """Return the strongest emotion and its intensity"""\cf0 \cb1 \
\cb2         max_idx = np.argmax(\cf9 self\cf0 .emotion)\cb1 \
\cb2         \cf3 return\cf0  \cf9 self\cf0 .emotion_names[max_idx], \cf9 self\cf0 .emotion[max_idx]\cb1 \
\
\
\cf6 \cb2 # Ablation Study classes (maintaining backward compatibility)\cf0 \cb1 \
\cf7 \cb2 class\cf0  \cf10 ECIA_NoEmotion\cf0 (\cf10 ECIA\cf0 ):\cb1 \
\cb2     \cf4 """Emotion system removal"""\cf0 \cb1 \
\cb2     \cf7 def\cf0  \cf8 emotional_processing\cf0 (\cf9 self\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .prev_reward \cf7 is\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .prev_reward = \cf5 0.5\cf0 \cb1 \
\cb2         \cf9 self\cf0 .emotion = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .prev_reward = reward\cb1 \
\
\
\cf7 \cb2 class\cf0  \cf10 ECIA_NoMemory\cf0 (\cf10 ECIA\cf0 ):\cb1 \
\cb2     \cf4 """Memory system removal"""\cf0 \cb1 \
\cb2     \cf7 def\cf0  \cf8 store_adaptive_memory\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 , \cf9 prediction_error\cf0 ):\cb1 \
\cb2         \cf3 pass\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_adaptive_memory_bias\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf3 return\cf0  np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\
\
\cf7 \cb2 class\cf0  \cf10 ECIA_NoDopamine\cf0 (\cf10 ECIA\cf0 ):\cb1 \
\cb2     \cf4 """Dopamine adaptive learning removal"""\cf0 \cb1 \
\cb2     \cf7 def\cf0  \cf8 update_dopamine_learning\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         prediction_error = reward - \cf9 self\cf0 .q_values[action]\cb1 \
\cb2         \cf9 self\cf0 .q_values[action] += \cf5 0.1\cf0  * prediction_error\cb1 \
\cb2         \cf9 self\cf0 .performance_tracker.append(reward)\cb1 \
\cb2         \cf3 return\cf0  prediction_error\cb1 \
\
\
\cf7 \cb2 class\cf0  \cf10 ECIA_SingleCore\cf0 (\cf10 ECIA\cf0 ):\cb1 \
\cb2     \cf4 """Single core version (already default is single core)"""\cf0 \cb1 \
\cb2     \cf3 pass\cf0 \cb1 \
\
\
\cf7 \cb2 class\cf0  \cf10 ECIA_NoDopamine_NoMemory\cf0 (\cf10 ECIA\cf0 ):\cb1 \
\cb2     \cf4 """Dopamine + Memory removal"""\cf0 \cb1 \
\cb2     \cf7 def\cf0  \cf8 update_dopamine_learning\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         prediction_error = reward - \cf9 self\cf0 .q_values[action]\cb1 \
\cb2         \cf9 self\cf0 .q_values[action] += \cf5 0.1\cf0  * prediction_error\cb1 \
\cb2         \cf9 self\cf0 .performance_tracker.append(reward)\cb1 \
\cb2         \cf3 return\cf0  prediction_error\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 store_adaptive_memory\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 , \cf9 prediction_error\cf0 ):\cb1 \
\cb2         \cf3 pass\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_adaptive_memory_bias\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf3 return\cf0  np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\
\
\cf7 \cb2 class\cf0  \cf10 ECIA_NoDopamine_NoEmotion\cf0 (\cf10 ECIA\cf0 ):\cb1 \
\cb2     \cf4 """Dopamine + Emotion removal"""\cf0 \cb1 \
\cb2     \cf7 def\cf0  \cf8 update_dopamine_learning\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         prediction_error = reward - \cf9 self\cf0 .q_values[action]\cb1 \
\cb2         \cf9 self\cf0 .q_values[action] += \cf5 0.1\cf0  * prediction_error\cb1 \
\cb2         \cf9 self\cf0 .performance_tracker.append(reward)\cb1 \
\cb2         \cf3 return\cf0  prediction_error\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 emotional_processing\cf0 (\cf9 self\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .prev_reward \cf7 is\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .prev_reward = \cf5 0.5\cf0 \cb1 \
\cb2         \cf9 self\cf0 .emotion = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .prev_reward = reward\cb1 \
\
\
\cf7 \cb2 class\cf0  \cf10 ECIA_NoMemory_NoEmotion\cf0 (\cf10 ECIA\cf0 ):\cb1 \
\cb2     \cf4 """Memory + Emotion removal"""\cf0 \cb1 \
\cb2     \cf7 def\cf0  \cf8 store_adaptive_memory\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 , \cf9 prediction_error\cf0 ):\cb1 \
\cb2         \cf3 pass\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_adaptive_memory_bias\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf3 return\cf0  np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 emotional_processing\cf0 (\cf9 self\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .prev_reward \cf7 is\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .prev_reward = \cf5 0.5\cf0 \cb1 \
\cb2         \cf9 self\cf0 .emotion = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .prev_reward = reward\cb1 \
\
\
\cf7 \cb2 class\cf0  \cf10 ECIA_NoAll_Components\cf0 (\cf10 ECIA\cf0 ):\cb1 \
\cb2     \cf4 """All advanced components removal"""\cf0 \cb1 \
\cb2     \cf7 def\cf0  \cf8 update_dopamine_learning\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         prediction_error = reward - \cf9 self\cf0 .q_values[action]\cb1 \
\cb2         \cf9 self\cf0 .q_values[action] += \cf5 0.1\cf0  * prediction_error\cb1 \
\cb2         \cf9 self\cf0 .performance_tracker.append(reward)\cb1 \
\cb2         \cf3 return\cf0  prediction_error\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 store_adaptive_memory\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 , \cf9 prediction_error\cf0 ):\cb1 \
\cb2         \cf3 pass\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 compute_adaptive_memory_bias\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf3 return\cf0  np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 emotional_processing\cf0 (\cf9 self\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .prev_reward \cf7 is\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .prev_reward = \cf5 0.5\cf0 \cb1 \
\cb2         \cf9 self\cf0 .emotion = np.zeros(\cf5 8\cf0 )\cb1 \
\cb2         \cf9 self\cf0 .prev_reward = reward\cb1 \
\
\
\cf6 \cb2 # Basic agents\cf0 \cb1 \
\cf7 \cb2 class\cf0  EpsilonGreedyAgent:\cb1 \
\cb2     \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 , \cf9 n_actions\cf0 =\cf5 5\cf0 , \cf9 epsilon\cf0 =\cf5 0.1\cf0 , \cf9 random_state\cf0 =\cf7 None\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .n_actions = n_actions\cb1 \
\cb2         \cf9 self\cf0 .epsilon = epsilon\cb1 \
\cb2         \cf9 self\cf0 .q_values = np.zeros(n_actions)\cb1 \
\cb2         \cf9 self\cf0 .action_counts = np.zeros(n_actions)\cb1 \
\
\cb2         \cf3 if\cf0  random_state \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .rng = np.random.RandomState(random_state)\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .rng = np.random.RandomState()\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 reset\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .q_values = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         \cf9 self\cf0 .action_counts = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 select_action\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf3 if\cf0  \cf9 self\cf0 .rng.rand() < \cf9 self\cf0 .epsilon:\cb1 \
\cb2             \cf3 return\cf0  \cf9 self\cf0 .rng.choice(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         \cf3 return\cf0  np.argmax(\cf9 self\cf0 .q_values)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 update\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .action_counts[action] += \cf5 1\cf0 \cb1 \
\cb2         alpha = \cf5 1\cf0  / \cf9 self\cf0 .action_counts[action]\cb1 \
\cb2         \cf9 self\cf0 .q_values[action] += alpha * (reward - \cf9 self\cf0 .q_values[action])\cb1 \
\
\cf7 \cb2 class\cf0  ThompsonSamplingAgent:\cb1 \
\cb2     \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 , \cf9 n_actions\cf0 =\cf5 5\cf0 , \cf9 random_state\cf0 =\cf7 None\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .n_actions = n_actions\cb1 \
\cb2         \cf9 self\cf0 .priors = [(\cf5 0.0\cf0 , \cf5 1.0\cf0 ) \cf3 for\cf0  _ \cf7 in\cf0  \cf8 range\cf0 (n_actions)]\cb1 \
\cb2         \cf9 self\cf0 .observations = [[] \cf3 for\cf0  _ \cf7 in\cf0  \cf8 range\cf0 (n_actions)]\cb1 \
\
\cb2         \cf3 if\cf0  random_state \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .rng = np.random.RandomState(random_state)\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .rng = np.random.RandomState()\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 reset\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .priors = [(\cf5 0.0\cf0 , \cf5 1.0\cf0 ) \cf3 for\cf0  _ \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions)]\cb1 \
\cb2         \cf9 self\cf0 .observations = [[] \cf3 for\cf0  _ \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions)]\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 select_action\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         samples = [\cf9 self\cf0 .rng.normal(mu, sigma) \cf3 for\cf0  mu, sigma \cf7 in\cf0  \cf9 self\cf0 .priors]\cb1 \
\cb2         \cf3 return\cf0  np.argmax(samples)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 update\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .observations[action].append(reward)\cb1 \
\cb2         data = \cf9 self\cf0 .observations[action]\cb1 \
\cb2         \cf3 if\cf0  \cf8 len\cf0 (data) > \cf5 1\cf0 :\cb1 \
\cb2             mu = np.mean(data)\cb1 \
\cb2             sigma = np.std(data) \cf3 if\cf0  np.std(data) > \cf5 0\cf0  \cf3 else\cf0  \cf5 1.0\cf0 \cb1 \
\cb2             \cf9 self\cf0 .priors[action] = (mu, sigma)\cb1 \
\
\cf7 \cb2 class\cf0  UCBAgent:\cb1 \
\cb2     \cf4 """UCB with controlled randomness"""\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 __init__\cf0 (\cf9 self\cf0 , \cf9 n_actions\cf0 =\cf5 5\cf0 , \cf9 c\cf0 =\cf5 0.5\cf0 , \cf9 random_state\cf0 =\cf7 None\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .n_actions = n_actions\cb1 \
\cb2         \cf9 self\cf0 .c = c\cb1 \
\cb2         \cf9 self\cf0 .q_values = np.zeros(n_actions)\cb1 \
\cb2         \cf9 self\cf0 .action_counts = np.zeros(n_actions)\cb1 \
\cb2         \cf9 self\cf0 .total_steps = \cf5 0\cf0 \cb1 \
\
\cb2         \cf3 if\cf0  random_state \cf7 is\cf0  \cf7 not\cf0  \cf7 None\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .rng = np.random.RandomState(random_state)\cb1 \
\cb2         \cf3 else\cf0 :\cb1 \
\cb2             \cf9 self\cf0 .rng = np.random.RandomState()\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 reset\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .q_values = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         \cf9 self\cf0 .action_counts = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\cb2         \cf9 self\cf0 .total_steps = \cf5 0\cf0 \cb1 \
\
\cb2     \cf7 def\cf0  \cf8 select_action\cf0 (\cf9 self\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .total_steps += \cf5 1\cf0 \cb1 \
\cb2         ucb_values = np.zeros(\cf9 self\cf0 .n_actions)\cb1 \
\
\cb2         \cf3 for\cf0  a \cf7 in\cf0  \cf8 range\cf0 (\cf9 self\cf0 .n_actions):\cb1 \
\cb2             \cf3 if\cf0  \cf9 self\cf0 .action_counts[a] == \cf5 0\cf0 :\cb1 \
\cb2                 \cf3 return\cf0  a\cb1 \
\cb2             bonus = \cf9 self\cf0 .c * np.sqrt(np.log(\cf9 self\cf0 .total_steps) / \cf9 self\cf0 .action_counts[a])\cb1 \
\cb2             ucb_values[a] = \cf9 self\cf0 .q_values[a] + bonus\cb1 \
\
\cb2         \cf3 return\cf0  np.argmax(ucb_values)\cb1 \
\
\cb2     \cf7 def\cf0  \cf8 update\cf0 (\cf9 self\cf0 , \cf9 action\cf0 , \cf9 reward\cf0 ):\cb1 \
\cb2         \cf9 self\cf0 .action_counts[action] += \cf5 1\cf0 \cb1 \
\cb2         alpha = \cf5 1\cf0  / \cf9 self\cf0 .action_counts[action]\cb1 \
\cb2         \cf9 self\cf0 .q_values[action] += alpha * (reward - \cf9 self\cf0 .q_values[action])\cb1 \
\
\
\cf7 \cb2 def\cf0  \cf8 test_unified_ecia\cf0 ():\cb1 \
\cb2     \cf4 """Universal integrated ECIA test"""\cf0 \cb1 \
\cb2     agent = ECIA(n_actions=\cf5 5\cf0 , memory_size=\cf5 15\cf0 )\cb1 \
\
\
\cb2     \cf6 # Various environment pattern simulation\cf0 \cb1 \
\cb2     \cf8 print\cf0 (\cf4 "\\n Various environment pattern test:"\cf0 )\cb1 \
\
\cb2     \cf6 # Pattern 1: Stable environment\cf0 \cb1 \
\cb2     \cf8 print\cf0 (\cf4 " Stable environment simulation..."\cf0 )\cb1 \
\cb2     \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf5 20\cf0 ):\cb1 \
\cb2         agent.update_context(i/\cf5 50\cf0 , \cf5 0\cf0 )\cb1 \
\cb2         action = agent.select_action()\cb1 \
\cb2         reward = \cf5 0.7\cf0  + np.random.normal(\cf5 0\cf0 , \cf5 0.1\cf0 )\cb1 \
\cb2         agent.update(action, reward)\cb1 \
\
\cb2     stable_status = agent.get_memory_status()\cb1 \
\cb2     \cf8 print\cf0 (\cf7 f\cf4 " Memory activation: \cf0 \{stable_status[\cf4 'memory_activation_level'\cf0 ]\}\cf4 "\cf0 )\cb1 \
\cb2     \cf8 print\cf0 (\cf7 f\cf4 " Environment stability: \cf0 \{stable_status[\cf4 'environment_stability'\cf0 ]\}\cf4 "\cf0 )\cb1 \
\cb2     \cf8 print\cf0 (\cf7 f\cf4 " Memory need score: \cf0 \{stable_status[\cf4 'memory_need_score'\cf0 ]\}\cf4 "\cf0 )\cb1 \
\
\cb2     \cf6 # Pattern 2: Changing environment\cf0 \cb1 \
\cb2     \cf8 print\cf0 (\cf4 "\\n Changing environment simulation..."\cf0 )\cb1 \
\cb2     \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf5 20\cf0 ):\cb1 \
\cb2         agent.update_context((\cf5 20\cf0 +i)/\cf5 50\cf0 , (i//\cf5 10\cf0 ))\cb1 \
\cb2         action = agent.select_action()\cb1 \
\cb2         reward = \cf5 0.3\cf0  \cf3 if\cf0  i < \cf5 10\cf0  \cf3 else\cf0  \cf5 0.8\cf0  + np.random.normal(\cf5 0\cf0 , \cf5 0.2\cf0 )\cb1 \
\cb2         agent.update(action, reward)\cb1 \
\
\cb2     changing_status = agent.get_memory_status()\cb1 \
\cb2     \cf8 print\cf0 (\cf7 f\cf4 " Memory activation: \cf0 \{changing_status[\cf4 'memory_activation_level'\cf0 ]\}\cf4 "\cf0 )\cb1 \
\cb2     \cf8 print\cf0 (\cf7 f\cf4 " Environment stability: \cf0 \{changing_status[\cf4 'environment_stability'\cf0 ]\}\cf4 "\cf0 )\cb1 \
\cb2     \cf8 print\cf0 (\cf7 f\cf4 " Memory need score: \cf0 \{changing_status[\cf4 'memory_need_score'\cf0 ]\}\cf4 "\cf0 )\cb1 \
\cb2     \cf8 print\cf0 (\cf7 f\cf4 " Performance trend: \cf0 \{changing_status[\cf4 'performance_trend'\cf0 ]\}\cf4 "\cf0 )\cb1 \
\
\
\cb2     agent.reset()\cb1 \
\
\cf7 \cb2 def\cf0  \cf8 bonferroni_correction\cf0 (\cf9 p_values\cf0 , \cf9 alpha\cf0 =\cf5 0.05\cf0 ):\cb1 \
\cb2     \cf4 """Simple Bonferroni correction implementation without statsmodels"""\cf0 \cb1 \
\cb2     p_values = np.array(p_values)\cb1 \
\cb2     n_tests = \cf8 len\cf0 (p_values)\cb1 \
\
\cb2     \cf6 # Bonferroni correction\cf0 \cb1 \
\cb2     p_corrected = p_values * n_tests\cb1 \
\cb2     p_corrected = np.clip(p_corrected, \cf5 0\cf0 , \cf5 1\cf0 )\cb1 \
\
\cb2     rejected = p_corrected < alpha\cb1 \
\
\cb2     \cf3 return\cf0  rejected, p_corrected\cb1 \
\
\cf7 \cb2 def\cf0  \cf8 perform_multiple_comparison_correction\cf0 (\cf9 complete_meta_results\cf0 , \cf9 save_path\cf0 =\cf4 "content/Results/meta_analysis"\cf0 ):\cb1 \
\cb2     \cf4 """Perform Bonferroni correction for multiple comparisons in analysis"""\cf0 \cb1 \
\
\cb2     \cf3 from\cf0  scipy.stats \cf3 import\cf0  ttest_ind, mannwhitneyu\cb1 \
\cb2     \cf3 import\cf0  pandas \cf3 as\cf0  pd\cb1 \
\
\cb2     \cf8 print\cf0 (\cf4 "\\n MULTIPLE COMPARISON CORRECTION (Bonferroni)"\cf0 )\cb1 \
\cb2     \cf8 print\cf0 (\cf4 "="\cf0  * \cf5 70\cf0 )\cb1 \
\
\cb2     correction_results = \{\}\cb1 \
\
\cb2     \cf3 for\cf0  env_name, env_results \cf7 in\cf0  complete_meta_results.items():\cb1 \
\cb2         \cf8 print\cf0 (\cf7 f\cf4 "\\n Environment: \cf0 \{env_name\}\cf4 "\cf0 )\cb1 \
\cb2         \cf6 # Extract seed-level data for all agents\cf0 \cb1 \
\cb2         agent_data = \{\}\cb1 \
\cb2         agent_names = []\cb1 \
\
\cb2         \cf3 for\cf0  agent_name, agent_result \cf7 in\cf0  env_results.items():\cb1 \
\cb2             \cf3 if\cf0  agent_result[\cf4 'meta_statistics'\cf0 ][\cf4 'n_master_seeds'\cf0 ] > \cf5 0\cf0 :\cb1 \
\cb2                 seed_means = []\cb1 \
\cb2                 \cf3 for\cf0  seed_key, seed_result \cf7 in\cf0  agent_result[\cf4 'individual_seeds'\cf0 ].items():\cb1 \
\cb2                     \cf3 if\cf0  seed_result[\cf4 'success_rate'\cf0 ] > \cf5 0\cf0 :\cb1 \
\cb2                         seed_means.append(seed_result[\cf4 'mean_reward'\cf0 ])\cb1 \
\
\cb2                 \cf3 if\cf0  \cf8 len\cf0 (seed_means) >= \cf5 3\cf0 :  \cf6 # Minimum data requirement\cf0 \cb1 \
\cb2                     agent_data[agent_name] = seed_means\cb1 \
\cb2                     agent_names.append(agent_name)\cb1 \
\
\cb2         \cf3 if\cf0  \cf8 len\cf0 (agent_names) < \cf5 2\cf0 :\cb1 \
\cb2             \cf8 print\cf0 (\cf7 f\cf4 "   Insufficient data for comparisons in \cf0 \{env_name\}\cf4 "\cf0 )\cb1 \
\cb2             \cf3 continue\cf0 \cb1 \
\
\cb2         \cf6 # Perform all pairwise comparisons\cf0 \cb1 \
\cb2         comparison_results = []\cb1 \
\cb2         p_values = []\cb1 \
\cb2         comparison_names = []\cb1 \
\
\cb2         \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf8 len\cf0 (agent_names)):\cb1 \
\cb2             \cf3 for\cf0  j \cf7 in\cf0  \cf8 range\cf0 (i+\cf5 1\cf0 , \cf8 len\cf0 (agent_names)):\cb1 \
\cb2                 agent1, agent2 = agent_names[i], agent_names[j]\cb1 \
\cb2                 data1, data2 = agent_data[agent1], agent_data[agent2]\cb1 \
\
\cb2                 \cf6 # Perform appropriate statistical test\cf0 \cb1 \
\cb2                 \cf3 try\cf0 :\cb1 \
\cb2                     \cf6 # Use t-test (could be enhanced with normality checking)\cf0 \cb1 \
\cb2                     stat, p_val = ttest_ind(data1, data2, equal_var=\cf7 False\cf0 )\cb1 \
\
\cb2                     \cf6 # Effect size (Cohen's d)\cf0 \cb1 \
\cb2                     pooled_std = np.sqrt(((\cf8 len\cf0 (data1) - \cf5 1\cf0 ) * np.var(data1) +\cb1 \
\cb2                                          (\cf8 len\cf0 (data2) - \cf5 1\cf0 ) * np.var(data2)) /\cb1 \
\cb2                                         (\cf8 len\cf0 (data1) + \cf8 len\cf0 (data2) - \cf5 2\cf0 ))\cb1 \
\cb2                     cohens_d = (np.mean(data1) - np.mean(data2)) / pooled_std \cf3 if\cf0  pooled_std > \cf5 0\cf0  \cf3 else\cf0  \cf5 0\cf0 \cb1 \
\
\cb2                     comparison_results.append(\{\cb1 \
\cb2                         \cf4 'Agent_1'\cf0 : agent1,\cb1 \
\cb2                         \cf4 'Agent_2'\cf0 : agent2,\cb1 \
\cb2                         \cf4 'Mean_1'\cf0 : np.mean(data1),\cb1 \
\cb2                         \cf4 'Mean_2'\cf0 : np.mean(data2),\cb1 \
\cb2                         \cf4 'Mean_Diff'\cf0 : np.mean(data1) - np.mean(data2),\cb1 \
\cb2                         \cf4 'P_Value_Raw'\cf0 : p_val,\cb1 \
\cb2                         \cf4 'Cohens_D'\cf0 : cohens_d,\cb1 \
\cb2                         \cf4 'Test_Statistic'\cf0 : stat\cb1 \
\cb2                     \})\cb1 \
\
\cb2                     p_values.append(p_val)\cb1 \
\cb2                     comparison_names.append(\cf7 f\cf4 "\cf0 \{agent1\}\cf4 _vs_\cf0 \{agent2\}\cf4 "\cf0 )\cb1 \
\
\cb2                 \cf3 except\cf0  Exception \cf3 as\cf0  e:\cb1 \
\cb2                     \cf8 print\cf0 (\cf7 f\cf4 "    \uc0\u9888 \u65039  Test failed for \cf0 \{agent1\}\cf4  vs \cf0 \{agent2\}\cf4 : \cf0 \{e\}\cf4 "\cf0 )\cb1 \
\
\cb2         \cf3 if\cf0  p_values:\cb1 \
\cb2             \cf6 # Apply Bonferroni correction\cf0 \cb1 \
\cb2             rejected, p_corrected = bonferroni_correction(p_values, alpha=\cf5 0.05\cf0 )\cb1 \
\
\
\cb2             \cf6 # Add corrected results\cf0 \cb1 \
\cb2             \cf3 for\cf0  i, result \cf7 in\cf0  \cf8 enumerate\cf0 (comparison_results):\cb1 \
\cb2                 result[\cf4 'P_Value_Bonferroni'\cf0 ] = p_corrected[i]\cb1 \
\cb2                 result[\cf4 'Significant_Bonferroni'\cf0 ] = rejected[i]\cb1 \
\cb2                 result[\cf4 'Significant_Raw'\cf0 ] = result[\cf4 'P_Value_Raw'\cf0 ] < \cf5 0.05\cf0 \cb1 \
\
\cb2             correction_results[env_name] = comparison_results\cb1 \
\
\cb2             \cf6 # Print significant results after correction\cf0 \cb1 \
\cb2             significant_after_correction = [r \cf3 for\cf0  r \cf7 in\cf0  comparison_results \cf3 if\cf0  r[\cf4 'Significant_Bonferroni'\cf0 ]]\cb1 \
\
\cb2             \cf8 print\cf0 (\cf7 f\cf4 "   Total comparisons: \cf0 \{\cf8 len\cf0 (comparison_results)\}\cf4 "\cf0 )\cb1 \
\cb2             \cf8 print\cf0 (\cf7 f\cf4 "   Significant before correction: \cf0 \{\cf8 sum\cf0 (r[\cf4 'Significant_Raw'\cf0 ] \cf3 for\cf0  r \cf7 in\cf0  comparison_results)\}\cf4 "\cf0 )\cb1 \
\cb2             \cf8 print\cf0 (\cf7 f\cf4 "   Significant after Bonferroni: \cf0 \{\cf8 len\cf0 (significant_after_correction)\}\cf4 "\cf0 )\cb1 \
\
\cb2             \cf3 if\cf0  significant_after_correction:\cb1 \
\cb2                 \cf8 print\cf0 (\cf7 f\cf4 "   Significant comparisons after Bonferroni correction:"\cf0 )\cb1 \
\cb2                 \cf3 for\cf0  result \cf7 in\cf0  significant_after_correction:\cb1 \
\cb2                     direction = \cf4 ">"\cf0  \cf3 if\cf0  result[\cf4 'Mean_Diff'\cf0 ] > \cf5 0\cf0  \cf3 else\cf0  \cf4 "<"\cf0 \cb1 \
\cb2                     \cf8 print\cf0 (\cf7 f\cf4 "    \cf0 \{result[\cf4 'Agent_1'\cf0 ]\}\cf4  \cf0 \{direction\}\cf4  \cf0 \{result[\cf4 'Agent_2'\cf0 ]\}\cf4 : "\cf0 \cb1 \
\cb2                           \cf7 f\cf4 "p_corrected=\cf0 \{result[\cf4 'P_Value_Bonferroni'\cf0 ]\cf5 :.6f\cf0 \}\cf4 , d=\cf0 \{result[\cf4 'Cohens_D'\cf0 ]\cf5 :.3f\cf0 \}\cf4 "\cf0 )\cb1 \
\cb2             \cf3 else\cf0 :\cb1 \
\cb2                 \cf8 print\cf0 (\cf7 f\cf4 "   No significant differences after Bonferroni correction"\cf0 )\cb1 \
\
\cb2         \cf6 # Save detailed results to CSV\cf0 \cb1 \
\cb2         \cf3 if\cf0  comparison_results:\cb1 \
\cb2             df = pd.DataFrame(comparison_results)\cb1 \
\cb2             csv_filename = \cf7 f\cf4 "\cf0 \{save_path\}\cf4 /bonferroni_correction_\cf0 \{env_name\}\cf4 .csv"\cf0 \cb1 \
\cb2             df.to_csv(csv_filename, index=\cf7 False\cf0 )\cb1 \
\cb2             \cf8 print\cf0 (\cf7 f\cf4 "   Detailed results saved: \cf0 \{csv_filename\}\cf4 "\cf0 )\cb1 \
\
\cb2     \cf6 # Save overall correction results\cf0 \cb1 \
\cb2     \cf3 with\cf0  \cf8 open\cf0 (\cf7 f\cf4 "\cf0 \{save_path\}\cf4 /bonferroni_correction_results.pkl"\cf0 , \cf4 "wb"\cf0 ) \cf3 as\cf0  f:\cb1 \
\cb2         pickle.dump(correction_results, f)\cb1 \
\
\cb2     \cf8 print\cf0 (\cf7 f\cf4 "\\n Multiple comparison correction completed!"\cf0 )\cb1 \
\cb2     \cf3 return\cf0  correction_results\cb1 \
\
\cf7 \cb2 def\cf0  \cf8 perform_multiple_comparison_correction_cross_dataset\cf0 (\cf9 complete_results\cf0 , \cf9 save_path\cf0 =\cf4 "content/Results/cross_dataset_study"\cf0 ):\cb1 \
\cb2     \cf4 """Perform Bonferroni correction for cross-dataset study (ECIA_Full vs Baseline only)"""\cf0 \cb1 \
\
\cb2     \cf3 from\cf0  scipy.stats \cf3 import\cf0  ttest_ind\cb1 \
\cb2     \cf3 import\cf0  pandas \cf3 as\cf0  pd\cb1 \
\
\cb2     \cf8 print\cf0 (\cf4 "\\n MULTIPLE COMPARISON CORRECTION (Bonferroni) - Cross-Dataset Study"\cf0 )\cb1 \
\cb2     \cf8 print\cf0 (\cf4 "="\cf0  * \cf5 70\cf0 )\cb1 \
\
\cb2     \cf6 # Focus on ECIA_Full vs Baseline only\cf0 \cb1 \
\cb2     baseline_agents = [\cf4 "EpsilonGreedy"\cf0 , \cf4 "UCB"\cf0 , \cf4 "TS"\cf0 ]\cb1 \
\cb2     target_agents = [\cf4 "ECIA_Full"\cf0 ] + baseline_agents\cb1 \
\
\cb2     \cf6 # Metrics to test\cf0 \cb1 \
\cb2     metrics_to_test = [\cb1 \
\cb2         \cf4 'overall_performance_mean'\cf0 ,\cb1 \
\cb2         \cf4 'recovery_rate_mean'\cf0 ,\cb1 \
\cb2         \cf4 'recovery_time_mean'\cf0 \cb1 \
\cb2     ]\cb1 \
\
\cb2     correction_results = \{\}\cb1 \
\
\cb2     \cf3 for\cf0  metric_name \cf7 in\cf0  metrics_to_test:\cb1 \
\cb2         \cf8 print\cf0 (\cf7 f\cf4 "\\n Metric: \cf0 \{metric_name\}\cf4 "\cf0 )\cb1 \
\
\cb2         \cf6 # Extract data for this metric\cf0 \cb1 \
\cb2         agent_data = \{\}\cb1 \
\
\cb2         \cf3 for\cf0  agent_name \cf7 in\cf0  target_agents:\cb1 \
\cb2             \cf3 if\cf0  agent_name \cf7 in\cf0  complete_results:\cb1 \
\cb2                 agent_result = complete_results[agent_name]\cb1 \
\
\cb2                 metric_values = []\cb1 \
\cb2                 \cf3 for\cf0  seed_key, seed_result \cf7 in\cf0  agent_result[\cf4 'individual_seeds'\cf0 ].items():\cb1 \
\cb2                     \cf3 if\cf0  seed_result[\cf4 'n_experiments'\cf0 ] > \cf5 0\cf0 :\cb1 \
\cb2                         \cf3 if\cf0  metric_name == \cf4 'overall_performance_mean'\cf0 :\cb1 \
\cb2                             metric_values.extend(seed_result[\cf4 'overall_performances'\cf0 ])\cb1 \
\cb2                         \cf3 elif\cf0  metric_name == \cf4 'recovery_rate_mean'\cf0 :\cb1 \
\cb2                             metric_values.extend(seed_result[\cf4 'recovery_rates'\cf0 ])\cb1 \
\cb2                         \cf3 elif\cf0  metric_name == \cf4 'recovery_time_mean'\cf0 :\cb1 \
\cb2                             metric_values.extend(seed_result[\cf4 'recovery_times'\cf0 ])\cb1 \
\cb2                 \cf3 if\cf0  \cf8 len\cf0 (metric_values) >= \cf5 3\cf0 :\cb1 \
\cb2                     agent_data[agent_name] = metric_values\cb1 \
\
\cb2         \cf3 if\cf0  \cf8 len\cf0 (agent_data) < \cf5 2\cf0 :\cb1 \
\cb2             \cf8 print\cf0 (\cf7 f\cf4 "   Insufficient data for \cf0 \{metric_name\}\cf4 "\cf0 )\cb1 \
\cb2             \cf3 continue\cf0 \cb1 \
\
\cb2         \cf6 # Perform pairwise comparisons\cf0 \cb1 \
\cb2         comparison_results = []\cb1 \
\cb2         p_values = []\cb1 \
\
\cb2         agent_names = \cf10 list\cf0 (agent_data.keys())\cb1 \
\cb2         \cf3 for\cf0  i \cf7 in\cf0  \cf8 range\cf0 (\cf8 len\cf0 (agent_names)):\cb1 \
\cb2             \cf3 for\cf0  j \cf7 in\cf0  \cf8 range\cf0 (i+\cf5 1\cf0 , \cf8 len\cf0 (agent_names)):\cb1 \
\cb2                 agent1, agent2 = agent_names[i], agent_names[j]\cb1 \
\cb2                 data1, data2 = agent_data[agent1], agent_data[agent2]\cb1 \
\
\cb2                 \cf3 try\cf0 :\cb1 \
\cb2                     stat, p_val = ttest_ind(data1, data2, equal_var=\cf7 False\cf0 )\cb1 \
\
\cb2                     \cf6 # Effect size\cf0 \cb1 \
\cb2                     pooled_std = np.sqrt(((\cf8 len\cf0 (data1) - \cf5 1\cf0 ) * np.var(data1) +\cb1 \
\cb2                                          (\cf8 len\cf0 (data2) - \cf5 1\cf0 ) * np.var(data2)) /\cb1 \
\cb2                                         (\cf8 len\cf0 (data1) + \cf8 len\cf0 (data2) - \cf5 2\cf0 ))\cb1 \
\cb2                     cohens_d = (np.mean(data1) - np.mean(data2)) / pooled_std \cf3 if\cf0  pooled_std > \cf5 0\cf0  \cf3 else\cf0  \cf5 0\cf0 \cb1 \
\
\cb2                     comparison_results.append(\{\cb1 \
\cb2                         \cf4 'Metric'\cf0 : metric_name,\cb1 \
\cb2                         \cf4 'Agent_1'\cf0 : agent1,\cb1 \
\cb2                         \cf4 'Agent_2'\cf0 : agent2,\cb1 \
\cb2                         \cf4 'Mean_1'\cf0 : np.mean(data1),\cb1 \
\cb2                         \cf4 'Mean_2'\cf0 : np.mean(data2),\cb1 \
\cb2                         \cf4 'Mean_Diff'\cf0 : np.mean(data1) - np.mean(data2),\cb1 \
\cb2                         \cf4 'P_Value_Raw'\cf0 : p_val,\cb1 \
\cb2                         \cf4 'Cohens_D'\cf0 : cohens_d\cb1 \
\cb2                     \})\cb1 \
\
\cb2                     p_values.append(p_val)\cb1 \
\
\cb2                 \cf3 except\cf0  Exception \cf3 as\cf0  e:\cb1 \
\cb2                     \cf8 print\cf0 (\cf7 f\cf4 "    Test failed for \cf0 \{agent1\}\cf4  vs \cf0 \{agent2\}\cf4 : \cf0 \{e\}\cf4 "\cf0 )\cb1 \
\
\cb2         \cf3 if\cf0  p_values:\cb1 \
\cb2             \cf6 # Apply Bonferroni correction\cf0 \cb1 \
\cb2             rejected, p_corrected = bonferroni_correction(p_values, alpha=\cf5 0.05\cf0 )\cb1 \
\
\cb2             \cf6 # Add corrected results\cf0 \cb1 \
\cb2             \cf3 for\cf0  i, result \cf7 in\cf0  \cf8 enumerate\cf0 (comparison_results):\cb1 \
\cb2                 result[\cf4 'P_Value_Bonferroni'\cf0 ] = p_corrected[i]\cb1 \
\cb2                 result[\cf4 'Significant_Bonferroni'\cf0 ] = rejected[i]\cb1 \
\cb2                 result[\cf4 'Significant_Raw'\cf0 ] = result[\cf4 'P_Value_Raw'\cf0 ] < \cf5 0.05\cf0 \cb1 \
\
\cb2             correction_results[metric_name] = comparison_results\cb1 \
\
\cb2             \cf6 # Print results\cf0 \cb1 \
\cb2             significant_after = \cf8 sum\cf0 (r[\cf4 'Significant_Bonferroni'\cf0 ] \cf3 for\cf0  r \cf7 in\cf0  comparison_results)\cb1 \
\cb2             significant_before = \cf8 sum\cf0 (r[\cf4 'Significant_Raw'\cf0 ] \cf3 for\cf0  r \cf7 in\cf0  comparison_results)\cb1 \
\
\cb2             \cf8 print\cf0 (\cf7 f\cf4 "   Significant before/after correction: \cf0 \{significant_before\}\cf4 /\cf0 \{significant_after\}\cf4 "\cf0 )\cb1 \
\
\cb2             \cf3 if\cf0  significant_after > \cf5 0\cf0 :\cb1 \
\cb2                 \cf8 print\cf0 (\cf7 f\cf4 "   Significant after Bonferroni:"\cf0 )\cb1 \
\cb2                 \cf3 for\cf0  result \cf7 in\cf0  comparison_results:\cb1 \
\cb2                     \cf3 if\cf0  result[\cf4 'Significant_Bonferroni'\cf0 ]:\cb1 \
\cb2                         direction = \cf4 ">"\cf0  \cf3 if\cf0  result[\cf4 'Mean_Diff'\cf0 ] > \cf5 0\cf0  \cf3 else\cf0  \cf4 "<"\cf0 \cb1 \
\cb2                         \cf8 print\cf0 (\cf7 f\cf4 "    \cf0 \{result[\cf4 'Agent_1'\cf0 ]\}\cf4  \cf0 \{direction\}\cf4  \cf0 \{result[\cf4 'Agent_2'\cf0 ]\}\cf4 : "\cf0 \cb1 \
\cb2                               \cf7 f\cf4 "p_corrected=\cf0 \{result[\cf4 'P_Value_Bonferroni'\cf0 ]\cf5 :.6f\cf0 \}\cf4 "\cf0 )\cb1 \
\
\cb2     \cf6 # Save results\cf0 \cb1 \
\cb2     \cf3 if\cf0  correction_results:\cb1 \
\cb2         all_results = []\cb1 \
\cb2         \cf3 for\cf0  metric_name, metric_results \cf7 in\cf0  correction_results.items():\cb1 \
\cb2             all_results.extend(metric_results)\cb1 \
\
\cb2         df = pd.DataFrame(all_results)\cb1 \
\cb2         csv_filename = \cf7 f\cf4 "\cf0 \{save_path\}\cf4 /cross_dataset_bonferroni_correction.csv"\cf0 \cb1 \
\cb2         df.to_csv(csv_filename, index=\cf7 False\cf0 )\cb1 \
\cb2         \cf8 print\cf0 (\cf7 f\cf4 "\\n\uc0\u55357 \u56516  Detailed results saved: \cf0 \{csv_filename\}\cf4 "\cf0 )\cb1 \
\
\cb2         \cf3 with\cf0  \cf8 open\cf0 (\cf7 f\cf4 "\cf0 \{save_path\}\cf4 /cross_dataset_bonferroni_correction_results.pkl"\cf0 , \cf4 "wb"\cf0 ) \cf3 as\cf0  f:\cb1 \
\cb2             pickle.dump(correction_results, f)\cb1 \
\
\cb2     \cf8 print\cf0 (\cf7 f\cf4 "\\n Cross-dataset multiple comparison correction completed!"\cf0 )\cb1 \
\cb2     \cf3 return\cf0  correction_results\cb1 \
\
\
\cf3 \cb2 if\cf0  \cf9 __name__\cf0  == \cf4 "__main__"\cf0 :\cb1 \
\cb2     test_unified_ecia()\cb1 \
\
\
\
import numpy as np\
import matplotlib.pyplot as plt\
import os\
from tqdm import tqdm\
\
# =====================\
# 1. SIMPLE EMOTION COLLECTOR\
# =====================\
\
def collect_ecia_emotions_simple(env_class, n_trials=200, n_runs=50, env_name="Environment"):\
    """Simple emotion collection for ECIA_Full"""\
\
    print(f" Collecting ECIA_Full emotions for \{env_name\} (\{n_runs\} runs)")\
\
    # ECIA_Full parameters\
    ecia_params = \{\
        "n_actions": 5,\
        "epsilon": 0.03,\
        "eta": 0.55,\
        "xi": 0.001,\
        "memory_threshold": 0.015,\
        "memory_influence": 0.3,\
        "memory_similarity_threshold": 0.035,\
        "top_k": 3,\
        "alpha": 0.22,\
        "window_size": 30,\
        "memory_size": 15,\
        "emotion_decay": 0.96,\
        "min_eta": 0.095,\
    \}\
\
    # Store emotion data for all runs\
    all_emotion_data = []  # [run][trial][emotion_idx]\
\
    for run in tqdm(range(n_runs), desc=f"\{env_name\}"):\
        # Initialize environment and agent\
        env = env_class(random_state=run)\
        agent = ECIA(random_state=run + 1000, **ecia_params)\
\
        env.reset()\
        agent.reset()\
\
        # Store emotions for this run\
        run_emotions = []  # [trial][emotion_idx]\
\
        for trial in range(n_trials):\
            action = agent.select_action()\
            reward, context = env.step(action)\
\
            # Capture emotion state\
            emotion_state = agent.emotion.copy()  # 8 emotions\
            run_emotions.append(emotion_state)\
\
            # Update agent\
            agent.update(context, action, reward)\
\
        all_emotion_data.append(run_emotions)\
\
    return np.array(all_emotion_data)  # Shape: [n_runs, n_trials, 8]\
\
def run_complete_meta_analysis():\
    """Run complete meta-analysis for all agents and environments"""\
\
    print("=" * 80)\
    print("META-ANALYSIS STUDY")\
    print("12 Master Seeds \'d7 300 Runs = 3600 Total Experiments per Agent")\
    print("=" * 80)\
\
    # Optimized agent parameters\
    optimized_params = \{\
        "n_actions": 5,\
        "epsilon": 0.03,\
        "eta": 0.55,\
        "xi": 0.001,\
        "memory_threshold": 0.015,\
        "memory_influence": 0.3,\
        "memory_similarity_threshold": 0.035,\
        "top_k": 3,\
        "alpha": 0.22,\
        "window_size": 30,\
        "memory_size": 15,\
        "emotion_decay": 0.96,\
        "min_eta": 0.095,\
    \}\
\
    # Agent configurations for meta-analysis\
    agents = \{\
        # Baseline algorithms\
        "EpsilonGreedy": (EpsilonGreedyAgent, \{"n_actions": 5, "epsilon": 0.1\}),\
        "UCB": (UCBAgent, \{"n_actions": 5, "c": 0.5\}),\
        "TS" : (ThompsonSamplingAgent, \{"n_actions":5\}),\
        # ECIA Full\
        "ECIA_Full": (ECIA, optimized_params),\
        "ECIA_NoEmotion": (ECIA_NoEmotion, optimized_params),\
        "ECIA_NoMemory": (ECIA_NoMemory, optimized_params),\
        "ECIA_NoDopamine": (ECIA_NoDopamine, optimized_params),\
        "ECIA_NoDop_NoMem": (ECIA_NoDopamine_NoMemory, optimized_params),\
        "ECIA_NoDop_NoEmo": (ECIA_NoDopamine_NoEmotion, optimized_params),\
        "ECIA_NoMem_NoEmo": (ECIA_NoMemory_NoEmotion, optimized_params),\
        "ECIA_NoAll": (ECIA_NoAll_Components, optimized_params)\
    \}\
\
\
    # Environment configurations\
    environments = \{\
        "EnvA": EnvironmentA,\
        "EnvB": EnvironmentB,\
        "EnvC": EnvironmentC,\
    \}\
\
    # Store all meta-analysis results\
    complete_meta_results = \{\}\
\
    # Run meta-analysis for each environment\
    for env_name, env_class in environments.items():\
        print(f"\\n ENVIRONMENT \{env_name\} Meta-Analysis")\
        n_trials = 200\
\
\
        print(f" Configuration: \{len(MANAGER.SEEDS)\} seeds \'d7 \{MANAGER.N_RUNS_PER_SEED\} runs = \{len(MANAGER.SEEDS) * MANAGER.N_RUNS_PER_SEED\} experiments")\
        print(f"Expected change points: \{get_environment_change_points(env_name)\}")\
        print("-" * 60)\
\
        complete_meta_results[env_name] = \{\}\
\
        # Run meta-analysis for each agent in this environment\
        for agent_name, (agent_class, agent_kwargs) in agents.items():\
            print(f"\\n Agent: \{agent_name\}")\
\
            meta_result = run_meta_analysis_for_agent(\
                env_class, agent_class, agent_kwargs,\
                n_trials=n_trials, env_name=env_name\
            )\
\
            complete_meta_results[env_name][agent_name] = meta_result\
\
            # Print comprehensive meta-statistics\
            meta_stats = meta_result['meta_statistics']\
            print(f"   Meta-Analysis Results:")\
            print(f"    Overall Performance:")\
            print(f"      Mean \'b1 Std: \{meta_stats['meta_mean']:.4f\} \'b1 \{meta_stats['meta_std']:.4f\}")\
            print(f"      95% CI: [\{meta_stats['ci_lower']:.4f\}, \{meta_stats['ci_upper']:.4f\}]")\
            print(f"    Recovery Analysis:")\
            print(f"      Recovery Rate: \{meta_stats['meta_recovery_rate_mean']:.4f\} \'b1 \{meta_stats['meta_recovery_rate_std']:.4f\}")\
            print(f"      Recovery Time: \{meta_stats['meta_recovery_time_mean']:.2f\} \'b1 \{meta_stats['meta_recovery_time_std']:.2f\} trials")\
            print(f"    Experiment Details:")\
            print(f"      Total experiments: \{meta_stats['total_experiments']\}")\
            print(f"      Master seeds used: \{meta_stats['n_master_seeds']\}")\
            print(f"      Success rate: \{meta_stats['meta_success_rate']:.1%\}")\
\
    # Save complete results\
    save_results_safely(complete_meta_results, "content/Results/complete_results_full.pkl")\
\
    # Save lightweight version\
    lightweight_results = create_lightweight_results(complete_meta_results)\
    save_results_safely(lightweight_results, "content/Results/complete_results.pkl")\
\
    print(f" Lightweight results saved (original: ~1GB \uc0\u8594  compressed: ~10-50MB)")\
\
    # Save configuration\
    MANAGER.save_configuration()\
\
    print(f"\\n Complete Meta-Analysis finished!")\
    print(f" Results saved in: content/Results/meta_analysis/")\
\
    return complete_meta_results\
\
def save_results_safely(results, filepath):\
    """Safe pickle saving with backup"""\
    import tempfile\
    import shutil\
\
    # Create temporary file first\
    temp_filepath = filepath + ".tmp"\
\
    try:\
        with open(temp_filepath, "wb") as f:\
            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\
\
        # If successful, replace original file\
        shutil.move(temp_filepath, filepath)\
        print(f" Results safely saved: \{filepath\}")\
\
    except Exception as e:\
        print(f" Failed to save \{filepath\}: \{e\}")\
        if os.path.exists(temp_filepath):\
            os.remove(temp_filepath)\
\
def create_lightweight_results(complete_meta_results):\
    """Create lightweight version of results for saving"""\
\
    lightweight_results = \{\}\
\
    for env_name, env_results in complete_meta_results.items():\
        lightweight_results[env_name] = \{\}\
\
        for agent_name, agent_result in env_results.items():\
            if agent_result['meta_statistics']['n_master_seeds'] > 0:\
\
                lightweight_agent_result = \{\
                    'meta_statistics': agent_result['meta_statistics'],  \
                    'seed_level_summary': \{\}  \
                \}\
\
\
                for seed_key, seed_result in agent_result['individual_seeds'].items():\
                    if seed_result['success_rate'] > 0:\
                        lightweight_agent_result['seed_level_summary'][seed_key] = \{\
                            'master_seed': seed_result['master_seed'],\
                            'mean_reward': seed_result['mean_reward'],\
                            'std_reward': seed_result['std_reward'],\
                            'success_rate': seed_result['success_rate'],\
                            'n_experiments': seed_result['n_experiments']\
                        \}\
\
                lightweight_results[env_name][agent_name] = lightweight_agent_result\
\
    return lightweight_results\
\
\
\
#4. META-ANALYSIS EXPERIMENT EXECUTION\
# =====================\
\
def run_single_experiment(env_class, agent_class, agent_kwargs,\
                                   n_trials=200, env_name="Unknown",\
                                   experiment_seed=42):\
    """Run a single experiment with specific seed"""\
\
    try:\
        # Initialize environment with seed\
        env = env_class(random_state=experiment_seed)\
\
        # Initialize agent with seed\
        agent_kwargs_with_seed = agent_kwargs.copy()\
        agent_kwargs_with_seed['random_state'] = experiment_seed + 1\
        agent = agent_class(**agent_kwargs_with_seed)\
\
        env.reset()\
        agent.reset()\
\
        rewards = []\
        actions = []\
\
        for t in range(n_trials):\
            action = agent.select_action()\
            reward, context = env.step(action)\
\
            # ECIA - context save\
            if hasattr(agent, 'context'):  # if ECIA\
                agent.update(context, action, reward)\
            else:  # other agent\
                agent.update(action, reward)\
\
            rewards.append(reward)\
            actions.append(action)\
\
        return \{\
            "rewards": np.array(rewards),\
            "actions": np.array(actions),\
            "mean_reward": np.mean(rewards),\
            "success": True,\
            "experiment_seed": experiment_seed,\
            "env_instance": env\
        \}\
\
    except Exception as e:\
        print(f"    Experiment error: \{e\}")\
        return \{\
            "rewards": np.zeros(n_trials),\
            "actions": np.zeros(n_trials),\
            "mean_reward": 0.0,\
            "success": False,\
            "experiment_seed": experiment_seed,\
            "env_instance": None\
        \}\
\
def run_master_seed_experiments(master_seed, env_class, agent_class,\
                                         agent_kwargs, n_trials=200, env_name="Environment"):\
    """Run experiments for a single master seed (environment-specific)"""\
\
    # Get environment-specific settings\
    seeds = MANAGER.SEEDS\
    n_runs = MANAGER.N_RUNS_PER_SEED\
    print(f" Master Seed \{master_seed\}: Running \{n_runs\} experiments...")\
\
    # Generate experiment seeds with env_name\
    experiment_seeds = MANAGER.generate_experiment_seeds(master_seed, env_name)\
\
    all_rewards = []\
    all_actions = []\
    all_env_instances = []\
    success_count = 0\
\
    for run_id, experiment_seed in enumerate(tqdm(experiment_seeds, desc=f"Seed-\{master_seed\}")):\
        result = run_single_experiment(\
            env_class, agent_class, agent_kwargs,\
            n_trials, env_name, experiment_seed\
        )\
\
        if result["success"]:\
            all_rewards.append(result["rewards"])\
            all_actions.append(result["actions"])\
            success_count += 1\
\
            if "env_instance" in result:\
                all_env_instances.append(result["env_instance"])\
\
    master_seed_result = \{\
        "master_seed": master_seed,\
        "rewards": np.array(all_rewards) if success_count > 0 else np.zeros((1, n_trials)),\
        "actions": np.array(all_actions) if success_count > 0 else np.zeros((1, n_trials)),\
        "mean_reward": np.mean([np.mean(r) for r in all_rewards]) if success_count > 0 else 0.0,\
        "std_reward": np.std([np.mean(r) for r in all_rewards]) if success_count > 0 else 0.0,\
        "success_rate": success_count / len(experiment_seeds),\
        "env_instances": all_env_instances,\
        "n_experiments": len(experiment_seeds)\
    \}\
\
    print(f"  \uc0\u9989  Master Seed \{master_seed\}: Success rate \{master_seed_result['success_rate']:.1%\}, "\
          f"Mean reward \{master_seed_result['mean_reward']:.4f\} \'b1 \{master_seed_result['std_reward']:.4f\}")\
\
    return master_seed_result\
\
\
\
def run_meta_analysis_for_agent(env_class, agent_class, agent_kwargs,\
                                         n_trials=200, env_name="Environment"):\
    """Run complete meta-analysis for one agent with comprehensive analysis"""\
\
    seeds = MANAGER.SEEDS\
\
    meta_results = \{\}\
\
    for master_seed in seeds:  \
        master_seed_result = run_master_seed_experiments(\
            master_seed, env_class, agent_class, agent_kwargs, n_trials, env_name\
        )\
        meta_results[f"seed_\{master_seed\}"] = master_seed_result\
\
\
    # Calculate meta-statistics across all seeds\
    all_mean_rewards = []\
    all_std_rewards = []\
    all_success_rates = []\
    all_recovery_rates = []\
    all_recovery_times = []\
\
    for seed_result in meta_results.values():\
        if seed_result['success_rate'] > 0:\
            rewards = seed_result['rewards']\
            env_instances = seed_result.get('env_instances', None)\
\
            # Basic statistics\
            all_mean_rewards.append(seed_result['mean_reward'])\
            all_std_rewards.append(seed_result['std_reward'])\
            all_success_rates.append(seed_result['success_rate'])\
\
            # Comprehensive analysis for this master seed\
            try:\
                # Recovery rate calculation\
                recovery_rates = compute_unified_recovery_rate(rewards, env_name, env_instances)\
                if len(recovery_rates) > 0:\
                    all_recovery_rates.append(np.mean(recovery_rates))\
\
                # Recovery time measurement\
                recovery_times = measure_unified_recovery_time(rewards, env_name, env_instances)\
                if len(recovery_times) > 0:\
                    all_recovery_times.append(np.mean(recovery_times))\
\
\
            except Exception as e:\
                print(f"    Warning: Comprehensive analysis failed for master seed \{seed_result['master_seed']\}: \{e\}")\
\
    if all_mean_rewards:\
        meta_statistics = \{\
            # Basic meta-statistics\
            "meta_mean": np.mean(all_mean_rewards),\
            "meta_std": np.std(all_mean_rewards),\
            "meta_sem": np.std(all_mean_rewards) / np.sqrt(len(all_mean_rewards)),\
            "meta_success_rate": np.mean(all_success_rates),\
            "n_master_seeds": len(all_mean_rewards),\
            "total_experiments": sum(seed_result['n_experiments'] for seed_result in meta_results.values()),\
\
            # Comprehensive meta-statistics\
            "meta_recovery_rate_mean": np.mean(all_recovery_rates) if all_recovery_rates else 0.0,\
            "meta_recovery_rate_std": np.std(all_recovery_rates) if all_recovery_rates else 0.0,\
            "meta_recovery_time_mean": np.mean(all_recovery_times) if all_recovery_times else 0.0,\
            "meta_recovery_time_std": np.std(all_recovery_times) if all_recovery_times else 0.0\
        \}\
\
        # Calculate confidence intervals\
        if len(all_mean_rewards) > 1:\
            alpha = 0.05  # 95% confidence interval\
            t_critical = stats.t.ppf(1 - alpha/2, len(all_mean_rewards) - 1)\
\
            # Basic CI\
            margin_error = t_critical * meta_statistics['meta_sem']\
            meta_statistics['ci_lower'] = meta_statistics['meta_mean'] - margin_error\
            meta_statistics['ci_upper'] = meta_statistics['meta_mean'] + margin_error\
\
            # Recovery rate CI\
            if all_recovery_rates:\
                recovery_sem = np.std(all_recovery_rates) / np.sqrt(len(all_recovery_rates))\
                recovery_margin = t_critical * recovery_sem\
                meta_statistics['recovery_rate_ci_lower'] = meta_statistics['meta_recovery_rate_mean'] - recovery_margin\
                meta_statistics['recovery_rate_ci_upper'] = meta_statistics['meta_recovery_rate_mean'] + recovery_margin\
\
            # Recovery time CI\
            if all_recovery_times:\
                time_sem = np.std(all_recovery_times) / np.sqrt(len(all_recovery_times))\
                time_margin = t_critical * time_sem\
                meta_statistics['recovery_time_ci_lower'] = meta_statistics['meta_recovery_time_mean'] - time_margin\
                meta_statistics['recovery_time_ci_upper'] = meta_statistics['meta_recovery_time_mean'] + time_margin\
        else:\
            meta_statistics.update(\{\
                'ci_lower': meta_statistics['meta_mean'],\
                'ci_upper': meta_statistics['meta_mean'],\
                'recovery_rate_ci_lower': meta_statistics['meta_recovery_rate_mean'],\
                'recovery_rate_ci_upper': meta_statistics['meta_recovery_rate_mean'],\
                'recovery_time_ci_lower': meta_statistics['meta_recovery_time_mean'],\
                'recovery_time_ci_upper': meta_statistics['meta_recovery_time_mean']\
            \})\
    else:\
        meta_statistics = \{\
            "meta_mean": 0.0, "meta_std": 0.0, "meta_sem": 0.0,\
            "meta_success_rate": 0.0, "n_master_seeds": 0, "total_experiments": 0,\
            "ci_lower": 0.0, "ci_upper": 0.0,\
            "meta_recovery_rate_mean": 0.0, "meta_recovery_rate_std": 0.0,\
            "meta_recovery_time_mean": 0.0, "meta_recovery_time_std": 0.0,\
            "recovery_rate_ci_lower": 0.0, "recovery_rate_ci_upper": 0.0,\
            "recovery_time_ci_lower": 0.0, "recovery_time_ci_upper": 0.0\
        \}\
\
    complete_result = \{\
        "individual_seeds": meta_results,\
        "meta_statistics": meta_statistics,\
        "seeds": seeds,\
        "env_name": env_name,\
        "comprehensive_analysis_included": True\
    \}\
\
    return complete_result\
\
\
# =====================\
# 6. META-ANALYSIS STATISTICAL COMPARISON\
# =====================\
\
def perform_meta_statistical_comparison(complete_meta_results):\
    """Perform statistical comparison across meta-analysis results"""\
\
    print("\\n META-ANALYSIS STATISTICAL COMPARISON")\
    print("=" * 60)\
\
    meta_comparison_results = \{\}\
\
    for env_name, env_results in complete_meta_results.items():\
        print(f"\\n Environment: \{env_name\}")\
        print("-" * 40)\
\
        # Extract meta-means for all agents\
        agent_meta_data = \{\}\
        for agent_name, agent_result in env_results.items():\
            meta_stats = agent_result['meta_statistics']\
            if meta_stats['n_master_seeds'] > 0:\
                # Collect individual seed results for statistical testing\
                seed_means = []\
                for seed_key, seed_result in agent_result['individual_seeds'].items():\
                    if seed_result['success_rate'] > 0:\
                        seed_means.append(seed_result['mean_reward'])\
\
                agent_meta_data[agent_name] = \{\
                    'seed_means': seed_means,\
                    'meta_mean': meta_stats['meta_mean'],\
                    'meta_std': meta_stats['meta_std'],\
                    'meta_sem': meta_stats['meta_sem'],\
                    'n_seeds': len(seed_means)\
                \}\
\
        # Perform pairwise comparisons\
        env_comparisons = \{\}\
        agent_names = list(agent_meta_data.keys())\
\
        for i in range(len(agent_names)):\
            for j in range(i+1, len(agent_names)):\
                agent1, agent2 = agent_names[i], agent_names[j]\
\
                data1 = agent_meta_data[agent1]['seed_means']\
                data2 = agent_meta_data[agent2]['seed_means']\
\
                if len(data1) >= 3 and len(data2) >= 3:\
                    # Perform appropriate statistical test\
                    try:\
                        # Step 1: Normality test (Shapiro-Wilk)\
                        _, p_norm1 = stats.shapiro(data1)\
                        _, p_norm2 = stats.shapiro(data2)\
\
                        if p_norm1 > 0.05 and p_norm2 > 0.05:\
                            # Both normal - Step 2: Equal variance test (Levene's)\
                            _, p_levene = stats.levene(data1, data2)\
\
                            if p_levene > 0.05:\
                                # Equal variances - use Student's t-test\
                                stat, p_val = stats.ttest_ind(data1, data2, equal_var=True)\
                                test_used = "Student's t-test"\
                            else:\
                                # Unequal variances - use Welch's t-test\
                                stat, p_val = stats.ttest_ind(data1, data2, equal_var=False)\
                                test_used = "Welch's t-test"\
                        else:\
                            # Non-normal - use Mann-Whitney U\
                            stat, p_val = stats.mannwhitneyu(data1, data2, alternative='two-sided')\
                            test_used = "Mann-Whitney U test"\
\
\
                        # Calculate effect size (Cohen's d)\
                        pooled_std = np.sqrt(((len(data1) - 1) * np.var(data1) +\
                                            (len(data2) - 1) * np.var(data2)) /\
                                           (len(data1) + len(data2) - 2))\
\
                        if pooled_std > 0:\
                            cohens_d = (np.mean(data1) - np.mean(data2)) / pooled_std\
                        else:\
                            cohens_d = 0\
\
                        comparison_key = f"\{agent1\}_vs_\{agent2\}"\
                        env_comparisons[comparison_key] = \{\
                            'mean_diff': np.mean(data1) - np.mean(data2),\
                            'p_value': p_val,\
                            'cohens_d': cohens_d,\
                            'test_used': test_used,\
                            'significant': p_val < 0.05,\
                            'effect_size_interpretation': interpret_cohens_d(cohens_d)\
                        \}\
\
                        # Print significant results\
                        if p_val < 0.05:\
                            direction = ">" if np.mean(data1) > np.mean(data2) else "<"\
                            print(f"  \{agent1\} \{direction\} \{agent2\}:")\
                            print(f"    p-value: \{p_val:.4f\} (\{test_used\})")\
                            print(f"    Effect size: \{cohens_d:.3f\} (\{interpret_cohens_d(cohens_d)\})")\
\
                    except Exception as e:\
                        print(f"    Statistical test failed for \{agent1\} vs \{agent2\}: \{e\}")\
\
        meta_comparison_results[env_name] = env_comparisons\
\
    return meta_comparison_results\
\
\
def interpret_cohens_d(d):\
    """Interpret Cohen's d effect size"""\
    abs_d = abs(d)\
    if abs_d < 0.2:\
        return "negligible"\
    elif abs_d < 0.5:\
        return "small"\
    elif abs_d < 0.8:\
        return "medium"\
    else:\
        return "large"\
\
\
# =====================\
# 7. VISUALIZATION AND REPORTING\
# =====================\
\
def save_csv_data(complete_meta_results, save_path="content/Results/meta_analysis"):\
    """Save comprehensive analysis data to CSV with statistical measures"""\
\
    import pandas as pd\
    import os\
\
    os.makedirs(save_path, exist_ok=True)\
\
    # Environment-level summary with full statistics\
    for env_name, env_results in complete_meta_results.items():\
        print(f"\\n Creating enhanced CSV for \{env_name\}")\
\
        # Agent-level summary data\
        agent_summary_data = []\
\
        # Individual experiment-level data\
        detailed_data = []\
\
        for agent_name, agent_result in env_results.items():\
            if agent_result['meta_statistics']['n_master_seeds'] > 0:\
                meta_stats = agent_result['meta_statistics']\
\
                # Agent summary with full statistics\
                agent_summary_data.append(\{\
                    'Environment': env_name,\
                    'Agent': agent_name,\
                    'Agent_Type': 'BASELINE' if agent_name in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',\
\
                    # Core Performance Metrics\
                    'Meta_Mean': meta_stats['meta_mean'],\
                    'Meta_Std': meta_stats['meta_std'],\
                    'Meta_SEM': meta_stats['meta_sem'],\
                    'CI_Lower_95': meta_stats.get('ci_lower', 0),\
                    'CI_Upper_95': meta_stats.get('ci_upper', 0),\
                    'CI_Width': meta_stats.get('ci_upper', 0) - meta_stats.get('ci_lower', 0),\
\
                    # Recovery Metrics\
                    'Recovery_Rate_Mean': meta_stats.get('meta_recovery_rate_mean', 0),\
                    'Recovery_Rate_Std': meta_stats.get('meta_recovery_rate_std', 0),\
                    'Recovery_Rate_CI_Lower': meta_stats.get('recovery_rate_ci_lower', 0),\
                    'Recovery_Rate_CI_Upper': meta_stats.get('recovery_rate_ci_upper', 0),\
\
                    'Recovery_Time_Mean': meta_stats.get('meta_recovery_time_mean', 0),\
                    'Recovery_Time_Std': meta_stats.get('meta_recovery_time_std', 0),\
                    'Recovery_Time_CI_Lower': meta_stats.get('recovery_time_ci_lower', 0),\
                    'Recovery_Time_CI_Upper': meta_stats.get('recovery_time_ci_upper', 0),\
\
                    # Experimental Details\
                    'N_Master_Seeds': meta_stats['n_master_seeds'],\
                    'Total_Experiments': meta_stats['total_experiments'],\
                    'Success_Rate': meta_stats['meta_success_rate'],\
\
                    # Derived Statistics\
                    'Coefficient_of_Variation': meta_stats['meta_std'] / meta_stats['meta_mean'] if meta_stats['meta_mean'] != 0 else 0,\
                    'Performance_Reliability': 1 - (meta_stats['meta_std'] / meta_stats['meta_mean']) if meta_stats['meta_mean'] != 0 else 0\
                \})\
\
                # Collect all individual experiment data with seed information\
                for seed_key, seed_result in agent_result['individual_seeds'].items():\
                    if seed_result['success_rate'] > 0:\
                        rewards = seed_result['rewards']  # Shape: (300, 200)\
                        env_instances = seed_result.get('env_instances', None)\
\
                        # Each of 300 runs for this master seed\
                        for run_idx in range(rewards.shape[0]):\
                            run_rewards = rewards[run_idx]\
                            run_mean = np.mean(run_rewards)\
\
                            # Calculate recovery metrics for this run\
                            run_rewards_reshaped = run_rewards.reshape(1, -1)\
                            recovery_rates = compute_unified_recovery_rate(run_rewards_reshaped, env_name, env_instances)\
                            recovery_times = measure_unified_recovery_time(run_rewards_reshaped, env_name, env_instances)\
\
                            detailed_data.append(\{\
                                'Environment': env_name,\
                                'Agent': agent_name,\
                                'Agent_Type': 'BASELINE' if agent_name in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',\
                                'Master_Seed': seed_result['master_seed'],\
                                'Run_Index': run_idx,\
                                'Global_Experiment_Index': len(detailed_data),  # Unique experiment ID\
\
                                # Performance Metrics\
                                'Mean_Reward': run_mean,\
                                'Recovery_Rate': np.mean(recovery_rates) if len(recovery_rates) > 0 else 0,\
                                'Recovery_Time': np.mean(recovery_times) if len(recovery_times) > 0 else 0,\
\
                                # Additional Run Statistics\
                                'Reward_Std': np.std(run_rewards),\
                                'Reward_Min': np.min(run_rewards),\
                                'Reward_Max': np.max(run_rewards),\
                                'Reward_Median': np.median(run_rewards),\
                                'Reward_Q25': np.percentile(run_rewards, 25),\
                                'Reward_Q75': np.percentile(run_rewards, 75),\
\
                                # Trial-wise Performance Indicators\
                                'Early_Performance': np.mean(run_rewards[:50]),  # First 50 trials\
                                'Late_Performance': np.mean(run_rewards[-50:]),  # Last 50 trials\
                                'Performance_Trend': np.polyfit(range(len(run_rewards)), run_rewards, 1)[0],  # Linear trend\
                            \})\
\
        # Save agent summary CSV\
        summary_df = pd.DataFrame(agent_summary_data)\
        summary_csv = f"\{save_path\}/agent_summary_statistics_\{env_name\}.csv"\
        summary_df.to_csv(summary_csv, index=False)\
        print(f"   Agent summary saved: \{summary_csv\} (\{len(summary_df)\} agents)")\
\
        # Save detailed experiment CSV\
        detailed_df = pd.DataFrame(detailed_data)\
        detailed_csv = f"\{save_path\}/detailed_experiments_\{env_name\}.csv"\
        detailed_df.to_csv(detailed_csv, index=False)\
        print(f"   Detailed data saved: \{detailed_csv\} (\{len(detailed_df)\} experiments)")\
\
\
def plot_emotion_trajectories_simple(emotion_data, env_name, save_path="emotion_plots"):\
    """Plot 8 emotions in 4x2 grid (vertical layout) showing average trajectory over trials"""\
\
    os.makedirs(save_path, exist_ok=True)\
\
    emotion_names = [\
        "Fear", "Joy", "Hope", "Sadness",\
        "Curiosity", "Anger", "Pride", "Shame"\
    ]\
\
    # Calculate average emotion trajectories across runs\
    avg_emotions = np.mean(emotion_data, axis=0)  # Shape: [n_trials, 8]\
    std_emotions = np.std(emotion_data, axis=0)   # Shape: [n_trials, 8]\
\
    n_trials = avg_emotions.shape[0]\
    trials = np.arange(n_trials)\
\
    # Create 4x2 subplot (4 rows, 2 columns) - CHANGED FROM 2x4\
    fig, axes = plt.subplots(4, 2, figsize=(12, 16))  # CHANGED: taller figure\
    axes = axes.flatten()\
\
    colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4',\
              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\
\
    for i in range(8):\
        ax = axes[i]\
        emotion_name = emotion_names[i]\
\
        # Plot mean trajectory\
        ax.plot(trials, avg_emotions[:, i], color=colors[i], linewidth=2,\
                label=f'Mean \{emotion_name\}')\
\
        # Plot confidence interval (mean \'b1 std)\
        ax.fill_between(trials,\
                       avg_emotions[:, i] - std_emotions[:, i],\
                       avg_emotions[:, i] + std_emotions[:, i],\
                       color=colors[i], alpha=0.3)\
\
        ax.set_title(f'\{emotion_name\}', fontweight='bold', fontsize=12)\
        ax.set_xlabel('Trial')\
        ax.set_ylabel('Emotion Intensity')\
        ax.grid(True, alpha=0.3)\
        ax.set_ylim(0, 1)\
\
        # Add some stats\
        mean_intensity = np.mean(avg_emotions[:, i])\
        ax.text(0.02, 0.98, f'Avg: \{mean_intensity:.3f\}',\
                transform=ax.transAxes, fontsize=9,\
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8),\
                verticalalignment='top')\
\
    plt.suptitle(f'ECIA_Full Emotion Trajectories - \{env_name\}\\n'\
                 f'Average across \{emotion_data.shape[0]\} runs',\
                 fontsize=14, fontweight='bold')\
    plt.tight_layout()\
\
    # Save plot\
    plt.savefig(f"\{save_path\}/emotion_trajectories_\{env_name\}.png", dpi=300, bbox_inches='tight')\
    plt.close()\
\
    print(f" Emotion plot saved: \{save_path\}/emotion_trajectories_\{env_name\}.png")\
\
def plot_emotion_comparison_all_envs(data_path="content/Results/simple_emotion_analysis"):\
    """Create comparison plot showing all environments together - 4x2 layout"""\
\
    environments = ["EnvA", "EnvB", "EnvC"]\
    emotion_names = [\
        "Fear", "Joy", "Hope", "Sadness",\
        "Curiosity", "Anger", "Pride", "Shame"\
    ]\
\
    # Load all data\
    all_env_data = \{\}\
    for env_name in environments:\
        data_file = f"\{data_path\}/emotion_data_\{env_name\}.npy"\
        if os.path.exists(data_file):\
            emotion_data = np.load(data_file)\
            avg_emotions = np.mean(emotion_data, axis=0)  # Average across runs\
            all_env_data[env_name] = avg_emotions\
\
    if not all_env_data:\
        print(" No emotion data found for comparison plot")\
        return\
\
    # Create comparison plot - CHANGED TO 4x2\
    fig, axes = plt.subplots(4, 2, figsize=(12, 16))  # CHANGED: 4 rows, 2 columns\
    axes = axes.flatten()\
\
    colors = \{'EnvA': '#1f77b4', 'EnvB': '#ff7f0e', 'EnvC': '#2ca02c'\}\
\
    for i in range(8):\
        ax = axes[i]\
        emotion_name = emotion_names[i]\
\
        for env_name, avg_emotions in all_env_data.items():\
            trials = np.arange(len(avg_emotions))\
            ax.plot(trials, avg_emotions[:, i],\
                   color=colors[env_name], linewidth=2,\
                   label=env_name, alpha=0.8)\
\
        ax.set_title(f'\{emotion_name\}', fontweight='bold', fontsize=12)\
        ax.set_xlabel('Trial')\
        ax.set_ylabel('Emotion Intensity')\
        ax.grid(True, alpha=0.3)\
        ax.set_ylim(0, 1)\
\
        if i == 0:  # Add legend to first subplot\
            ax.legend(loc='upper right')\
\
    plt.suptitle('ECIA_Full Emotion Trajectories - Environment Comparison',\
                 fontsize=16, fontweight='bold')\
    plt.tight_layout()\
\
    # Save comparison plot\
    plt.savefig(f"\{data_path\}/emotion_comparison_all_environments.png",\
                dpi=300, bbox_inches='tight')\
    plt.close()\
\
    print(f" Comparison plot saved: \{data_path\}/emotion_comparison_all_environments.png")\
\
\
def save_enhanced_cross_dataset_csv_data(complete_results, save_path="content/Results/cross_dataset_study"):\
    """Save comprehensive cross-dataset study data to CSV with statistical measures"""\
\
    import pandas as pd\
    import os\
\
    os.makedirs(save_path, exist_ok=True)\
\
    # Agent-level summary data\
    agent_summary_data = []\
\
    # Individual experiment-level data\
    detailed_data = []\
\
    for agent_name, agent_result in complete_results.items():\
        meta_stats = agent_result['meta_statistics']\
\
        # Agent summary with full statistics\
        agent_summary_data.append(\{\
            'Agent': agent_name,\
            'Agent_Type': 'BASELINE' if agent_name in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',\
\
            # Core Performance Metrics\
            'Overall_Performance_Mean': meta_stats.get('overall_performance_mean', 0),\
            'Overall_Performance_Std': meta_stats.get('overall_performance_std', 0),\
\
            # Recovery Metrics with CI\
            'Recovery_Rate_Mean': meta_stats.get('recovery_rate_mean', 0),\
            'Recovery_Rate_Std': meta_stats.get('recovery_rate_std', 0),\
\
            'Recovery_Time_Mean': meta_stats.get('recovery_time_mean', 0),\
            'Recovery_Time_Std': meta_stats.get('recovery_time_std', 0),\
\
            # Experimental Details\
            'N_Master_Seeds': meta_stats.get('n_master_seeds', 0),\
            'Total_Experiments': meta_stats.get('n_total_experiments', 0),\
\
            # Derived Statistics\
            'Performance_CV': meta_stats.get('overall_performance_std', 0) / meta_stats.get('overall_performance_mean', 1),\
            'Recovery_Efficiency': meta_stats.get('recovery_rate_mean', 0) / max(meta_stats.get('recovery_time_mean', 1), 1)\
        \})\
\
        # Collect individual experiment data\
        for seed_key, seed_result in agent_result['individual_seeds'].items():\
            if seed_result['n_experiments'] > 0:\
                # Each experiment within this seed\
                overall_perfs = seed_result['overall_performances']\
                recovery_rates = seed_result['recovery_rates']\
                recovery_times = seed_result['recovery_times']\
\
                for exp_idx in range(len(overall_perfs)):\
                    detailed_data.append(\{\
                        'Agent': agent_name,\
                        'Agent_Type': 'BASELINE' if agent_name in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',\
                        'Master_Seed': seed_result['master_seed'],\
                        'Experiment_Index': exp_idx,\
                        'Global_Experiment_Index': len(detailed_data),\
\
                        # Core Metrics\
                        'Overall_Performance': overall_perfs[exp_idx] if exp_idx < len(overall_perfs) else 0,\
                        'Recovery_Rate': recovery_rates[exp_idx] if exp_idx < len(recovery_rates) else 0,\
                        'Recovery_Time': recovery_times[exp_idx] if exp_idx < len(recovery_times) else 0,\
\
                        # Performance Categories\
                        'Performance_Category': 'High' if (exp_idx < len(overall_perfs) and overall_perfs[exp_idx] > 0.7) else\
                                              'Medium' if (exp_idx < len(overall_perfs) and overall_perfs[exp_idx] > 0.5) else 'Low',\
                        'Recovery_Category': 'Fast' if (exp_idx < len(recovery_times) and recovery_times[exp_idx] < 20) else\
                                           'Medium' if (exp_idx < len(recovery_times) and recovery_times[exp_idx] < 40) else 'Slow'\
                    \})\
\
    # Calculate confidence intervals for agent summaries\
    for i, agent_data in enumerate(agent_summary_data):\
        agent_name = agent_data['Agent']\
        if agent_name in complete_results:\
            # Collect all experiment data for CI calculation\
            all_overall_perfs = []\
            all_recovery_rates = []\
            all_recovery_times = []\
\
            for seed_result in complete_results[agent_name]['individual_seeds'].values():\
                if seed_result['n_experiments'] > 0:\
                    all_overall_perfs.extend(seed_result['overall_performances'])\
                    all_recovery_rates.extend(seed_result['recovery_rates'])\
                    all_recovery_times.extend(seed_result['recovery_times'])\
\
            # Calculate 95% CI\
            if len(all_overall_perfs) > 1:\
                alpha = 0.05\
                t_critical = stats.t.ppf(1 - alpha/2, len(all_overall_perfs) - 1)\
\
                # Overall Performance CI\
                perf_sem = np.std(all_overall_perfs) / np.sqrt(len(all_overall_perfs))\
                perf_margin = t_critical * perf_sem\
                agent_summary_data[i]['Overall_Performance_CI_Lower'] = np.mean(all_overall_perfs) - perf_margin\
                agent_summary_data[i]['Overall_Performance_CI_Upper'] = np.mean(all_overall_perfs) + perf_margin\
                agent_summary_data[i]['Overall_Performance_SEM'] = perf_sem\
\
                # Recovery Rate CI\
                if all_recovery_rates:\
                    recovery_sem = np.std(all_recovery_rates) / np.sqrt(len(all_recovery_rates))\
                    recovery_margin = t_critical * recovery_sem\
                    agent_summary_data[i]['Recovery_Rate_CI_Lower'] = np.mean(all_recovery_rates) - recovery_margin\
                    agent_summary_data[i]['Recovery_Rate_CI_Upper'] = np.mean(all_recovery_rates) + recovery_margin\
                    agent_summary_data[i]['Recovery_Rate_SEM'] = recovery_sem\
\
                # Recovery Time CI\
                if all_recovery_times:\
                    time_sem = np.std(all_recovery_times) / np.sqrt(len(all_recovery_times))\
                    time_margin = t_critical * time_sem\
                    agent_summary_data[i]['Recovery_Time_CI_Lower'] = np.mean(all_recovery_times) - time_margin\
                    agent_summary_data[i]['Recovery_Time_CI_Upper'] = np.mean(all_recovery_times) + time_margin\
                    agent_summary_data[i]['Recovery_Time_SEM'] = time_sem\
\
    # Save agent summary CSV\
    summary_df = pd.DataFrame(agent_summary_data)\
    summary_csv = f"\{save_path\}/cross_dataset_agent_summary_statistics.csv"\
    summary_df.to_csv(summary_csv, index=False)\
    print(f"  \uc0\u55357 \u56516  Agent summary saved: \{summary_csv\} (\{len(summary_df)\} agents)")\
\
    # Save detailed experiment CSV\
    detailed_df = pd.DataFrame(detailed_data)\
    detailed_csv = f"\{save_path\}/cross_dataset_detailed_experiments.csv"\
    detailed_df.to_csv(detailed_csv, index=False)\
    print(f"   Detailed data saved: \{detailed_csv\} (\{len(detailed_df)\} experiments)")\
\
\
def create_statistical_comparison_csv(complete_meta_results, meta_comparisons, save_path="content/Results/meta_analysis"):\
    """Create CSV with statistical comparison results"""\
\
    import pandas as pd\
    import os\
\
    os.makedirs(save_path, exist_ok=True)\
\
    comparison_data = []\
\
    for env_name, env_comparisons in meta_comparisons.items():\
        for comparison_name, comparison_result in env_comparisons.items():\
            agent1, agent2 = comparison_name.split('_vs_')\
\
            comparison_data.append(\{\
                'Environment': env_name,\
                'Agent_1': agent1,\
                'Agent_2': agent2,\
                'Agent_1_Type': 'BASELINE' if agent1 in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',\
                'Agent_2_Type': 'BASELINE' if agent2 in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',\
\
                # Comparison Results\
                'Mean_Difference': comparison_result['mean_diff'],\
                'P_Value': comparison_result['p_value'],\
                'Cohens_D': comparison_result['cohens_d'],\
                'Effect_Size_Interpretation': comparison_result['effect_size_interpretation'],\
                'Test_Used': comparison_result['test_used'],\
                'Significant': comparison_result['significant'],\
\
                # Performance Direction\
                'Better_Agent': agent1 if comparison_result['mean_diff'] > 0 else agent2,\
                'Performance_Advantage': abs(comparison_result['mean_diff']),\
                'Effect_Size_Category': 'Large' if abs(comparison_result['cohens_d']) >= 0.8 else\
                                       'Medium' if abs(comparison_result['cohens_d']) >= 0.5 else\
                                       'Small' if abs(comparison_result['cohens_d']) >= 0.2 else 'Negligible'\
            \})\
\
    # Save comparison results\
    comparison_df = pd.DataFrame(comparison_data)\
    comparison_csv = f"\{save_path\}/statistical_comparisons.csv"\
    comparison_df.to_csv(comparison_csv, index=False)\
    print(f" Statistical comparisons saved: \{comparison_csv\}")\
\
    return comparison_df\
\
\
\
def generate_cross_dataset_unified_report(complete_results, save_path="content/Results/cross_dataset_study"):\
    """Generate comprehensive cross-dataset study report with unified metrics"""\
\
    os.makedirs(save_path, exist_ok=True)\
\
    report_lines = []\
    report_lines.append("CROSS-DATASET STUDY ANALYSIS REPORT (UNIFIED METRICS)")\
    report_lines.append("=" * 70)\
    report_lines.append("Study Design: EnvA,B,C Sequential Training \uc0\u8594  RandomShift Testing")\
    report_lines.append(f"Total Experiments: 3 master seeds \'d7 300 runs = 900 per agent")\
    report_lines.append("Metrics: Unified performance indicators consistent with single analysis")\
    report_lines.append("")\
\
    # Executive Summary\
    report_lines.append("EXECUTIVE SUMMARY")\
    report_lines.append("-" * 30)\
\
    # Calculate overall statistics using unified metrics\
    all_overall_perfs = []\
    all_recovery_rates = []\
    all_recovery_times = []\
    agent_names = list(complete_results.keys())\
\
    for agent_result in complete_results.values():\
        meta_stats = agent_result['meta_statistics']\
        all_overall_perfs.append(meta_stats.get('overall_performance_mean', 0))\
        all_recovery_rates.append(meta_stats.get('recovery_rate_mean', 0))\
        all_recovery_times.append(meta_stats.get('recovery_time_mean', 0))\
\
    report_lines.append(f"\'95 Total Agents Evaluated: \{len(agent_names)\}")\
    report_lines.append(f"\'95 Average Overall Performance: \{np.mean(all_overall_perfs):.4f\} \'b1 \{np.std(all_overall_perfs):.4f\}")\
    report_lines.append(f"\'95 Average Recovery Rate: \{np.mean(all_recovery_rates):.4f\} \'b1 \{np.std(all_recovery_rates):.4f\}")\
    report_lines.append(f"\'95 Average Recovery Time: \{np.mean(all_recovery_times):.2f\} \'b1 \{np.std(all_recovery_times):.2f\} trials")\
    report_lines.append("")\
\
    # Agent Rankings by Overall Performance\
    report_lines.append("AGENT RANKINGS BY OVERALL PERFORMANCE (UNIFIED METRICS)")\
    report_lines.append("-" * 55)\
\
    agent_performance = []\
    for agent_name, agent_result in complete_results.items():\
        meta_stats = agent_result['meta_statistics']\
        agent_performance.append((agent_name, meta_stats.get('overall_performance_mean', 0), meta_stats))\
\
    agent_performance.sort(key=lambda x: x[1], reverse=True)\
\
    for rank, (agent_name, overall_perf, meta_stats) in enumerate(agent_performance, 1):\
        agent_type = "BASELINE" if agent_name in ["EpsilonGreedy", "UCB", "TS"] else "ECIA"\
\
        report_lines.append(\
            f"  \{rank:2d\}. [\{agent_type:8s\}] \{agent_name:15s\} | "\
            f"Overall: \{overall_perf:.4f\} | "\
            f"Recovery: \{meta_stats.get('recovery_rate_mean', 0):.3f\} | "\
        )\
\
    report_lines.append("")\
\
    # Detailed Performance Metrics\
    report_lines.append("DETAILED PERFORMANCE METRICS")\
    report_lines.append("-" * 35)\
\
    for agent_name, agent_result in complete_results.items():\
        meta_stats = agent_result['meta_statistics']\
        report_lines.append(f"\\n\{agent_name\}:")\
        report_lines.append(f"  \'95 Overall Performance: \{meta_stats.get('overall_performance_mean', 0):.4f\} \'b1 \{meta_stats.get('overall_performance_std', 0):.4f\}")\
        report_lines.append(f"  \'95 Recovery Rate: \{meta_stats.get('recovery_rate_mean', 0):.4f\} \'b1 \{meta_stats.get('recovery_rate_std', 0):.4f\}")\
        report_lines.append(f"  \'95 Recovery Time: \{meta_stats.get('recovery_time_mean', 0):.2f\} \'b1 \{meta_stats.get('recovery_time_std', 0):.2f\} trials")\
\
    report_lines.append("")\
\
    # Key Findings\
    report_lines.append("KEY FINDINGS")\
    report_lines.append("-" * 20)\
\
    # Best performer\
    if agent_performance:\
        best_agent, best_score, _ = agent_performance[0]\
        report_lines.append(f"\'95 Best Overall Performance: \{best_agent\} (Score: \{best_score:.4f\})")\
\
        # ECIA vs Baseline comparison using unified metrics\
        ecia_overall_perfs = []\
        baseline_overall_perfs = []\
        ecia_recovery_rates = []\
        baseline_recovery_rates = []\
\
        for agent_name, agent_result in complete_results.items():\
            meta_stats = agent_result['meta_statistics']\
\
            if "ECIA" in agent_name:\
                ecia_overall_perfs.append(meta_stats.get('overall_performance_mean', 0))\
                ecia_recovery_rates.append(meta_stats.get('recovery_rate_mean', 0))\
            elif agent_name in ["EpsilonGreedy", "UCB", "TS"]:\
                baseline_overall_perfs.append(meta_stats.get('overall_performance_mean', 0))\
                baseline_recovery_rates.append(meta_stats.get('recovery_rate_mean', 0))\
\
        if ecia_overall_perfs and baseline_overall_perfs:\
            report_lines.append(f"\'95 ECIA vs Baseline Overall Performance:")\
            report_lines.append(f"  - ECIA Average: \{np.mean(ecia_overall_perfs):.4f\} \'b1 \{np.std(ecia_overall_perfs):.4f\}")\
            report_lines.append(f"  - Baseline Average: \{np.mean(baseline_overall_perfs):.4f\} \'b1 \{np.std(baseline_overall_perfs):.4f\}")\
\
            report_lines.append(f"\'95 ECIA vs Baseline Recovery Rate:")\
            report_lines.append(f"  - ECIA Average: \{np.mean(ecia_recovery_rates):.4f\} \'b1 \{np.std(ecia_recovery_rates):.4f\}")\
            report_lines.append(f"  - Baseline Average: \{np.mean(baseline_recovery_rates):.4f\} \'b1 \{np.std(baseline_recovery_rates):.4f\}")\
\
\
    report_lines.append("")\
\
    # Methodology\
    report_lines.append("METHODOLOGY")\
    report_lines.append("-" * 20)\
    report_lines.append("\'95 Training Phase: Sequential learning on EnvA (200 trials) \uc0\u8594  EnvB (200 trials) \u8594  EnvC (200 trials)")\
    report_lines.append("\'95 Testing Phase: Continued learning on RandomShiftEnvironment (600 trials)")\
    report_lines.append("\'95 Unified Metrics: Same calculation methods as single environment analysis")\
    report_lines.append("  - Overall Performance: Mean reward across test phase")\
    report_lines.append("  - Recovery Rate: Performance recovery after environment changes")\
    report_lines.append("  - Recovery Time: Trials needed to recover from performance drops")\
    report_lines.append("\'95 Seeds: 3 master seeds (42, 123, 456) \'d7 300 runs each")\
\
    # Save report\
    with open(f"\{save_path\}/cross_dataset_study_report_unified.txt", "w") as f:\
        f.write("\\n".join(report_lines))\
\
    print(f" Cross-dataset study unified report saved: \{save_path\}/cross_dataset_study_report_unified.txt")\
    return "\\n".join(report_lines)\
\
\
# =====================\
# 8. COMPREHENSIVE ANALYSIS FUNCTIONS (INTEGRATED)\
# =====================\
\
def get_environment_change_points(env_name, env_instance=None):\
    """Get change points for each environment"""\
    if env_name == "EnvA":\
        return [100]\
    elif env_name == "EnvB":\
        return [40, 80, 120, 160]\
    elif env_name == "EnvC":\
        if env_instance and hasattr(env_instance, 'change_points'):\
            return env_instance.change_points\
        else:\
            return []\
    else:\
        return []\
\
def get_environment_optimal_rewards(env_name, env_instance=None):\
    """Get optimal rewards for each segment"""\
    if env_name == "EnvA":\
        return [0.8, 0.9]\
    elif env_name == "EnvB":\
        return [0.95, 0.95, 0.95, 0.95, 0.95]\
    elif env_name == "EnvC":\
        if env_instance and hasattr(env_instance, 'optimal_rewards'):\
            return env_instance.optimal_rewards\
        else:\
            return []\
    else:\
        return []\
\
def calculate_noise_adjusted_parameters(env_name):\
    """Calculate parameters adjusted for noise level"""\
    noise_levels = \{\
        "EnvA": \{"sigma": 0.15, "noise_level": "medium"\},\
        "EnvB": \{"sigma": 0.05, "noise_level": "low"\},\
        "EnvC": \{"sigma": 0.15, "noise_level": "medium"\}\
    \}\
\
    noise_info = noise_levels.get(env_name, \{"noise_level": "medium"\})\
\
    if noise_info["noise_level"] == "low":\
        threshold_ratio = 0.90\
        stability_window = 3\
        min_stability_trials = 2\
    elif noise_info["noise_level"] == "medium":\
        threshold_ratio = 0.85\
        stability_window = 5\
        min_stability_trials = 3\
    else:\
        threshold_ratio = 0.80\
        stability_window = 7\
        min_stability_trials = 4\
\
    return \{\
        "threshold_ratio": threshold_ratio,\
        "stability_window": stability_window,\
        "min_stability_trials": min_stability_trials\
    \}\
\
def detect_randomshift_changes_from_rewards(rewards, window_size=20):\
    change_points = []\
\
\
    for i in range(window_size, len(rewards) - window_size):\
        before_window = rewards[i-window_size:i]\
        after_window = rewards[i:i+window_size]\
\
        # Calculate mean difference\
        mean_diff = abs(np.mean(after_window) - np.mean(before_window))\
\
        # Lower threshold for better detection\
        if mean_diff > 0.10:  # Reduced from 0.15 to 0.10\
            change_points.append(i)\
\
    # Remove too close change points\
    filtered_points = []\
    for point in change_points:\
        if not filtered_points or point - filtered_points[-1] > 25:  # Reduced from 30 to 25\
            filtered_points.append(point)\
\
    return filtered_points\
\
def compute_unified_recovery_rate(rewards, env_name, env_instances=None, analysis_window=30):\
\
    env_params = calculate_noise_adjusted_parameters(env_name)\
    all_recovery_rates = []\
\
    for run_idx in range(rewards.shape[0]):\
        run_rewards = rewards[run_idx]\
\
        # RandomShift\uc0\u51032  \u44221 \u50864  \u48372 \u49345 \u50640 \u49436  \u51649 \u51217  \u48320 \u54868 \u51216  \u44048 \u51648 \
        if env_name == "RandomShift":\
            change_points = detect_randomshift_changes_from_rewards(run_rewards)\
            if not change_points:\
                continue\
            optimal_rewards = [0.8] * len(change_points)\
        else:\
            if env_name == "EnvC":\
                if env_instances is not None and len(env_instances) > run_idx:\
                    actual_env = env_instances[run_idx]\
                    change_points = actual_env.change_points if hasattr(actual_env, 'change_points') else get_environment_change_points(env_name)\
                    optimal_rewards = actual_env.optimal_rewards if hasattr(actual_env, 'optimal_rewards') else get_environment_optimal_rewards(env_name)\
                else:\
                    change_points = get_environment_change_points(env_name)\
                    optimal_rewards = get_environment_optimal_rewards(env_name)\
            else:\
                change_points = get_environment_change_points(env_name)\
                optimal_rewards = get_environment_optimal_rewards(env_name)\
\
        run_recovery_rates = []\
\
        for change_idx, change_point in enumerate(change_points):\
            if change_point >= len(run_rewards) - analysis_window:\
                continue\
\
            post_start = change_point\
            post_end = min(change_point + analysis_window, len(run_rewards))\
\
            if post_end <= post_start:\
                continue\
\
            segment_optimal = optimal_rewards[min(change_idx, len(optimal_rewards) - 1)]\
            post_change_performance = np.mean(run_rewards[post_start:post_end])\
            recovery_rate = post_change_performance / segment_optimal\
\
\
            run_recovery_rates.append(recovery_rate)\
\
        if run_recovery_rates:\
            avg_recovery_rate = np.mean(run_recovery_rates)\
            all_recovery_rates.append(avg_recovery_rate)\
\
    return np.array(all_recovery_rates)\
\
def measure_unified_recovery_time(rewards, env_name, env_instances=None, analysis_window=50):\
\
    env_params = calculate_noise_adjusted_parameters(env_name)\
    all_recovery_times = []\
\
    for run_idx in range(rewards.shape[0]):\
        run_rewards = rewards[run_idx]\
\
        # RandomShift\uc0\u51032  \u44221 \u50864  \u48372 \u49345 \u50640 \u49436  \u51649 \u51217  \u48320 \u54868 \u51216  \u44048 \u51648 \
        if env_name == "RandomShift":\
            change_points = detect_randomshift_changes_from_rewards(run_rewards)\
            if not change_points:\
                continue\
            optimal_rewards = [0.8] * len(change_points)\
        else:\
            if env_name == "EnvC":\
                if env_instances is not None and len(env_instances) > run_idx:\
                    actual_env = env_instances[run_idx]\
                    change_points = actual_env.change_points if hasattr(actual_env, 'change_points') else get_environment_change_points(env_name)\
                    optimal_rewards = actual_env.optimal_rewards if hasattr(actual_env, 'optimal_rewards') else get_environment_optimal_rewards(env_name)\
                else:\
                    change_points = get_environment_change_points(env_name)\
                    optimal_rewards = get_environment_optimal_rewards(env_name)\
            else:\
                change_points = get_environment_change_points(env_name)\
                optimal_rewards = get_environment_optimal_rewards(env_name)\
\
        run_recovery_times = []\
\
        for change_idx, change_point in enumerate(change_points):\
            if change_point >= len(run_rewards) - 10:\
                continue\
\
            segment_optimal = optimal_rewards[min(change_idx, len(optimal_rewards) - 1)]\
            threshold = segment_optimal * env_params["threshold_ratio"]\
\
            post_change_start = change_point\
            post_change_end = min(change_point + analysis_window, len(run_rewards))\
            post_change_rewards = run_rewards[post_change_start:post_change_end]\
\
            recovery_time = len(post_change_rewards)\
            stability_window = env_params["stability_window"]\
            min_trials = env_params["min_stability_trials"]\
\
            for i in range(min_trials, len(post_change_rewards) - stability_window + 1):\
                window = post_change_rewards[i:i + stability_window]\
                if np.mean(window) >= threshold:\
                    recovery_time = i + stability_window // 2\
                    break\
\
            run_recovery_times.append(recovery_time)\
\
        if run_recovery_times:\
            avg_recovery_time = np.mean(run_recovery_times)\
            all_recovery_times.append(avg_recovery_time)\
\
    return np.array(all_recovery_times)\
\
\
def calculate_cross_dataset_unified_metrics(training_result, test_result, env_name="RandomShift"):\
\
    # Get test rewards and environment instance\
    test_rewards = test_result['test_rewards'].reshape(1, -1)  # Shape: (1, n_trials)\
    env_instances = [test_result['environment_instance']] if test_result.get('environment_instance') else None\
\
\
    try:\
        # Recovery rate calculation\
        recovery_rates = compute_unified_recovery_rate(test_rewards, env_name, env_instances)\
        recovery_rate_mean = np.mean(recovery_rates) if len(recovery_rates) > 0 else 0.0\
        recovery_rate_std = np.std(recovery_rates) if len(recovery_rates) > 0 else 0.0\
\
        # Recovery time measurement\
        recovery_times = measure_unified_recovery_time(test_rewards, env_name, env_instances)\
        recovery_time_mean = np.mean(recovery_times) if len(recovery_times) > 0 else 0.0\
        recovery_time_std = np.std(recovery_times) if len(recovery_times) > 0 else 0.0\
\
    except Exception as e:\
        import traceback\
        traceback.print_exc()\
\
        recovery_rate_mean = recovery_rate_std = 0.0\
        recovery_time_mean = recovery_time_std = 0.0\
    return \{\
        'overall_performance': test_result['test_performance'],\
        'recovery_rate_mean': recovery_rate_mean,\
        'recovery_rate_std': recovery_rate_std,\
        'recovery_time_mean': recovery_time_mean,\
        'recovery_time_std': recovery_time_std\
    \}\
# =====================\
# CROSS-DATASET & EMOTION TRAJECTORY\
# =====================\
\
import numpy as np\
import matplotlib.pyplot as plt\
import seaborn as sns\
import pandas as pd\
import os\
import pickle\
from tqdm import tqdm\
from collections import defaultdict, deque\
from scipy import stats\
import warnings\
warnings.filterwarnings('ignore')\
\
def train_agent_on_ABC_environments(agent_class, agent_kwargs, experiment_seed=42):\
    """Train agent on EnvA, B, C sequentially without reset"""\
\
    # Initialize agent with seed\
    agent_kwargs_with_seed = agent_kwargs.copy()\
    agent_kwargs_with_seed['random_state'] = experiment_seed + 1\
    agent = agent_class(**agent_kwargs_with_seed)\
\
    environments = [\
        (EnvironmentA, "EnvA"),\
        (EnvironmentB, "EnvB"),\
        (EnvironmentC, "EnvC")\
    ]\
\
    all_rewards = []\
    all_environments_performance = \{\}\
    all_env_instances = []\
\
    for env_class, env_name in environments:\
\
        # Initialize environment\
        env = env_class(random_state=experiment_seed + hash(env_name) % 1000)\
        env.reset()\
\
        env_rewards = []\
\
        # Train for 200 trials\
        for t in range(200):\
            action = agent.select_action()\
            reward, context = env.step(action)\
\
            # Update agent (cumulative learning)\
            if hasattr(agent, 'context'):  # ECIA family\
                agent.update(context, action, reward)\
            else:  # Baseline agents\
                agent.update(action, reward)\
\
            env_rewards.append(reward)\
\
        all_rewards.extend(env_rewards)\
        all_environments_performance[env_name] = \{\
            'rewards': np.array(env_rewards),\
            'mean_performance': np.mean(env_rewards)\
        \}\
        all_env_instances.append((env, env_name))\
\
    return \{\
        'trained_agent': agent,\
        'training_rewards': np.array(all_rewards),\
        'environments_performance': all_environments_performance,\
        'total_training_performance': np.mean(all_rewards),\
        'env_instances': all_env_instances\
    \}\
\
def test_agent_on_randomshift(trained_agent, experiment_seed=42, test_trials=100):\
    """Test trained agent on RandomShiftEnvironment with unified metrics"""\
\
    # Initialize test environment\
    env = RandomShiftEnvironment(\
        total_trials=test_trials,\
        random_state=experiment_seed + 999\
    )\
    env.reset()\
\
    test_rewards = []\
\
    for t in range(test_trials):\
        action = trained_agent.select_action()\
        reward, context = env.step(action)\
\
        # Continue learning in new environment\
        if hasattr(trained_agent, 'context'):  # ECIA family\
            trained_agent.update(context, action, reward)\
        else:  # Baseline agents\
            trained_agent.update(action, reward)\
\
        test_rewards.append(reward)\
\
    return \{\
        'test_rewards': np.array(test_rewards),\
        'test_performance': np.mean(test_rewards),\
        'environment_instance': env\
    \}\
\
def calculate_cross_dataset_unified_metrics(training_result, test_result, env_name="RandomShift"):\
    """Calculate unified metrics for cross-dataset results using single analysis methods"""\
\
     # Get test rewards and environment instance\
    test_rewards = test_result['test_rewards'].reshape(1, -1)  # Shape: (1, n_trials)\
    env_instances = [test_result['environment_instance']] if test_result.get('environment_instance') else None\
\
    try:\
        # Recovery rate calculation\
        recovery_rates = compute_unified_recovery_rate(test_rewards, env_name, env_instances)\
        recovery_rate_mean = np.mean(recovery_rates) if len(recovery_rates) > 0 else 0.0\
        recovery_rate_std = np.std(recovery_rates) if len(recovery_rates) > 0 else 0.0\
\
        # Recovery time measurement\
        recovery_times = measure_unified_recovery_time(test_rewards, env_name, env_instances)\
        recovery_time_mean = np.mean(recovery_times) if len(recovery_times) > 0 else 0.0\
        recovery_time_std = np.std(recovery_times) if len(recovery_times) > 0 else 0.0\
\
\
    except Exception as e:\
        print(f"Warning: Unified metrics calculation failed: \{e\}")\
        recovery_rate_mean = recovery_rate_std = 0.0\
        recovery_time_mean = recovery_time_std = 0.0\
\
    return \{\
        'overall_performance': test_result['test_performance'],\
        'recovery_rate_mean': recovery_rate_mean,\
        'recovery_rate_std': recovery_rate_std,\
        'recovery_time_mean': recovery_time_mean,\
        'recovery_time_std': recovery_time_std\
    \}\
\
def run_single_cross_dataset_experiment(agent_class, agent_kwargs, experiment_seed=42):\
    """Run single complete cross-dataset experiment with unified metrics"""\
\
    try:\
        # Phase 1: Train on EnvA, B, C\
        training_result = train_agent_on_ABC_environments(\
            agent_class, agent_kwargs, experiment_seed\
        )\
\
        # Phase 2: Test on RandomShift (with continued learning)\
        test_result = test_agent_on_randomshift(\
            training_result['trained_agent'], experiment_seed, test_trials=600\
        )\
\
        # Phase 3: Calculate unified metrics\
        unified_metrics = calculate_cross_dataset_unified_metrics(\
            training_result, test_result, "RandomShift"\
        )\
\
        return \{\
            'training_result': training_result,\
            'test_result': test_result,\
            'unified_metrics': unified_metrics,\
            'experiment_seed': experiment_seed,\
            'success': True\
        \}\
\
    except Exception as e:\
        print(f"    Experiment failed: \{e\}")\
        return \{\
            'training_result': None,\
            'test_result': None,\
            'unified_metrics': None,\
            'experiment_seed': experiment_seed,\
            'success': False\
        \}\
\
def run_cross_dataset_meta_analysis_for_agent(agent_class, agent_kwargs, agent_name="Agent"):\
    """Run cross-dataset meta-analysis for one agent with unified metrics"""\
\
    # Cross-dataset specific seeds and runs\
    cross_dataset_seeds = [34, 55, 89]\
    n_runs_per_seed = 300\
\
    print(f"\uc0\u55357 \u56580  Cross-Dataset Study: \{agent_name\}")\
\
    meta_results = \{\}\
\
    for master_seed in cross_dataset_seeds:\
        print(f"  \uc0\u55356 \u57088  Master Seed \{master_seed\}: Running \{n_runs_per_seed\} experiments...")\
\
        # Generate experiment seeds\
        np.random.seed(master_seed)\
        experiment_seeds = np.random.randint(0, 1000000, size=n_runs_per_seed)\
\
        seed_results = []\
\
        for experiment_seed in tqdm(experiment_seeds, desc=f"Seed-\{master_seed\}"):\
            result = run_single_cross_dataset_experiment(\
                agent_class, agent_kwargs, experiment_seed\
            )\
\
            if result['success']:\
                seed_results.append(result)\
\
        # Aggregate results for this master seed using unified metrics\
        if seed_results:\
            # Collect unified metrics\
            overall_performances = [r['unified_metrics']['overall_performance'] for r in seed_results]\
            recovery_rates = [r['unified_metrics']['recovery_rate_mean'] for r in seed_results]\
            recovery_times = [r['unified_metrics']['recovery_time_mean'] for r in seed_results]\
\
            # Traditional metrics for comparison\
            training_performances = [r['training_result']['total_training_performance'] for r in seed_results]\
            test_performances = [r['test_result']['test_performance'] for r in seed_results]\
\
            meta_results[f"seed_\{master_seed\}"] = \{\
                'master_seed': master_seed,\
                'n_experiments': len(seed_results),\
\
                # Unified metrics (primary)\
                'overall_performances': overall_performances,\
                'recovery_rates': recovery_rates,\
                'recovery_times': recovery_times,\
\
                # Aggregated unified metrics\
                'mean_overall_performance': np.mean(overall_performances),\
                'std_overall_performance': np.std(overall_performances),\
                'mean_recovery_rate': np.mean(recovery_rates),\
                'std_recovery_rate': np.std(recovery_rates),\
                'mean_recovery_time': np.mean(recovery_times),\
                'std_recovery_time': np.std(recovery_times)\
            \}\
\
        print(f"   Master Seed \{master_seed\}: \{len(seed_results)\} successful experiments")\
\
    # Calculate overall meta-statistics using unified metrics\
    all_overall_perfs = []\
    all_recovery_rates = []\
    all_recovery_times = []\
\
    for seed_result in meta_results.values():\
        # Unified metrics\
        all_overall_perfs.extend(seed_result['overall_performances'])\
        all_recovery_rates.extend(seed_result['recovery_rates'])\
        all_recovery_times.extend(seed_result['recovery_times'])\
\
\
    meta_statistics = \{\
        # Primary unified metrics\
        'overall_performance_mean': np.mean(all_overall_perfs) if all_overall_perfs else 0,\
        'overall_performance_std': np.std(all_overall_perfs) if all_overall_perfs else 0,\
        'recovery_rate_mean': np.mean(all_recovery_rates) if all_recovery_rates else 0,\
        'recovery_rate_std': np.std(all_recovery_rates) if all_recovery_rates else 0,\
        'recovery_time_mean': np.mean(all_recovery_times) if all_recovery_times else 0,\
        'recovery_time_std': np.std(all_recovery_times) if all_recovery_times else 0,\
\
        # Experiment details\
        'n_total_experiments': sum(r['n_experiments'] for r in meta_results.values()),\
        'n_master_seeds': len(meta_results)\
    \}\
\
    return \{\
        'individual_seeds': meta_results,\
        'meta_statistics': meta_statistics,\
        'agent_name': agent_name\
    \}\
\
def run_simple_emotion_analysis():\
    """Run simple emotion analysis for all environments"""\
\
    print(" SIMPLE EMOTION TRAJECTORY ANALYSIS")\
    print("=" * 50)\
    print(" Focus: 8 emotion plots showing trial-by-trial average")\
    print(" Agent: ECIA_Full only")\
    print(" Environments: EnvA, EnvB, EnvC")\
    print("=" * 50)\
\
    environments = \{\
        "EnvA": EnvironmentA,\
        "EnvB": EnvironmentB,\
        "EnvC": EnvironmentC\
    \}\
\
    save_path = "content/Results/simple_emotion_analysis"\
    os.makedirs(save_path, exist_ok=True)\
\
    for env_name, env_class in environments.items():\
        print(f"\\n Processing \{env_name\}...")\
\
        # Collect emotion data\
        emotion_data = collect_ecia_emotions_simple(\
            env_class, n_trials=200, n_runs=50, env_name=env_name\
        )\
\
        # Create plots\
        plot_emotion_trajectories_simple(emotion_data, env_name, save_path)\
\
        # Save raw data\
        np.save(f"\{save_path\}/emotion_data_\{env_name\}.npy", emotion_data)\
        print(f" Data saved: \{save_path\}/emotion_data_\{env_name\}.npy")\
\
    print(f"\\n Simple emotion analysis completed!")\
    print(f" Results saved in: \{save_path\}/")\
    print(f" Generated files:")\
    for env_name in environments.keys():\
        print(f"   - emotion_trajectories_\{env_name\}.png")\
        print(f"   - emotion_data_\{env_name\}.npy")\
\
def run_complete_cross_dataset_study():\
    """Run complete cross-dataset study for all agents with unified metrics"""\
\
    print("=" * 80)\
    print("CROSS-DATASET STUDY: EnvA,B,C \uc0\u8594  RandomShift (Unified Metrics)")\
    print("3 Master Seeds \'d7 300 Runs \'d7 600 Test Trials")\
    print("=" * 80)\
\
    # Agent configurations\
    optimized_params = \{\
        "n_actions": 5,\
        "epsilon": 0.03,\
        "eta": 0.55,\
        "xi": 0.001,\
        "memory_threshold": 0.015,\
        "memory_influence": 0.3,\
        "memory_similarity_threshold": 0.035,\
        "top_k": 3,\
        "alpha": 0.22,\
        "window_size": 30,\
        "memory_size": 15,\
        "emotion_decay": 0.96,\
        "min_eta": 0.095,\
    \}\
\
    agents = \{\
        "ECIA_Full": (ECIA, optimized_params),\
        "EpsilonGreedy": (EpsilonGreedyAgent, \{"n_actions": 5, "epsilon": 0.1\}),\
        "UCB": (UCBAgent, \{"n_actions": 5, "c": 0.5\}),\
        "TS": (ThompsonSamplingAgent, \{"n_actions": 5\})\}\
\
    complete_results = \{\}\
\
    for agent_name, (agent_class, agent_kwargs) in agents.items():\
        print(f"\\n Agent: \{agent_name\}")\
\
        result = run_cross_dataset_meta_analysis_for_agent(\
            agent_class, agent_kwargs, agent_name\
        )\
\
        complete_results[agent_name] = result\
\
        # Print summary with unified metrics\
        meta_stats = result['meta_statistics']\
        print(f"   Results Summary (Unified Metrics):")\
        print(f"    Overall Performance: \{meta_stats['overall_performance_mean']:.4f\} \'b1 \{meta_stats['overall_performance_std']:.4f\}")\
        print(f"    Recovery Rate: \{meta_stats['recovery_rate_mean']:.4f\} \'b1 \{meta_stats['recovery_rate_std']:.4f\}")\
        print(f"    Recovery Time: \{meta_stats['recovery_time_mean']:.2f\} \'b1 \{meta_stats['recovery_time_std']:.2f\} trials")\
        print(f"    Total Experiments: \{meta_stats['n_total_experiments']\}")\
\
    # Save results\
    os.makedirs("content/Results/cross_dataset_study", exist_ok=True)\
    with open("content/Results/cross_dataset_study/complete_cross_dataset_results_unified.pkl", "wb") as f:\
        pickle.dump(complete_results, f)\
\
    print(f"\\n Complete Cross-Dataset Study finished!")\
    print(f" Results saved in: content/Results/cross_dataset_study/")\
\
    return complete_results\
\
\
\
# =====================\
# 10. MAIN EXECUTION FUNCTION\
# =====================\
# MENU SYSTEM\
# =====================\
def run_selected_analysis(choice):\
    """Run selected analysis with enhanced CSV output only (no plots except emotions)"""\
\
    if choice == 1:\
        print("\\n\uc0\u55357 \u56589  META-ANALYSIS STUDY (CSV ENHANCED)")\
        print(" Statistical robustness evaluation on EnvA, EnvB, EnvC")\
        print("=" * 60)\
\
        try:\
            results = run_complete_meta_analysis()\
\
            if results:\
                print("\\n Creating Emotion Trajectory Plots Only...")\
                try:\
                    run_simple_emotion_analysis()\
                    print("    Emotion plots created")\
                except Exception as e:\
                    print(f"    Emotion analysis failed: \{e\}")\
\
                print("\\n Creating Enhanced CSV Data...")\
                try:\
                    save_csv_data(results)\
                    print("    Enhanced CSV data created")\
                except Exception as e:\
                    print(f"    Enhanced CSV creation failed: \{e\}")\
\
                print("\\n Performing Statistical Analysis...")\
                try:\
                    meta_comparisons = perform_meta_statistical_comparison(results)\
                    create_statistical_comparison_csv(results, meta_comparisons)\
                    perform_multiple_comparison_correction(results)\
                    print("    Statistical analysis completed")\
                except Exception as e:\
                    print(f"    Statistical analysis failed: \{e\}")\
\
                print("\\n Generating Report...")\
                generate_meta_analysis_report(results, meta_comparisons)\
\
            print(f"\\n META-ANALYSIS FINISHED!")\
\
        except Exception as e:\
            print(f" meta-analysis failed: \{e\}")\
            import traceback\
            traceback.print_exc()\
\
    elif choice == 2:\
        print("\\n CROSS-DATASET STUDY (CSV ENHANCED)")\
        print(" Adaptation evaluation: EnvA,B,C \uc0\u8594  RandomShift")\
        print("=" * 60)\
\
        try:\
            cross_dataset_results = run_complete_cross_dataset_study()\
\
            if cross_dataset_results:\
                print("\\n Creating Enhanced CSV Data...")\
                try:\
                    save_enhanced_cross_dataset_csv_data(cross_dataset_results)\
                    print("    Enhanced CSV data created")\
                except Exception as e:\
                    print(f"    Enhanced CSV creation failed: \{e\}")\
\
                print("\\n Performing Statistical Analysis...")\
                try:\
                    perform_multiple_comparison_correction_cross_dataset(cross_dataset_results)\
                    print("    Statistical analysis completed")\
                except Exception as e:\
                    print(f"    Statistical analysis failed: \{e\}")\
\
                print("\\n Generating Report...")\
                generate_cross_dataset_unified_report(cross_dataset_results)\
\
            print(f"\\n CROSS-DATASET STUDY FINISHED!")\
\
        except Exception as e:\
            print(f" Cross-dataset study failed: \{e\}")\
            import traceback\
            traceback.print_exc()\
\
    elif choice == 3:\
        print("\\n COMPLETE COMPREHENSIVE STUDY (CSV ENHANCED)")\
        print(" Running both studies with enhanced CSV output")\
        print("=" * 80)\
\
        # Run both studies\
        run_selected_analysis(1)  # EnvA-C\
        run_selected_analysis(2)  # Cross-dataset\
\
        print(f"\\n COMPLETE COMPREHENSIVE STUDY FINISHED!")\
\
# Updated main execution\
def main_analysis_menu():\
    """Main menu for CSV-enhanced analysis (no plots except emotions)"""\
\
    print(" COMPREHENSIVE ECIA EVALUATION SYSTEM (CSV ENHANCED)")\
    print("=" * 60)\
    print("Features:")\
    print("\'95 Enhanced CSV output with 95% CI, SEM, STD")\
    print("\'95 Statistical comparison tables")\
    print("\'95 Emotion trajectory plots only")\
    print("\'95 Comprehensive reports")\
    print("=" * 60)\
    print("Select analysis to run:")\
    print("1. Meta-Analysis (EnvA, B, C)")\
    print("2. Cross-Dataset Study (A,B,C \uc0\u8594  RandomShift)")\
    print("3. Both studies (Complete Evaluation)")\
    print("=" * 60)\
\
    while True:\
        try:\
            choice = int(input("Enter choice (1/2/3): ").strip())\
            if choice in [1, 2, 3]:\
                return choice\
            else:\
                print("Invalid choice. Please enter 1, 2, or 3.")\
        except ValueError:\
            print("Invalid input. Please enter a number.")\
\
def create_combined_analysis_report(results, transfer_results,\
                                  save_path="content/Results/combined_analysis"):\
    """Create combined analysis report"""\
\
    os.makedirs(save_path, exist_ok=True)\
\
    report_lines = []\
    report_lines.append("COMBINED META-ANALYSIS & CROSS-DATASET TRANSFER REPORT")\
    report_lines.append("=" * 80)\
    report_lines.append("")\
\
    report_lines.append("STUDY OVERVIEW")\
    report_lines.append("-" * 30)\
    report_lines.append("This report combines results from two comprehensive studies:")\
    report_lines.append("1. Meta-Analysis: Statistical robustness evaluation")\
    report_lines.append("2. Cross-Dataset Transfer: Generalization capability assessment")\
    report_lines.append("")\
\
    # ECIA Performance Summary\
    report_lines.append("ECIA PERFORMANCE SUMMARY")\
    report_lines.append("-" * 35)\
\
    # Extract ECIA_Full performance from both studies\
    report_lines.append("ECIA_Full Performance Across Studies:")\
\
    # results\
    if results:\
        report_lines.append("\\nMeta-Analysis Results:")\
        for env_name, env_results in results.items():\
            if 'ECIA_Full' in env_results:\
                meta_stats = env_results['ECIA_Full']['meta_statistics']\
                report_lines.append(f"  \'95 \{env_name\}: \{meta_stats['meta_mean']:.4f\} \'b1 \{meta_stats['meta_std']:.4f\}")\
\
    # Transfer results\
    if transfer_results:\
        report_lines.append("\\nCross-Dataset Transfer Results:")\
        for config_name, config_results in transfer_results.items():\
            if 'ECIA_Full' in config_results:\
                meta_stats = config_results['ECIA_Full']['meta_statistics']\
                report_lines.append(f"  \'95 \{config_name\}: \{meta_stats['overall_performance_mean']:.4f\} \'b1 \{meta_stats['overall_performance_std']:.4f\}")\
\
    report_lines.append("")\
\
    # Key Findings\
    report_lines.append("KEY FINDINGS")\
    report_lines.append("-" * 20)\
    report_lines.append("\'95 ECIA demonstrates superior performance in controlled environments")\
    report_lines.append("\'95 Cross-dataset adaptation capabilities vary by environment characteristics")\
    report_lines.append("\'95 Emotion trajectory analysis reveals adaptive decision-making patterns")\
    report_lines.append("\'95 Memory and emotion systems contribute significantly to robustness")\
\
    # Save combined report\
    with open(f"\{save_path\}/combined_analysis_report.txt", "w") as f:\
        f.write("\\n".join(report_lines))\
\
    print(f" Combined analysis report saved: \{save_path\}/combined_analysis_report.txt")\
\
\
def generate_meta_analysis_report(complete_meta_results, meta_comparisons,\
                                           save_path="content/Results/meta_analysis"):\
    """Generate comprehensive report of meta-analysis"""\
\
    report_lines = []\
    report_lines.append("BASED META-ANALYSIS REPORT")\
    report_lines.append("=" * 60)\
    report_lines.append(f"Master Seeds Used: \{MANAGER.SEEDS\}")\
    report_lines.append(f"Runs per Master Seed: \{MANAGER.N_RUNS_PER_SEED\}")\
    report_lines.append(f"Total Experiments per Agent: \{MANAGER.TOTAL_EXPERIMENTS\}")\
    report_lines.append("")\
\
    # Summary for each environment\
    for env_name, env_results in complete_meta_results.items():\
        report_lines.append(f"ENVIRONMENT: \{env_name\}")\
        report_lines.append("-" * 40)\
\
        # Sort agents by meta-mean performance\
        agent_performance = []\
        for agent_name, agent_result in env_results.items():\
            meta_stats = agent_result['meta_statistics']\
            if meta_stats['n_master_seeds'] > 0:\
                agent_performance.append((agent_name, meta_stats['meta_mean'], meta_stats))\
\
        agent_performance.sort(key=lambda x: x[1], reverse=True)\
\
        # Report rankings\
        for rank, (agent_name, meta_mean, meta_stats) in enumerate(agent_performance, 1):\
            agent_type = "BASELINE" if agent_name in ["EpsilonGreedy", "UCB"] else "ECIA"\
            report_lines.append(\
                f"  \{rank:2d\}. [\{agent_type:8s\}] \{agent_name:15s\} | "\
                f"Meta-Mean: \{meta_stats['meta_mean']:.4f\} \'b1 \{meta_stats['meta_std']:.4f\} | "\
                f"95% CI: [\{meta_stats['ci_lower']:.4f\}, \{meta_stats['ci_upper']:.4f\}]"\
            )\
\
        report_lines.append("")\
\
        # Statistical comparisons\
        if env_name in meta_comparisons:\
            significant_comparisons = [\
                (comp_name, comp_data) for comp_name, comp_data in meta_comparisons[env_name].items()\
                if comp_data['significant']\
            ]\
\
            if significant_comparisons:\
                report_lines.append("  Statistically Significant Differences:")\
                for comp_name, comp_data in significant_comparisons:\
                    agent1, agent2 = comp_name.split('_vs_')\
                    direction = ">" if comp_data['mean_diff'] > 0 else "<"\
                    report_lines.append(\
                        f"    \{agent1\} \{direction\} \{agent2\}: p=\{comp_data['p_value']:.4f\}, "\
                        f"d=\{comp_data['cohens_d']:.3f\} (\{comp_data['effect_size_interpretation']\})"\
                    )\
            else:\
                report_lines.append("  No statistically significant differences found.")\
\
        report_lines.append("")\
\
    # Methodology section\
    report_lines.append("METHODOLOGY")\
    report_lines.append("-" * 40)\
    report_lines.append("\'95 sequence master seeds ensure natural variation")\
    report_lines.append("\'95 Each master seed generates 800 deterministic experiment seeds")\
    report_lines.append("\'95 Meta-analysis aggregates results across 8 master seeds")\
    report_lines.append("\'95 Statistical testing with appropriate parametric/non-parametric tests")\
    report_lines.append("\'95 Effect sizes calculated using Cohen's d")\
    report_lines.append("\'95 95% confidence intervals using t-distribution")\
\
    # Save report\
    with open(f"\{save_path\}/meta_analysis_report.txt", "w") as f:\
        f.write("\\n".join(report_lines))\
\
    print(f" meta-analysis report saved: \{save_path\}/meta_analysis_report.txt")\
\
    return "\\n".join(report_lines)\
\
\
# =====================\
# 8. COMPREHENSIVE ANALYSIS FUNCTIONS (INTEGRATED)\
# =====================\
\
def get_environment_change_points(env_name, env_instance=None):\
    """Get change points for each environment"""\
    if env_name == "EnvA":\
        return [100]\
    elif env_name == "EnvB":\
        return [40, 80, 120, 160]\
    elif env_name == "EnvC":\
        if env_instance and hasattr(env_instance, 'change_points'):\
            return env_instance.change_points\
        else:\
            return []\
    else:\
        return []\
\
def get_environment_optimal_rewards(env_name, env_instance=None):\
    """Get optimal rewards for each segment"""\
    if env_name == "EnvA":\
        return [0.8, 0.9]\
    elif env_name == "EnvB":\
        return [0.95, 0.95, 0.95, 0.95, 0.95]\
    elif env_name == "EnvC":\
        if env_instance and hasattr(env_instance, 'optimal_rewards'):\
            return env_instance.optimal_rewards\
        else:\
            return []\
    else:\
        return []\
\
def calculate_noise_adjusted_parameters(env_name):\
    """Calculate parameters adjusted for noise level"""\
    noise_levels = \{\
        "EnvA": \{"sigma": 0.15, "noise_level": "medium"\},\
        "EnvB": \{"sigma": 0.05, "noise_level": "low"\},\
        "EnvC": \{"sigma": 0.15, "noise_level": "medium"\}\
    \}\
\
    noise_info = noise_levels.get(env_name, \{"noise_level": "medium"\})\
\
    if noise_info["noise_level"] == "low":\
        threshold_ratio = 0.90\
        stability_window = 3\
        min_stability_trials = 2\
    elif noise_info["noise_level"] == "medium":\
        threshold_ratio = 0.85\
        stability_window = 5\
        min_stability_trials = 3\
    else:\
        threshold_ratio = 0.80\
        stability_window = 7\
        min_stability_trials = 4\
\
    return \{\
        "threshold_ratio": threshold_ratio,\
        "stability_window": stability_window,\
        "min_stability_trials": min_stability_trials\
    \}\
\
\
\
\cf3 \cb2 if\cf0  \cf9 __name__\cf0  == \cf4 "__main__"\cf0 :\cb1 \
\cb2     choice = main_analysis_menu()\cb1 \
\cb2     run_selected_analysis(choice)\cb1 \
}