  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns
  import pandas as pd
  import os
  from scipy import stats
  import pickle
  from tqdm import tqdm
  import warnings
  from collections import deque
  import random
  warnings.filterwarnings('ignore')
  import pandas as pd

3
  # Create result directories
  os.makedirs("content/Results", exist_ok=True)
  os.makedirs("content/Results", exist_ok=True)

class MetaAnalysisManager:
    """Manages meta-analysis using Fibonacci sequence master seeds"""

    def __init__(self):
        # Fibonacci sequence (for meta-analysis)
        self.SEEDS = [34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765]
        self.N_RUNS_PER_SEED = 300
        self.RANDOMSHIFT_SEEDS = [34, 55, 89]
        self.RANDOMSHIFT_N_RUNS_PER_SEED = 300
        self.TOTAL_EXPERIMENTS = len(self.SEEDS) * self.N_RUNS_PER_SEED


        # Logging and tracking
        self.seed_logs = {}
        self.meta_results = {}

    def get_seeds_and_runs(self, env_name):
        """Get environment-specific seeds and runs"""
        return self.SEEDS, self.N_RUNS_PER_SEED

    def generate_experiment_seeds(self, master_seed, env_name=None):
        """Generate experiment seeds from a Fibonacci master seed"""
        np.random.seed(master_seed)
        experiment_seeds = []

        n_runs = self.N_RUNS_PER_SEED  # 300

        # Generate experiment seeds
        for i in range(n_runs):
            seed = np.random.randint(0, 1000000)  # Wide range to avoid collisions
            experiment_seeds.append(seed)

        self.seed_logs[master_seed] = {
            'first_10_seeds': experiment_seeds[:10],
            'total_seeds': len(experiment_seeds),
            'seed_range': [min(experiment_seeds), max(experiment_seeds)]
        }

        return experiment_seeds

    def save_configuration(self, filepath="content/Results/config.txt"):
        """Save complete meta-analysis configuration"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        with open(filepath, 'w') as f:
            f.write("Meta-Analysis Configuration\n")
            f.write("=" * 50 + "\n\n")

            # Regular environments
            f.write("REGULAR ENVIRONMENTS (EnvA, EnvB, EnvC):\n")
            f.write(f"Master Seeds: {self.SEEDS}\n")
            f.write(f"Runs per Master Seed: {self.N_RUNS_PER_SEED}\n")
            f.write(f"Total Experiments: {len(self.SEEDS) * self.N_RUNS_PER_SEED}\n\n")

            # RandomShift environment
            f.write("CROSS-DATASET ENVIRONMENT (RandomShift):\n")
            f.write(f"Master Seeds: {self.RANDOMSHIFT_SEEDS}\n")
            f.write(f"Runs per Master Seed: {self.RANDOMSHIFT_N_RUNS_PER_SEED}\n")
            f.write(f"Total Experiments: {len(self.RANDOMSHIFT_SEEDS) * self.RANDOMSHIFT_N_RUNS_PER_SEED}\n\n")

            f.write("Seed Generation Details:\n")
            f.write("-" * 30 + "\n")
            for master_seed, log in self.seed_logs.items():
                f.write(f"Master Seed {master_seed}:\n")
                f.write(f"  First 10 experiment seeds: {log['first_10_seeds']}\n")
                f.write(f"  Seed range: {log['seed_range']}\n")
                f.write(f"  Total generated: {log['total_seeds']}\n\n")


# Global manager instances
MANAGER = MetaAnalysisManager()

  class EnvironmentA:
      """Environment A with deterministic seed control"""

      def __init__(self, total_trials=200, sigma=0.15, random_state=None):
          self.total_trials = total_trials
          self.trial = 0
          self.n_actions = 5
          self.sigma = sigma
          self.context = None

          # Set up reproducible random state
          if random_state is not None:
              self.rng = np.random.RandomState(random_state)
          else:
              self.rng = np.random.RandomState()

      def reset(self):
          self.trial = 0
          self.update_context()
          return None

      def step(self, action):
          reward = self.get_reward(action)
          self.trial += 1
          self.update_context()
          return reward, self.context

      def get_reward(self, action):
          t = self.trial
          if t < 50:
              means = [0.8, 0.2, 0.2, 0.2, 0.2]
          elif t < 100:
              means = [0.8, 0.2, 0.3, 0.2, 0.2]
          elif t < 150:
              means = [0.2, 0.2, 0.3, 0.2, 0.9]
          else:
              means = [0.2, 0.4, 0.3, 0.2, 0.9]

          # Use controlled random state for noise
          return self.rng.normal(means[action], self.sigma)

      def update_context(self):
          t = self.trial
          norm_t = t / self.total_trials
          if t < 50:
              phase_id = 0
          elif t < 100:
              phase_id = 1
          elif t < 150:
              phase_id = 2
          else:
              phase_id = 3
          self.context = np.array([norm_t, phase_id])

  class EnvironmentB:
      """Environment B with deterministic seed control"""

      def __init__(self, total_trials=200, sigma=0.05, random_state=None):
          self.total_trials = total_trials
          self.trial = 0
          self.n_actions = 5
          self.sigma = sigma
          self.context = None

          if random_state is not None:
              self.rng = np.random.RandomState(random_state)
          else:
              self.rng = np.random.RandomState()


      def reset(self):
          self.trial = 0
          self.update_context()
          return None

      def step(self, action):
          reward = self.get_reward(action)
          self.trial += 1
          self.update_context()
          return reward, self.context

      def get_reward(self, action):
          t = self.trial
          base = [0.2] * 5
          phase = t // 40

          if phase % 2 == 0:
              base[0] = 0.95; base[4] = 0.01
          else:
              base[0] = 0.01; base[4] = 0.95

          for i in [1, 2, 3]:
              base[i] = 0.05

          reward = base[action] + self.rng.normal(0, self.sigma)
          return np.clip(reward, 0, 1)

      def update_context(self):
          t = self.trial
          norm_t = t / self.total_trials
          phase = t // 40
          phase_id = phase % 2
          self.context = np.array([norm_t, phase_id])


  class EnvironmentC:
      """Environment C with controlled randomness while maintaining stochastic nature"""
      def __init__(self, total_trials=200, sigma=0.15,
                  disturbance_prob=0.05, disturbance_strength=0.7,
                  random_state=None):
          self.total_trials = total_trials
          self.n_actions = 5
          self.sigma = sigma
          self.trial = 0
          self.disturbance_prob = disturbance_prob
          self.disturbance_strength = disturbance_strength

          # Set up controlled random state
          if random_state is not None:
              self.rng = np.random.RandomState(random_state)
          else:
              self.rng = np.random.RandomState()

          # Generate random but reproducible configuration
          self._generate_random_configuration()

          self.active_disturbance = False
          self.disturbance_action = None
          self.disturbance_duration = 0
          self.context = None
          self.update_reward_structure()

      def _generate_random_configuration(self):
          """Generate random configuration using controlled randomness"""
          # Generate 3 random change points between 30-180 (but deterministic)
          self.change_points = sorted(self.rng.choice(range(30, 181), size=3, replace=False).tolist())

          # Generate different optimal actions and rewards for each segment
          self.optimal_actions = []
          self.optimal_rewards = []
          used_actions = set()

          for i in range(4):  # 4 segments total
              # Ensure different optimal action for each segment
              if len(used_actions) < self.n_actions:
                  unused_actions = [a for a in range(self.n_actions) if a not in used_actions]
                  if unused_actions:
                      opt_action = self.rng.choice(unused_actions)
                  else:
                      prev_action = self.optimal_actions[-1] if self.optimal_actions else -1
                      opt_action = (prev_action + 1) % self.n_actions
              else:
                  prev_action = self.optimal_actions[-1] if self.optimal_actions else -1
                  available_actions = [a for a in range(self.n_actions) if a != prev_action]
                  opt_action = self.rng.choice(available_actions)

              used_actions.add(opt_action)
              self.optimal_actions.append(opt_action)

              # Generate different optimal reward levels
              opt_reward = self.rng.uniform(0.75, 0.9)
              self.optimal_rewards.append(opt_reward)


      def reset(self):
          self.trial = 0
          self.active_disturbance = False
          self.disturbance_action = None
          self.disturbance_duration = 0
          self.update_reward_structure()
          return None

      def step(self, action):
          reward = self.rng.normal(self.means[action], self.sigma)
          self.trial += 1
          self.update_reward_structure()
          return reward, self.context

      def update_reward_structure(self):
          t = self.trial
          norm_t = t / self.total_trials

          # Determine current segment
          phase = 0
          for i, cp in enumerate(self.change_points):
              if t >= cp:
                  phase = i + 1
              else:
                  break

          optimal_action = self.optimal_actions[phase]
          optimal_reward = self.optimal_rewards[phase]

          # Set basic reward structure
          means = [0.2] * self.n_actions
          means[optimal_action] = optimal_reward

          # Handle disturbances with controlled randomness
          if self.active_disturbance:
              self.disturbance_duration -= 1
              if self.disturbance_duration > 0 and self.disturbance_action != optimal_action:
                  means[self.disturbance_action] = self.disturbance_strength
              else:
                  self.active_disturbance = False
                  self.disturbance_action = None

          elif self.rng.random() < self.disturbance_prob:
              non_optimal_actions = [a for a in range(self.n_actions) if a != optimal_action]
              if non_optimal_actions:
                  self.disturbance_action = self.rng.choice(non_optimal_actions)
                  self.active_disturbance = True
                  self.disturbance_duration = self.rng.randint(1, 3)
                  means[self.disturbance_action] = self.disturbance_strength

          self.means = means
          self.context = np.array([norm_t, phase])



      def get_current_optimal_info(self):
          """Get current segment's optimal action and reward"""
          phase = 0
          for i, cp in enumerate(self.change_points):
              if self.trial >= cp:
                  phase = i + 1
              else:
                  break

          return {
              'phase': phase,
              'optimal_action': self.optimal_actions[phase],
              'optimal_reward': self.optimal_rewards[phase],
              'change_point': self.change_points[phase-1] if phase > 0 else 0
          }

# =====================
# 1. RandomShiftEnvironment
# =====================

class RandomShiftEnvironment:
    def __init__(self, total_trials=600, n_actions=5, random_state=None):
        self.total_trials = total_trials
        self.n_actions = n_actions
        self.trial = 0
        self.sigma = 0.12

        # Master seed for reproducibility
        if random_state is not None:
            self.master_rng = np.random.RandomState(random_state)
        else:
            self.master_rng = np.random.RandomState()

        # Enhanced features for ECIA
        self.reward_history = deque(maxlen=50)  # For pattern analysis
        self.action_sequence = deque(maxlen=10)  # For sequence rewards
        self.extreme_event_counter = 0

        # Pattern storage for reuse (ECIA advantage)
        self.stored_patterns = []  # Store patterns from phase 1

        # Multi-layered temporal patterns
        self.short_term_cycle = 0  # 10-15 trials
        self.medium_term_cycle = 0  # 50-80 trials
        self.long_term_phase = 0   # 150+ trials

        # Generate enhanced segment schedule
        self.segments = self._generate_enhanced_segments()
        self.current_segment_idx = 0
        self.segment_start_trial = 0

        # Current state
        self.current_rewards = [0.2] * self.n_actions
        self.context = None
        self.base_noise_level = 0.12

        # Initialize first segment
        self._initialize_current_segment()

    def _generate_enhanced_segments(self):
        """Generate segments with rich complexity patterns and pattern reuse"""
        segments = []
        current_trial = 0

        # Long-term phases (3 phases of ~200 trials each)
        phase_types = ['learning', 'challenging', 'mastery']
        phase_length = self.total_trials // 3

        for phase_idx, phase_type in enumerate(phase_types):
            phase_start = phase_idx * phase_length
            phase_end = min((phase_idx + 1) * phase_length, self.total_trials)

            # Generate segments within each phase
            phase_trials = phase_start
            while phase_trials < phase_end:
                # Segment length based on phase type
                if phase_type == 'learning':
                    segment_length = self.master_rng.randint(80, 120)
                elif phase_type == 'challenging':
                    segment_length = self.master_rng.randint(60, 100)
                else:  # mastery
                    segment_length = self.master_rng.randint(40, 80)

                segment_end = min(phase_trials + segment_length, phase_end)

                segment = {
                    'start': phase_trials,
                    'end': segment_end,
                    'length': segment_end - phase_trials,
                    'phase_type': phase_type,
                    'phase_idx': phase_idx,
                    'config': self._generate_segment_config(phase_type, phase_idx, segment_end - phase_trials)
                }

                segments.append(segment)
                phase_trials = segment_end

        return segments

    def _generate_segment_config(self, phase_type, phase_idx, segment_length):
        """Generate enhanced segment configuration with pattern reuse"""
        config = {
            'phase_type': phase_type,
            'reward_structure': 'layered',
            'is_reused_pattern': False
        }

        # Pattern reuse logic (only after phase 1)
        if phase_idx > 0 and self.stored_patterns and self.master_rng.random() < 0.25:
            # Reuse pattern from phase 1
            base_pattern = self.master_rng.choice(self.stored_patterns)

            # 60% identical, 40% similar variation
            if self.master_rng.random() < 0.6:
                # Completely identical pattern
                config.update(base_pattern)
                config['is_reused_pattern'] = True
                config['reuse_type'] = 'identical'
            else:
                # Similar pattern with variations
                config.update(self._create_similar_pattern(base_pattern))
                config['is_reused_pattern'] = True
                config['reuse_type'] = 'similar'
        else:
            # Generate new pattern
            config.update(self._generate_new_pattern_config(phase_type, segment_length))

            # Store pattern if it's phase 1 (learning phase)
            if phase_idx == 0:
                pattern_to_store = config.copy()
                self.stored_patterns.append(pattern_to_store)

        return config

    def _generate_new_pattern_config(self, phase_type, segment_length):
        """Generate new pattern configuration"""
        # Generate layered reward structure with uneven gaps
        actions = list(range(self.n_actions))
        self.master_rng.shuffle(actions)

        config = {}

        if phase_type == 'learning':
            # Clear hierarchy with uneven gaps for complex learning
            config.update({
                'excellent_actions': actions[:1],      # 0.80-0.90
                'good_actions': actions[1:2],          # 0.75-0.82 (small gap: 0.065)
                'medium_actions': actions[2:3],        # 0.45-0.55 (large gap: 0.285)
                'poor_actions': actions[3:4],          # 0.15-0.25 (large gap: 0.30)
                'bad_actions': actions[4:],            # 0.05-0.15 (small gap: 0.10)
                'noise_multiplier': 0.8,
                'extreme_event_prob': 0.02,
                'sequence_bonus_prob': 0.1,
                'emotional_shock_point': segment_length // 2,  
                'shock_magnitude': 0.4
            })
        elif phase_type == 'challenging':
            # Moderate hierarchy with context-dependent patterns
            config.update({
                'excellent_actions': actions[:1],      # 0.80-0.85
                'good_actions': actions[1:3],          # 0.75-0.80 (small gap)
                'medium_actions': actions[3:4],        # 0.40-0.50 (large gap)
                'poor_actions': actions[4:],           # 0.20-0.30 (large gap)
                'bad_actions': [],
                'noise_multiplier': 1.0,
                'extreme_event_prob': 0.05,
                'sequence_bonus_prob': 0.15,
                'context_dependent_bonus': 0.15, 
                'memory_window': 20  # Long-term memory dependency
            })
        else:  # mastery
            # Complex patterns requiring sophisticated learning
            config.update({
                'excellent_actions': actions[:2],      # 0.75-0.80
                'good_actions': actions[2:3],          # 0.70-0.75 (small gap)
                'medium_actions': actions[3:],         # 0.35-0.45 (large gap)
                'poor_actions': [],
                'bad_actions': [],
                'noise_multiplier': 1.2,
                'extreme_event_prob': 0.08,
                'sequence_bonus_prob': 0.2,
                'expectation_violation_prob': 0.06,  
                'long_term_memory_bonus': 0.1 
            })

        return config

    def _create_similar_pattern(self, base_pattern):
        """Create similar but not identical pattern"""
        similar_pattern = base_pattern.copy()

        # Introduce small variations
        variation_type = self.master_rng.choice(['timing', 'magnitude', 'order'])

        if variation_type == 'timing':
            # Change timing of key events by ±20%
            if 'emotional_shock_point' in similar_pattern:
                original_point = similar_pattern['emotional_shock_point']
                variation = int(original_point * 0.2)
                similar_pattern['emotional_shock_point'] = original_point + self.master_rng.randint(-variation, variation+1)

        elif variation_type == 'magnitude':
            # Slightly adjust reward magnitudes
            if 'shock_magnitude' in similar_pattern:
                similar_pattern['shock_magnitude'] *= self.master_rng.uniform(0.8, 1.2)
            if 'context_dependent_bonus' in similar_pattern:
                similar_pattern['context_dependent_bonus'] *= self.master_rng.uniform(0.8, 1.2)

        elif variation_type == 'order':
            # Swap positions of some action groups
            if 'good_actions' in similar_pattern and 'medium_actions' in similar_pattern:
                if len(similar_pattern['good_actions']) == 1 and len(similar_pattern['medium_actions']) == 1:
                    # Swap good and medium
                    temp = similar_pattern['good_actions'][0]
                    similar_pattern['good_actions'][0] = similar_pattern['medium_actions'][0]
                    similar_pattern['medium_actions'][0] = temp

        similar_pattern['reuse_type'] = 'similar'
        return similar_pattern

    def _initialize_current_segment(self):
        """Initialize current segment"""
        if self.current_segment_idx >= len(self.segments):
            return

        segment = self.segments[self.current_segment_idx]
        self.segment_start_trial = segment['start']
        self._update_rewards()

    def _update_rewards(self):
        """Update reward structure with enhanced features"""
        segment = self.segments[self.current_segment_idx]
        config = segment['config']
        trial_in_segment = self.trial - self.segment_start_trial

        # Base reward structure with uneven gaps
        rewards = [0.2] * self.n_actions

        # Apply layered reward structure
        for action in config.get('excellent_actions', []):
            rewards[action] = self.master_rng.uniform(0.80, 0.90)
        for action in config.get('good_actions', []):
            rewards[action] = self.master_rng.uniform(0.75, 0.82)  # Small gap
        for action in config.get('medium_actions', []):
            rewards[action] = self.master_rng.uniform(0.45, 0.55)  # Large gap from good
        for action in config.get('poor_actions', []):
            rewards[action] = self.master_rng.uniform(0.15, 0.25)  # Large gap from medium
        for action in config.get('bad_actions', []):
            rewards[action] = self.master_rng.uniform(0.05, 0.15)  # Small gap from poor

        # ECIA-specific features
        self._apply_ecia_features(rewards, config, trial_in_segment)

        # Apply sequence bonuses/penalties (for all agents)
        self._apply_sequence_effects(rewards, config)

        # Apply temporal variations
        self._apply_temporal_patterns(rewards, trial_in_segment)

        self.current_rewards = rewards

    def _apply_ecia_features(self, rewards, config, trial_in_segment):
        """Apply ECIA-specific learning opportunities"""

        # Emotional shock (sudden reversal)
        if 'emotional_shock_point' in config and trial_in_segment == config['emotional_shock_point']:
            excellent_actions = config.get('excellent_actions', [])
            poor_actions = config.get('poor_actions', [])

            if excellent_actions and poor_actions:
                # Dramatic reversal: best becomes worst, worst becomes best
                excellent_action = excellent_actions[0]
                poor_action = poor_actions[0]

                # Swap their rewards dramatically
                rewards[excellent_action] = 0.1  # Dramatic drop
                rewards[poor_action] = 0.85       # Dramatic rise

        # Context-dependent bonuses
        if 'context_dependent_bonus' in config and len(self.reward_history) >= 5:
            recent_avg = np.mean(list(self.reward_history)[-5:])
            context_bonus = config['context_dependent_bonus']

            # Bonus based on recent performance context
            if recent_avg > 0.6:  # Good performance context
                for action in config.get('excellent_actions', []):
                    rewards[action] += context_bonus
            elif recent_avg < 0.4:  # Poor performance context
                for action in config.get('medium_actions', []):
                    rewards[action] += context_bonus * 0.5

        # Long-term memory bonuses
        if 'long_term_memory_bonus' in config and len(self.reward_history) >= 30:
            distant_avg = np.mean(list(self.reward_history)[-30:-20])
            recent_avg = np.mean(list(self.reward_history)[-10:])

            # Reward for recognizing long-term patterns
            if abs(distant_avg - recent_avg) < 0.1:  # Similar patterns
                memory_bonus = config['long_term_memory_bonus']
                for i in range(self.n_actions):
                    rewards[i] += memory_bonus

        # Expectation violation (surprise learning)
        if 'expectation_violation_prob' in config:
            if self.master_rng.random() < config['expectation_violation_prob']:
                surprise_action = self.master_rng.randint(0, self.n_actions)
                if self.master_rng.random() < 0.7:
                    rewards[surprise_action] = min(0.95, rewards[surprise_action] + 0.3)
                else:
                    rewards[surprise_action] = max(0.05, rewards[surprise_action] - 0.3)

    def _apply_sequence_effects(self, rewards, config):
        """Apply sequence bonuses/penalties (fair for all agents)"""
        if len(self.action_sequence) >= 3:
            last_actions = list(self.action_sequence)[-3:]

            # Diversity bonus
            if len(set(last_actions)) == 3:  # All different
                if self.master_rng.random() < config.get('sequence_bonus_prob', 0.1):
                    for i in range(self.n_actions):
                        rewards[i] += 0.1

            # Habit penalty
            elif len(set(last_actions)) == 1:  # All same
                repeated_action = last_actions[0]
                rewards[repeated_action] *= 0.85  # 15% penalty

    def _apply_temporal_patterns(self, rewards, trial_in_segment):
        """Apply multi-layered temporal patterns"""

        # Short-term cycle (10-15 trials)
        short_cycle_pos = (trial_in_segment % 12) / 12.0
        short_modifier = 0.05 * np.sin(2 * np.pi * short_cycle_pos)

        # Medium-term cycle (50-80 trials)
        medium_cycle_pos = (trial_in_segment % 65) / 65.0
        medium_modifier = 0.03 * np.cos(2 * np.pi * medium_cycle_pos)

        # Apply modifiers
        for i in range(self.n_actions):
            rewards[i] += short_modifier + medium_modifier
            rewards[i] = max(0.05, min(0.95, rewards[i]))  # Clamp

    def _update_context(self):
        """Update context (keeping it fair - same as original)"""
        norm_t = self.trial / self.total_trials
        phase_id = self.current_segment_idx % 4  # Simple phase cycling
        self.context = np.array([norm_t, phase_id])

    def step(self, action):
        """Execute one step with enhanced features"""

        # Check if we need to move to next segment
        current_segment = self.segments[self.current_segment_idx]
        if self.trial >= current_segment['end'] and self.current_segment_idx < len(self.segments) - 1:
            self.current_segment_idx += 1
            self._initialize_current_segment()

        # Update rewards for current trial
        self._update_rewards()

        # Get base reward
        base_reward = self.current_rewards[action]

        # Apply noise (enhanced based on phase)
        segment = self.segments[self.current_segment_idx]
        noise_multiplier = segment['config'].get('noise_multiplier', 1.0)
        noise = self.master_rng.normal(0, self.sigma * noise_multiplier)
        final_reward = np.clip(base_reward + noise, 0, 1)

        # Update tracking
        self.reward_history.append(final_reward)
        self.action_sequence.append(action)

        # Update trial and context
        self.trial += 1
        self._update_context()

        return final_reward, self.context

    def reset(self):
        """Reset environment to initial state"""
        self.trial = 0
        self.current_segment_idx = 0
        self.segment_start_trial = 0
        self.reward_history.clear()
        self.action_sequence.clear()
        self.extreme_event_counter = 0

        self._initialize_current_segment()
        self._update_context()
        return None

    def get_change_points(self):
        """Get actual change points for analysis"""
        return [segment['start'] for segment in self.segments[1:]]

    def get_pattern_reuse_info(self):
        """Get information about pattern reuse for analysis"""
        reuse_info = []
        for segment in self.segments:
            if segment['config'].get('is_reused_pattern', False):
                reuse_info.append({
                    'start': segment['start'],
                    'end': segment['end'],
                    'reuse_type': segment['config'].get('reuse_type', 'unknown')
                })
        return reuse_info

import numpy as np
from collections import deque

class ECIA:

    def __init__(self, n_actions=5, epsilon=0.03, eta=0.55, xi=0.001,
                 memory_threshold=0.015, memory_influence=0.3,
                 window_size=30, min_eta=0.095, memory_size=15,
                 alpha=0.22, memory_similarity_threshold=0.035,
                 top_k=3, emotion_decay=0.96, random_state=None):

        if random_state is not None:
            self.rng = np.random.RandomState(random_state)
        else:
            self.rng = np.random.RandomState()

        # Basic parameters
        self.n_actions = n_actions
        self.epsilon = epsilon
        self.xi = xi
        self.alpha = alpha
        self.emotion_decay = emotion_decay
        self.window_size = window_size
        self.min_eta = min_eta
        self.top_k = top_k

        # Dynamic eta adjustment
        self.base_eta = eta
        self.eta = self.base_eta
        self.eta_adaptation_counter = 0

        # Memory system
        self.base_memory_threshold = memory_threshold
        self.base_memory_influence = memory_influence
        self.memory_size = memory_size
        self.memory_similarity_threshold = memory_similarity_threshold
        self.memory_activation_level = 1.0
        self.memory_quality_threshold = 0.15
        self.memory_usage_history = deque(maxlen=20)
        self.memory_cooldown = 0

        # Environment detection
        self.memory_effectiveness_tracker = deque(maxlen=50)
        self.environment_stability_tracker = deque(maxlen=30)
        self.change_detection_window = deque(maxlen=15)
        self.stable_performance_counter = 0

        # Context clustering
        self.context_clusters = {}
        self.cluster_performance = {}
        self.use_context_clustering = True

        # Basic initialization
        self.q_values = np.zeros(n_actions)
        self.emotion = np.zeros(8)
        self.action_counts = np.zeros(n_actions)
        self.time = 0
        self.prev_reward = 0.5
        self.context = None

        # Emotion system
        self.emotion_names = [
            "fear", "joy", "hope", "sadness",
            "curiosity", "anger", "pride", "shame"
        ]
        self.emotion_weight = np.array([
            -0.15, 0.4, 0.3, -0.2, 0.35, -0.25, 0.25, -0.3
        ])
        self.max_total_emotion_energy = 2.5
        self.emotion_momentum = np.zeros(8)

        # Memory storage
        self.episodic_memory = []

        # Other variables
        self.performance_tracker = deque(maxlen=25)
        self.recent_context_changes = deque(maxlen=10)
        self.action_history = deque(maxlen=20)
        self.reward_history = deque(maxlen=20)
        self.learning_boost = 0.2
        self.successful_emotion_patterns = {}
        self.neurogenesis_cycle = 25
        self.emotion_learning_rates = np.array([
            0.15, 0.25, 0.20, 0.12, 0.30, 0.18, 0.22, 0.28
        ])
        self.emotion_action_history = deque(maxlen=12)
        self.context_change_threshold = 0.1
        self.habit_strength_factor = 0.025

    def assess_environment_stability(self):
        """Comprehensive environment stability assessment"""
        if len(self.recent_context_changes) < 5:
            return 0.5

        # Context change stability
        context_changes = list(self.recent_context_changes)
        avg_change = np.mean(context_changes)
        change_variance = np.var(context_changes)
        context_stability = 1.0 - min(avg_change + change_variance * 0.5, 1.0)

        # Reward stability
        if len(self.reward_history) >= 10:
            recent_rewards = list(self.reward_history)[-10:]
            reward_stability = 1.0 - min(np.std(recent_rewards), 1.0)
        else:
            reward_stability = 0.5

        # Performance trend stability
        if len(self.performance_tracker) >= 8:
            recent_performance = list(self.performance_tracker)[-8:]
            performance_trend = abs(np.polyfit(range(8), recent_performance, 1)[0])
            trend_stability = 1.0 - min(performance_trend * 5, 1.0)
        else:
            trend_stability = 0.5

        # Overall stability
        stability = 0.4 * context_stability + 0.4 * reward_stability + 0.2 * trend_stability
        self.environment_stability_tracker.append(stability)
        return np.mean(self.environment_stability_tracker) if self.environment_stability_tracker else 0.5

    def evaluate_memory_effectiveness(self):
        """Memory system effectiveness evaluation"""
        if len(self.memory_usage_history) < 10:
            return 0.5

        memory_used_rewards = [r for used, r in self.memory_usage_history if used]
        memory_unused_rewards = [r for used, r in self.memory_usage_history if not used]

        if len(memory_used_rewards) > 3 and len(memory_unused_rewards) > 3:
            used_avg = np.mean(memory_used_rewards)
            unused_avg = np.mean(memory_unused_rewards)
            effectiveness = (used_avg - unused_avg + 1.0) / 2.0
        else:
            effectiveness = 0.5

        self.memory_effectiveness_tracker.append(effectiveness)
        return np.mean(self.memory_effectiveness_tracker) if self.memory_effectiveness_tracker else 0.5

    def detect_environment_change(self):
        """Environment change detection"""
        if len(self.reward_history) < 10:
            return False

        recent_rewards = list(self.reward_history)[-10:]
        older_rewards = list(self.reward_history)[-20:-10] if len(self.reward_history) >= 20 else recent_rewards

        recent_mean = np.mean(recent_rewards)
        older_mean = np.mean(older_rewards)
        performance_drop = older_mean - recent_mean > 0.2
        performance_volatility = np.std(recent_rewards) > 0.25

        return performance_drop or performance_volatility

    def calculate_memory_need(self):
        """Memory need score calculation (0~1)"""
        if len(self.reward_history) < 10:
            return 0.5

        # Performance volatility
        recent_rewards = list(self.reward_history)[-10:]
        reward_volatility = np.std(recent_rewards)
        volatility_score = min(reward_volatility * 2, 1.0)

        # Exploration vs exploitation imbalance
        if len(self.action_history) >= 10:
            action_diversity = len(set(list(self.action_history)[-10:])) / self.n_actions
            exploration_score = abs(action_diversity - 0.6) * 2
        else:
            exploration_score = 0.5

        # Q-value variance
        q_variance = np.var(self.q_values) if np.sum(self.q_values) != 0 else 0.5
        uncertainty_score = min(q_variance * 5, 1.0)

        # Emotion intensity
        emotion_intensity = np.linalg.norm(self.emotion) / np.sqrt(8)
        need_score = (0.3 * volatility_score + 0.25 * exploration_score +
                     0.25 * uncertainty_score + 0.2 * emotion_intensity)

        return np.clip(need_score, 0.0, 1.0)

    def analyze_performance_trend(self):
        """Recent performance trend analysis"""
        if len(self.performance_tracker) < 10:
            return 0.0

        recent_performance = list(self.performance_tracker)[-10:]
        x = np.arange(len(recent_performance))

        if len(recent_performance) >= 3:
            trend = np.polyfit(x, recent_performance, 1)[0]
            return trend
        else:
            return 0.0

    def adaptive_memory_control(self):
        """Simple and clean adaptive memory control - pure experience-based without environment classification"""
        effectiveness = self.evaluate_memory_effectiveness()

        # Core: Judge only based on memory effectiveness
        if effectiveness > 0.6:
            # Memory is helpful → use more
            self.memory_activation_level = 0.8 + 0.2 * effectiveness
        elif effectiveness > 0.4:
            # Memory is somewhat helpful → moderate use
            self.memory_activation_level = 0.4 + 0.4 * effectiveness
        else:
            # Memory is not very helpful → minimize
            self.memory_activation_level = 0.1 + 0.3 * effectiveness

        self.memory_activation_level = np.clip(self.memory_activation_level, 0.05, 1.0)

        # Dynamic parameter adjustment
        self.current_memory_threshold = self.base_memory_threshold / max(0.1, self.memory_activation_level)
        self.current_memory_influence = self.base_memory_influence * self.memory_activation_level

    def identify_context_cluster(self, context):
        """Context cluster identification"""
        if context is None or len(context) < 2:
            return "default"

        norm_t = context[0] if len(context) > 0 else 0
        phase_id = int(context[1]) if len(context) > 1 else 0
        time_cluster = int(norm_t * 4)

        return f"phase_{phase_id}_time_{time_cluster}"

    def compute_memory_quality_score(self, action, reward, prediction_error):
        """Memory quality score calculation"""
        error_score = min(abs(prediction_error), 0.5) / 0.5
        extreme_reward_score = abs(reward - 0.5) / 0.5
        emotion_intensity = np.linalg.norm(self.emotion) / np.sqrt(8)

        return 0.4 * error_score + 0.4 * extreme_reward_score + 0.2 * emotion_intensity

    def store_adaptive_memory(self, action, reward, prediction_error):
        """Universal adaptive memory storage"""
        if self.memory_activation_level < 0.1 or self.context is None:
            return

        quality_score = self.compute_memory_quality_score(action, reward, prediction_error)

        if quality_score < self.memory_quality_threshold:
            return

        if abs(prediction_error) > self.current_memory_threshold:
            memory = {
                'action': action,
                'reward': reward,
                'context': self.context.copy(),
                'time': self.time,
                'prediction_error': abs(prediction_error),
                'quality_score': quality_score,
                'emotion_state': self.emotion.copy()
            }

            # Universal memory storage
            if self.use_context_clustering and self.memory_activation_level > 0.6:
                cluster_id = self.identify_context_cluster(self.context)
                memory['cluster_id'] = cluster_id

                if cluster_id not in self.context_clusters:
                    self.context_clusters[cluster_id] = []
                    self.cluster_performance[cluster_id] = deque(maxlen=20)

                self.context_clusters[cluster_id].append(memory)
                self.cluster_performance[cluster_id].append(reward)

                if len(self.context_clusters[cluster_id]) > self.memory_size // 6:
                    self.context_clusters[cluster_id].sort(
                        key=lambda x: x['quality_score'], reverse=True
                    )
                    self.context_clusters[cluster_id] = self.context_clusters[cluster_id][:self.memory_size // 6]

            # Store in general memory as well
            self.episodic_memory.append(memory)
            if len(self.episodic_memory) > self.memory_size:
                self.episodic_memory.sort(key=lambda x: (
                    0.6 * x['quality_score'] + 0.4 * (x['time'] / max(1, self.time))
                ), reverse=True)
                self.episodic_memory = self.episodic_memory[:self.memory_size]

    def compute_adaptive_memory_bias(self):
        """Universal adaptive memory bias calculation"""
        if self.context is None or self.memory_activation_level < 0.1:
            self.memory_usage_history.append((False, self.prev_reward))
            return np.zeros(self.n_actions)

        relevant_memories = []

        # Universal memory retrieval strategy
        if self.use_context_clustering and self.memory_activation_level > 0.5 and self.context_clusters:
            cluster_id = self.identify_context_cluster(self.context)
            cluster_memories = self.context_clusters.get(cluster_id, [])

            if cluster_memories:
                relevant_memories.extend(cluster_memories[-3:])

            if len(relevant_memories) < 2:
                for cid, memories in self.context_clusters.items():
                    if len(memories) > 0 and cid in self.cluster_performance:
                        cluster_perf = np.mean(self.cluster_performance[cid])
                        if cluster_perf > 0.6:
                            relevant_memories.extend(memories[-2:])
                            if len(relevant_memories) >= 4:
                                break

        # Similarity-based search from general memory
        if len(relevant_memories) < 3:
            similarity_memories = []
            for memory in self.episodic_memory[-30:]:
                if memory.get('context') is not None:
                    context_sim = self.compute_similarity(self.context, memory['context'])
                    if context_sim > self.memory_similarity_threshold:
                        emotion_sim = self.compute_similarity(self.emotion, memory['emotion_state'])
                        combined_sim = 0.7 * context_sim + 0.3 * emotion_sim
                        similarity_memories.append((combined_sim, memory))

            if similarity_memories:
                similarity_memories.sort(key=lambda x: x[0], reverse=True)
                additional_memories = [mem for _, mem in similarity_memories[:max(1, 4-len(relevant_memories))]]
                relevant_memories.extend(additional_memories)

        # Use high-quality memories if insufficient
        if len(relevant_memories) < 2:
            high_quality_memories = [m for m in self.episodic_memory if m['quality_score'] > 0.7]
            if high_quality_memories:
                relevant_memories.extend(high_quality_memories[-2:])

        if not relevant_memories:
            self.memory_usage_history.append((False, self.prev_reward))
            return np.zeros(self.n_actions)

        # Memory bias calculation
        bias = np.zeros(self.n_actions)
        total_weight = 0

        for memory in relevant_memories[-5:]:
            action = memory['action']
            reward = memory['reward']
            quality = memory['quality_score']
            weight = quality * self.current_memory_influence

            if reward > 0.6:
                bias[action] += reward * weight
            elif reward < 0.4:
                bias[action] -= (0.5 - reward) * weight * 0.5

            total_weight += weight

        if total_weight > 0:
            bias = bias / (1.0 + total_weight * 0.2)

        self.memory_usage_history.append((True, self.prev_reward))
        return bias

    def emotional_processing(self, reward):
        """Improved emotional processing"""
        if self.prev_reward is None:
            self.prev_reward = 0.5

        self.emotion = self.emotion_decay * self.emotion
        self.reward_history.append(reward)
        recent_rewards = list(self.reward_history)

        current_emotion_updates = np.zeros(8)
        intensity_factor = 0.7

        # Fear
        if reward < self.prev_reward - 0.15:
            fear_strength = min(0.6, 0.15 + 0.4 * abs(reward - self.prev_reward))
            current_emotion_updates[0] = fear_strength * intensity_factor

        # Joy
        if reward > 0.7:
            joy_strength = min(0.7, 0.2 + 0.5 * reward)
            current_emotion_updates[1] = joy_strength * intensity_factor

        # Hope
        if len(recent_rewards) >= 4:
            recent_trend = np.polyfit(range(4), recent_rewards[-4:], 1)[0]
            if recent_trend > 0.03:
                hope_strength = min(0.6, 0.15 + 0.6 * recent_trend * 10)
                current_emotion_updates[2] = hope_strength * intensity_factor

        # Sadness
        if len(recent_rewards) >= 6:
            avg_recent = np.mean(recent_rewards[-6:])
            if avg_recent < 0.4:
                sadness_strength = min(0.5, 0.1 + 0.4 * (0.4 - avg_recent) / 0.4)
                current_emotion_updates[3] = sadness_strength * intensity_factor

        # Curiosity
        if len(self.action_history) > 0:
            action_diversity = len(set(self.action_history)) / min(len(self.action_history), self.n_actions)
            recent_performance = np.mean(recent_rewards[-3:]) if len(recent_rewards) >= 3 else 0.5

            if recent_performance < 0.6 or action_diversity < 0.8:
                curiosity_strength = min(0.7, 0.2 + 0.3 * (1 - action_diversity) +
                                       0.2 * max(0, 0.6 - recent_performance))
                current_emotion_updates[4] = curiosity_strength * intensity_factor

        # Anger
        expected_improvement = 0.05 * self.time / 200
        expected_reward = 0.5 + expected_improvement
        if reward < expected_reward - 0.2:
            anger_strength = min(0.5, 0.1 + 0.4 * abs(reward - expected_reward))
            current_emotion_updates[5] = anger_strength * intensity_factor

        # Pride
        if len(recent_rewards) >= 5:
            success_rate = sum(r > 0.7 for r in recent_rewards[-5:]) / 5
            if success_rate > 0.6:
                pride_strength = min(0.6, 0.1 + 0.4 * success_rate)
                current_emotion_updates[6] = pride_strength * intensity_factor

        # Shame
        if len(recent_rewards) >= 4:
            failure_rate = sum(r < 0.3 for r in recent_rewards[-4:]) / 4
            if failure_rate > 0.5:
                shame_strength = min(0.4, 0.1 + 0.3 * failure_rate)
                current_emotion_updates[7] = shame_strength * intensity_factor

        self.resolve_emotion_conflicts(current_emotion_updates)

        for i in range(8):
            self.emotion_momentum[i] = 0.7 * self.emotion_momentum[i] + 0.3 * current_emotion_updates[i]
            emotion_change = 0.5 * current_emotion_updates[i] + 0.5 * self.emotion_momentum[i]
            self.emotion[i] = 0.7 * self.emotion[i] + 0.3 * emotion_change

        self.normalize_emotions()
        self.prev_reward = reward

    def resolve_emotion_conflicts(self, emotion_updates):
        """Emotion conflict resolution"""
        conflicting_pairs = [(0, 1), (2, 3), (6, 7)]

        for idx1, idx2 in conflicting_pairs:
            if emotion_updates[idx1] > 0.5 and emotion_updates[idx2] > 0.5:
                avg_strength = (emotion_updates[idx1] + emotion_updates[idx2]) / 2
                emotion_updates[idx1] = avg_strength * 0.8
                emotion_updates[idx2] = avg_strength * 0.8

    def normalize_emotions(self):
        """Emotion normalization"""
        self.emotion = np.clip(self.emotion, 0.0, 1.0)
        total_emotion_energy = np.sum(self.emotion)

        if total_emotion_energy > self.max_total_emotion_energy:
            self.emotion = self.emotion * (self.max_total_emotion_energy / total_emotion_energy)

    def adaptive_eta_adjustment(self):
        """Dynamic eta adjustment"""
        self.eta_adaptation_counter += 1

        if self.eta_adaptation_counter >= 20 and len(self.performance_tracker) >= 10:
            recent_performance = np.mean(list(self.performance_tracker)[-10:])
            performance_variance = np.var(list(self.performance_tracker)[-10:])

            if recent_performance > 0.75:
                self.eta = self.base_eta * 0.6
            elif recent_performance > 0.6:
                self.eta = self.base_eta * 0.8
            elif recent_performance < 0.4:
                self.eta = self.base_eta * 1.3
            elif performance_variance > 0.05:
                self.eta = self.base_eta * 1.1
            else:
                self.eta = self.base_eta

            self.eta = np.clip(self.eta, self.base_eta * 0.3, self.base_eta * 1.5)
            self.eta_adaptation_counter = 0

    def select_top_emotions(self):
        """Core emotion selection"""
        emotion_indices = np.argsort(self.emotion)[-self.top_k:]
        selective_emotions = np.zeros(8)
        selective_emotions[emotion_indices] = self.emotion[emotion_indices]
        return selective_emotions

    def compute_direct_emotion_influence(self):
        """Direct emotion-action mapping"""
        influences = np.zeros(self.n_actions)
        selective_emotions = self.select_top_emotions()

        # Fear
        if selective_emotions[0] > 0.1:
            fear_level = selective_emotions[0]
            min_q = np.min(self.q_values)
            max_q = np.max(self.q_values)
            q_range = max_q - min_q + 0.001

            for action in range(self.n_actions):
                relative_badness = (max_q - self.q_values[action]) / q_range
                influences[action] -= fear_level * 0.4 * relative_badness

        # Joy
        if selective_emotions[1] > 0.1:
            joy_level = selective_emotions[1]
            min_q = np.min(self.q_values)
            max_q = np.max(self.q_values)
            q_range = max_q - min_q + 0.001

            for action in range(self.n_actions):
                relative_goodness = (self.q_values[action] - min_q) / q_range
                influences[action] += joy_level * 0.4 * relative_goodness

        # Curiosity
        if selective_emotions[4] > 0.1:
            curiosity_level = selective_emotions[4]
            min_count = np.min(self.action_counts)
            max_count = np.max(self.action_counts) + 1

            for action in range(self.n_actions):
                exploration_factor = (max_count - self.action_counts[action]) / max_count
                influences[action] += curiosity_level * 0.4 * exploration_factor

        return influences

    def hippocampal_neurogenesis(self):
        """Hippocampal neurogenesis simulation"""
        if self.time % self.neurogenesis_cycle == 0:
            emotion_intensity = np.linalg.norm(self.emotion)
            base_boost = 0.2

            if emotion_intensity > 0.7:
                self.learning_boost = base_boost * 1.3
            elif self.emotion[4] > 0.6:
                self.learning_boost = base_boost * 1.2
            else:
                self.learning_boost = base_boost
        else:
            self.learning_boost = max(0, self.learning_boost - 0.01)

    def prefrontal_modulation(self):
        """Prefrontal cortex modulation"""
        if len(self.episodic_memory) < 5:
            return 0.5

        recent_rewards = [mem['reward'] for mem in self.episodic_memory[-10:]]
        reward_stability = 1.0 - np.std(recent_rewards) if recent_rewards else 0.5

        context_change = np.mean(self.recent_context_changes) if self.recent_context_changes else 0
        emotion_volatility = np.std(self.emotion) if np.sum(self.emotion) > 0 else 0
        emotion_stability = 1.0 - min(emotion_volatility, 1.0)

        performance_trend = 0.5
        if len(self.performance_tracker) >= 5:
            recent_performance = list(self.performance_tracker)[-5:]
            performance_trend = np.mean(recent_performance)

        stability = (0.35 * reward_stability + 0.25 * emotion_stability -
                    0.15 * context_change + 0.25 * performance_trend)

        return np.clip(stability, 0.1, 0.9)

    def compute_uncertainty_bonus(self):
        """Uncertainty-based exploration bonus"""
        uncertainty = np.zeros(self.n_actions)
        total_experiences = self.time + 1

        for a in range(self.n_actions):
            action_count = self.action_counts[a] + 1
            count_uncertainty = np.sqrt(np.log(total_experiences) / action_count)

            # Collect reward history for this action
            action_rewards = [mem['reward'] for mem in self.episodic_memory if mem['action'] == a]

            # Calculate reward standard deviation
            if len(action_rewards) > 1:
                reward_std = np.std(action_rewards)
            else:
                reward_std = 0.5  # Default value

            # Emotion factor
            emotion_factor = 1.0
            if self.emotion[4] > 0.6:  # Curiosity
                emotion_factor = 1.3
            elif self.emotion[0] > 0.6:  # Fear
                emotion_factor = 0.7

            uncertainty[a] = count_uncertainty * (1 + reward_std) * emotion_factor

        return uncertainty * 0.3

    def build_context(self, norm_t, phase_id):
        """Enhanced context construction"""
        grid_patterns = []

        for scale in [1.0, 3.0, 6.0]:
            for offset in [0.0, 0.33, 0.67]:
                grid_patterns.append(np.sin(2*np.pi * (norm_t * scale + offset)))
                grid_patterns.append(np.cos(2*np.pi * (norm_t * scale + offset)))

        time_cells = [
            np.exp(-(norm_t - 0.25)**2 / 0.15),
            np.exp(-(norm_t - 0.5)**2 / 0.15),
            np.exp(-(norm_t - 0.75)**2 / 0.15)
        ]

        emotion_context = self.emotion * 1.5

        return np.concatenate([grid_patterns, time_cells, [phase_id], emotion_context])

    def update_context(self, norm_t, phase_id):
        """Context update and change tracking"""
        self.prev_context = self.context.copy() if self.context is not None else None
        self.context = self.build_context(norm_t, phase_id)

        if self.prev_context is not None:
            min_len = min(len(self.context), len(self.prev_context))
            context_change = np.linalg.norm(self.context[:min_len] - self.prev_context[:min_len])
            self.recent_context_changes.append(context_change)

    def select_action(self):
        """Integrated action selection"""
        self.hippocampal_neurogenesis()
        self.adaptive_eta_adjustment()
        self.adaptive_memory_control()

        pfc_stability = self.prefrontal_modulation()

        # Emotion-based exploration adjustment
        curiosity_boost = self.emotion[4] * 0.2
        fear_penalty = self.emotion[0] * 0.25
        joy_exploitation = self.emotion[1] * 0.15

        adaptive_epsilon = self.epsilon * (1 - pfc_stability)
        adaptive_epsilon = np.clip(
            adaptive_epsilon + curiosity_boost - fear_penalty - joy_exploitation,
            0.01, 0.2
        )

        # Exploration action
        if self.rng.rand() < adaptive_epsilon:
            if self.emotion[4] > 0.6:
                action_probs = 1.0 / (self.action_counts + 0.1)
                action_probs = action_probs / np.sum(action_probs)
                return self.rng.choice(self.n_actions, p=action_probs)
            elif self.emotion[5] > 0.6:
                if len(self.action_history) > 0:
                    recent_action = self.action_history[-1]
                    available_actions = [a for a in range(self.n_actions) if a != recent_action]
                    if available_actions:
                        return self.rng.choice(available_actions)
            return self.rng.choice(self.n_actions)

        # Exploitation action
        emotion_influences = self.compute_direct_emotion_influence()
        memory_bias = self.compute_adaptive_memory_bias()
        uncertainty_bonus = self.compute_uncertainty_bonus()

        final_values = (self.q_values +
                       self.eta * emotion_influences +
                       memory_bias +
                       uncertainty_bonus +
                       self.xi * self.rng.randn(self.n_actions))

        return np.argmax(final_values)

    def update_dopamine_learning(self, action, reward):
        """Dopamine-based learning"""
        prediction_error = reward - self.q_values[action]
        emotion_intensity = np.linalg.norm(self.emotion)

        base_alpha = self.alpha
        emotional_boost = 1.0 + emotion_intensity * 0.3

        if abs(prediction_error) > 0.3:
            adaptive_alpha = base_alpha * 1.3 * emotional_boost
        elif abs(prediction_error) > 0.15:
            adaptive_alpha = base_alpha * 1.1 * emotional_boost
        else:
            adaptive_alpha = base_alpha * emotional_boost

        adaptive_alpha += self.learning_boost

        if self.emotion[4] > 0.6:
            adaptive_alpha *= 1.2

        if self.emotion[0] > 0.7:
            adaptive_alpha *= 0.85

        self.q_values[action] += adaptive_alpha * prediction_error

        habit_strength = min(0.3, self.habit_strength_factor * self.action_counts[action])
        if reward > 0.65:
            self.q_values[action] += habit_strength * reward

        self.performance_tracker.append(reward)
        return prediction_error

    def update(self, *args, **kwargs):
        """Integrated update"""
        if len(args) == 2:
            action, reward = args
            context = None
        elif len(args) == 3:
            context, action, reward = args
            self.context = context
        else:
            action, reward = args[0], args[1]
            context = None

        self.hippocampal_neurogenesis()
        prediction_error = self.update_dopamine_learning(action, reward)
        self.emotional_processing(reward)
        self.store_adaptive_memory(action, reward, prediction_error)

        self.action_history.append(action)
        self.action_counts[action] += 1
        self.time += 1

    def compute_similarity(self, c1, c2):
        """Context similarity calculation"""
        if not isinstance(c1, np.ndarray) or not isinstance(c2, np.ndarray):
            return 0.0

        min_len = min(len(c1), len(c2))
        if min_len == 0:
            return 0.0

        c1_part, c2_part = c1[:min_len], c2[:min_len]

        if np.linalg.norm(c1_part) == 0 or np.linalg.norm(c2_part) == 0:
            return 0.0

        return np.dot(c1_part, c2_part) / (np.linalg.norm(c1_part) * np.linalg.norm(c2_part))

    def reset(self):
        """Complete reset"""
        self.q_values = np.zeros(self.n_actions)
        self.emotion = np.zeros(8)
        self.emotion_momentum = np.zeros(8)
        self.action_counts = np.zeros(self.n_actions)
        self.time = 0
        self.prev_reward = 0.5
        self.context = None
        self.eta = self.base_eta
        self.learning_boost = 0.2
        self.successful_emotion_patterns = {}

        # Universal memory system reset
        self.memory_effectiveness_tracker.clear()
        self.environment_stability_tracker.clear()
        self.memory_activation_level = 1.0
        self.memory_usage_history.clear()
        self.change_detection_window.clear()
        self.stable_performance_counter = 0
        self.memory_cooldown = 0
        self.context_clusters.clear()
        self.cluster_performance.clear()

        self.performance_tracker.clear()
        self.recent_context_changes.clear()
        self.action_history.clear()
        self.reward_history.clear()
        self.episodic_memory.clear()
        self.emotion_action_history.clear()
        self.eta_adaptation_counter = 0

    def get_memory_status(self):
        """Universal memory system status"""
        return {
            'memory_activation_level': round(self.memory_activation_level, 3),
            'environment_stability': round(self.assess_environment_stability(), 3),
            'memory_effectiveness': round(self.evaluate_memory_effectiveness(), 3),
            'memory_need_score': round(self.calculate_memory_need(), 3),
            'current_memory_threshold': round(getattr(self, 'current_memory_threshold', self.base_memory_threshold), 3),
            'current_memory_influence': round(getattr(self, 'current_memory_influence', self.base_memory_influence), 3),
            'memory_count': len(self.episodic_memory),
            'high_quality_memories': len([m for m in self.episodic_memory if m['quality_score'] > 0.5]),
            'context_clusters': len(self.context_clusters),
            'memory_cooldown': self.memory_cooldown,
            'performance_trend': round(self.analyze_performance_trend(), 3) if len(self.performance_tracker) >= 10 else 0.0
        }

    def get_emotion_summary(self):
        """Current state summary"""
        selected_emotions = self.select_top_emotions()

        summary = {
            'current_eta': round(self.eta, 3),
            'eta_ratio': round(self.eta / self.base_eta, 2),
            'total_emotion_energy': round(np.sum(self.emotion), 3),
            'selected_emotions': {},
            'all_emotions': {},
            'memory_status': self.get_memory_status(),
            'learning_boost': round(self.learning_boost, 3),
            'pfc_stability': round(self.prefrontal_modulation(), 3) if len(self.episodic_memory) >= 5 else 0.5,
            'neurogenesis_cycle': self.time % self.neurogenesis_cycle,
            'successful_patterns': len(self.successful_emotion_patterns)
        }

        for i, name in enumerate(self.emotion_names):
            if selected_emotions[i] > 0.1:
                summary['selected_emotions'][name] = round(float(selected_emotions[i]), 3)

        for i, name in enumerate(self.emotion_names):
            if self.emotion[i] > 0.1:
                summary['all_emotions'][name] = round(float(self.emotion[i]), 3)

        return summary

    def get_detailed_decision_breakdown(self):
        """Detailed decision process analysis"""
        breakdown = {
            'q_values': [round(q, 3) for q in self.q_values],
            'emotion_influences': [round(inf, 3) for inf in self.compute_direct_emotion_influence()],
            'memory_bias': [round(mb, 3) for mb in self.compute_adaptive_memory_bias()],
            'uncertainty_bonus': [round(ub, 3) for ub in self.compute_uncertainty_bonus()],
            'pfc_stability': round(self.prefrontal_modulation(), 3),
            'current_emotions': {name: round(float(self.emotion[i]), 3)
                               for i, name in enumerate(self.emotion_names)
                               if self.emotion[i] > 0.1},
            'memory_status': self.get_memory_status()
        }
        return breakdown

    def get_dominant_emotion(self):
        """Return the strongest emotion and its intensity"""
        max_idx = np.argmax(self.emotion)
        return self.emotion_names[max_idx], self.emotion[max_idx]


# Ablation Study classes (maintaining backward compatibility)
class ECIA_NoEmotion(ECIA):
    """Emotion system removal"""
    def emotional_processing(self, reward):
        if self.prev_reward is None:
            self.prev_reward = 0.5
        self.emotion = np.zeros(8)
        self.prev_reward = reward


class ECIA_NoMemory(ECIA):
    """Memory system removal"""
    def store_adaptive_memory(self, action, reward, prediction_error):
        pass

    def compute_adaptive_memory_bias(self):
        return np.zeros(self.n_actions)


class ECIA_NoDopamine(ECIA):
    """Dopamine adaptive learning removal"""
    def update_dopamine_learning(self, action, reward):
        prediction_error = reward - self.q_values[action]
        self.q_values[action] += 0.1 * prediction_error
        self.performance_tracker.append(reward)
        return prediction_error


class ECIA_SingleCore(ECIA):
    """Single core version (already default is single core)"""
    pass


class ECIA_NoDopamine_NoMemory(ECIA):
    """Dopamine + Memory removal"""
    def update_dopamine_learning(self, action, reward):
        prediction_error = reward - self.q_values[action]
        self.q_values[action] += 0.1 * prediction_error
        self.performance_tracker.append(reward)
        return prediction_error

    def store_adaptive_memory(self, action, reward, prediction_error):
        pass

    def compute_adaptive_memory_bias(self):
        return np.zeros(self.n_actions)


class ECIA_NoDopamine_NoEmotion(ECIA):
    """Dopamine + Emotion removal"""
    def update_dopamine_learning(self, action, reward):
        prediction_error = reward - self.q_values[action]
        self.q_values[action] += 0.1 * prediction_error
        self.performance_tracker.append(reward)
        return prediction_error

    def emotional_processing(self, reward):
        if self.prev_reward is None:
            self.prev_reward = 0.5
        self.emotion = np.zeros(8)
        self.prev_reward = reward


class ECIA_NoMemory_NoEmotion(ECIA):
    """Memory + Emotion removal"""
    def store_adaptive_memory(self, action, reward, prediction_error):
        pass

    def compute_adaptive_memory_bias(self):
        return np.zeros(self.n_actions)

    def emotional_processing(self, reward):
        if self.prev_reward is None:
            self.prev_reward = 0.5
        self.emotion = np.zeros(8)
        self.prev_reward = reward


class ECIA_NoAll_Components(ECIA):
    """All advanced components removal"""
    def update_dopamine_learning(self, action, reward):
        prediction_error = reward - self.q_values[action]
        self.q_values[action] += 0.1 * prediction_error
        self.performance_tracker.append(reward)
        return prediction_error

    def store_adaptive_memory(self, action, reward, prediction_error):
        pass

    def compute_adaptive_memory_bias(self):
        return np.zeros(self.n_actions)

    def emotional_processing(self, reward):
        if self.prev_reward is None:
            self.prev_reward = 0.5
        self.emotion = np.zeros(8)
        self.prev_reward = reward


# Basic agents
class EpsilonGreedyAgent:
    def __init__(self, n_actions=5, epsilon=0.1, random_state=None):
        self.n_actions = n_actions
        self.epsilon = epsilon
        self.q_values = np.zeros(n_actions)
        self.action_counts = np.zeros(n_actions)

        if random_state is not None:
            self.rng = np.random.RandomState(random_state)
        else:
            self.rng = np.random.RandomState()

    def reset(self):
        self.q_values = np.zeros(self.n_actions)
        self.action_counts = np.zeros(self.n_actions)

    def select_action(self):
        if self.rng.rand() < self.epsilon:
            return self.rng.choice(self.n_actions)
        return np.argmax(self.q_values)

    def update(self, action, reward):
        self.action_counts[action] += 1
        alpha = 1 / self.action_counts[action]
        self.q_values[action] += alpha * (reward - self.q_values[action])

class ThompsonSamplingAgent:
    def __init__(self, n_actions=5, random_state=None):
        self.n_actions = n_actions
        self.priors = [(0.0, 1.0) for _ in range(n_actions)]
        self.observations = [[] for _ in range(n_actions)]

        if random_state is not None:
            self.rng = np.random.RandomState(random_state)
        else:
            self.rng = np.random.RandomState()

    def reset(self):
        self.priors = [(0.0, 1.0) for _ in range(self.n_actions)]
        self.observations = [[] for _ in range(self.n_actions)]

    def select_action(self):
        samples = [self.rng.normal(mu, sigma) for mu, sigma in self.priors]
        return np.argmax(samples)

    def update(self, action, reward):
        self.observations[action].append(reward)
        data = self.observations[action]
        if len(data) > 1:
            mu = np.mean(data)
            sigma = np.std(data) if np.std(data) > 0 else 1.0
            self.priors[action] = (mu, sigma)

class UCBAgent:
    """UCB with controlled randomness"""

    def __init__(self, n_actions=5, c=0.5, random_state=None):
        self.n_actions = n_actions
        self.c = c
        self.q_values = np.zeros(n_actions)
        self.action_counts = np.zeros(n_actions)
        self.total_steps = 0

        if random_state is not None:
            self.rng = np.random.RandomState(random_state)
        else:
            self.rng = np.random.RandomState()

    def reset(self):
        self.q_values = np.zeros(self.n_actions)
        self.action_counts = np.zeros(self.n_actions)
        self.total_steps = 0

    def select_action(self):
        self.total_steps += 1
        ucb_values = np.zeros(self.n_actions)

        for a in range(self.n_actions):
            if self.action_counts[a] == 0:
                return a
            bonus = self.c * np.sqrt(np.log(self.total_steps) / self.action_counts[a])
            ucb_values[a] = self.q_values[a] + bonus

        return np.argmax(ucb_values)

    def update(self, action, reward):
        self.action_counts[action] += 1
        alpha = 1 / self.action_counts[action]
        self.q_values[action] += alpha * (reward - self.q_values[action])


def test_unified_ecia():
    """Universal integrated ECIA test"""
    agent = ECIA(n_actions=5, memory_size=15)


    # Various environment pattern simulation
    print("\n Various environment pattern test:")

    # Pattern 1: Stable environment
    print(" Stable environment simulation...")
    for i in range(20):
        agent.update_context(i/50, 0)
        action = agent.select_action()
        reward = 0.7 + np.random.normal(0, 0.1)
        agent.update(action, reward)

    stable_status = agent.get_memory_status()
    print(f" Memory activation: {stable_status['memory_activation_level']}")
    print(f" Environment stability: {stable_status['environment_stability']}")
    print(f" Memory need score: {stable_status['memory_need_score']}")

    # Pattern 2: Changing environment
    print("\n Changing environment simulation...")
    for i in range(20):
        agent.update_context((20+i)/50, (i//10))
        action = agent.select_action()
        reward = 0.3 if i < 10 else 0.8 + np.random.normal(0, 0.2)
        agent.update(action, reward)

    changing_status = agent.get_memory_status()
    print(f" Memory activation: {changing_status['memory_activation_level']}")
    print(f" Environment stability: {changing_status['environment_stability']}")
    print(f" Memory need score: {changing_status['memory_need_score']}")
    print(f" Performance trend: {changing_status['performance_trend']}")


    agent.reset()

def bonferroni_correction(p_values, alpha=0.05):
    """Simple Bonferroni correction implementation without statsmodels"""
    p_values = np.array(p_values)
    n_tests = len(p_values)

    # Bonferroni correction
    p_corrected = p_values * n_tests
    p_corrected = np.clip(p_corrected, 0, 1)

    rejected = p_corrected < alpha

    return rejected, p_corrected

def perform_multiple_comparison_correction(complete_meta_results, save_path="content/Results/meta_analysis"):
    """Perform Bonferroni correction for multiple comparisons in analysis"""

    from scipy.stats import ttest_ind, mannwhitneyu
    import pandas as pd

    print("\n MULTIPLE COMPARISON CORRECTION (Bonferroni)")
    print("=" * 70)

    correction_results = {}

    for env_name, env_results in complete_meta_results.items():
        print(f"\n Environment: {env_name}")
        # Extract seed-level data for all agents
        agent_data = {}
        agent_names = []

        for agent_name, agent_result in env_results.items():
            if agent_result['meta_statistics']['n_master_seeds'] > 0:
                seed_means = []
                for seed_key, seed_result in agent_result['individual_seeds'].items():
                    if seed_result['success_rate'] > 0:
                        seed_means.append(seed_result['mean_reward'])

                if len(seed_means) >= 3:  # Minimum data requirement
                    agent_data[agent_name] = seed_means
                    agent_names.append(agent_name)

        if len(agent_names) < 2:
            print(f"   Insufficient data for comparisons in {env_name}")
            continue

        # Perform all pairwise comparisons
        comparison_results = []
        p_values = []
        comparison_names = []

        for i in range(len(agent_names)):
            for j in range(i+1, len(agent_names)):
                agent1, agent2 = agent_names[i], agent_names[j]
                data1, data2 = agent_data[agent1], agent_data[agent2]

                # Perform appropriate statistical test
                try:
                    # Use t-test (could be enhanced with normality checking)
                    stat, p_val = ttest_ind(data1, data2, equal_var=False)

                    # Effect size (Cohen's d)
                    pooled_std = np.sqrt(((len(data1) - 1) * np.var(data1) +
                                         (len(data2) - 1) * np.var(data2)) /
                                        (len(data1) + len(data2) - 2))
                    cohens_d = (np.mean(data1) - np.mean(data2)) / pooled_std if pooled_std > 0 else 0

                    comparison_results.append({
                        'Agent_1': agent1,
                        'Agent_2': agent2,
                        'Mean_1': np.mean(data1),
                        'Mean_2': np.mean(data2),
                        'Mean_Diff': np.mean(data1) - np.mean(data2),
                        'P_Value_Raw': p_val,
                        'Cohens_D': cohens_d,
                        'Test_Statistic': stat
                    })

                    p_values.append(p_val)
                    comparison_names.append(f"{agent1}_vs_{agent2}")

                except Exception as e:
                    print(f"    ⚠️ Test failed for {agent1} vs {agent2}: {e}")

        if p_values:
            # Apply Bonferroni correction
            rejected, p_corrected = bonferroni_correction(p_values, alpha=0.05)


            # Add corrected results
            for i, result in enumerate(comparison_results):
                result['P_Value_Bonferroni'] = p_corrected[i]
                result['Significant_Bonferroni'] = rejected[i]
                result['Significant_Raw'] = result['P_Value_Raw'] < 0.05

            correction_results[env_name] = comparison_results

            # Print significant results after correction
            significant_after_correction = [r for r in comparison_results if r['Significant_Bonferroni']]

            print(f"   Total comparisons: {len(comparison_results)}")
            print(f"   Significant before correction: {sum(r['Significant_Raw'] for r in comparison_results)}")
            print(f"   Significant after Bonferroni: {len(significant_after_correction)}")

            if significant_after_correction:
                print(f"   Significant comparisons after Bonferroni correction:")
                for result in significant_after_correction:
                    direction = ">" if result['Mean_Diff'] > 0 else "<"
                    print(f"    {result['Agent_1']} {direction} {result['Agent_2']}: "
                          f"p_corrected={result['P_Value_Bonferroni']:.6f}, d={result['Cohens_D']:.3f}")
            else:
                print(f"   No significant differences after Bonferroni correction")

        # Save detailed results to CSV
        if comparison_results:
            df = pd.DataFrame(comparison_results)
            csv_filename = f"{save_path}/bonferroni_correction_{env_name}.csv"
            df.to_csv(csv_filename, index=False)
            print(f"   Detailed results saved: {csv_filename}")

    # Save overall correction results
    with open(f"{save_path}/bonferroni_correction_results.pkl", "wb") as f:
        pickle.dump(correction_results, f)

    print(f"\n Multiple comparison correction completed!")
    return correction_results

def perform_multiple_comparison_correction_cross_dataset(complete_results, save_path="content/Results/cross_dataset_study"):
    """Perform Bonferroni correction for cross-dataset study (ECIA_Full vs Baseline only)"""

    from scipy.stats import ttest_ind
    import pandas as pd

    print("\n MULTIPLE COMPARISON CORRECTION (Bonferroni) - Cross-Dataset Study")
    print("=" * 70)

    # Focus on ECIA_Full vs Baseline only
    baseline_agents = ["EpsilonGreedy", "UCB", "TS"]
    target_agents = ["ECIA_Full"] + baseline_agents

    # Metrics to test
    metrics_to_test = [
        'overall_performance_mean',
        'recovery_rate_mean',
        'recovery_time_mean'
    ]

    correction_results = {}

    for metric_name in metrics_to_test:
        print(f"\n Metric: {metric_name}")

        # Extract data for this metric
        agent_data = {}

        for agent_name in target_agents:
            if agent_name in complete_results:
                agent_result = complete_results[agent_name]

                metric_values = []
                for seed_key, seed_result in agent_result['individual_seeds'].items():
                    if seed_result['n_experiments'] > 0:
                        if metric_name == 'overall_performance_mean':
                            metric_values.extend(seed_result['overall_performances'])
                        elif metric_name == 'recovery_rate_mean':
                            metric_values.extend(seed_result['recovery_rates'])
                        elif metric_name == 'recovery_time_mean':
                            metric_values.extend(seed_result['recovery_times'])
                if len(metric_values) >= 3:
                    agent_data[agent_name] = metric_values

        if len(agent_data) < 2:
            print(f"   Insufficient data for {metric_name}")
            continue

        # Perform pairwise comparisons
        comparison_results = []
        p_values = []

        agent_names = list(agent_data.keys())
        for i in range(len(agent_names)):
            for j in range(i+1, len(agent_names)):
                agent1, agent2 = agent_names[i], agent_names[j]
                data1, data2 = agent_data[agent1], agent_data[agent2]

                try:
                    stat, p_val = ttest_ind(data1, data2, equal_var=False)

                    # Effect size
                    pooled_std = np.sqrt(((len(data1) - 1) * np.var(data1) +
                                         (len(data2) - 1) * np.var(data2)) /
                                        (len(data1) + len(data2) - 2))
                    cohens_d = (np.mean(data1) - np.mean(data2)) / pooled_std if pooled_std > 0 else 0

                    comparison_results.append({
                        'Metric': metric_name,
                        'Agent_1': agent1,
                        'Agent_2': agent2,
                        'Mean_1': np.mean(data1),
                        'Mean_2': np.mean(data2),
                        'Mean_Diff': np.mean(data1) - np.mean(data2),
                        'P_Value_Raw': p_val,
                        'Cohens_D': cohens_d
                    })

                    p_values.append(p_val)

                except Exception as e:
                    print(f"    Test failed for {agent1} vs {agent2}: {e}")

        if p_values:
            # Apply Bonferroni correction
            rejected, p_corrected = bonferroni_correction(p_values, alpha=0.05)

            # Add corrected results
            for i, result in enumerate(comparison_results):
                result['P_Value_Bonferroni'] = p_corrected[i]
                result['Significant_Bonferroni'] = rejected[i]
                result['Significant_Raw'] = result['P_Value_Raw'] < 0.05

            correction_results[metric_name] = comparison_results

            # Print results
            significant_after = sum(r['Significant_Bonferroni'] for r in comparison_results)
            significant_before = sum(r['Significant_Raw'] for r in comparison_results)

            print(f"   Significant before/after correction: {significant_before}/{significant_after}")

            if significant_after > 0:
                print(f"   Significant after Bonferroni:")
                for result in comparison_results:
                    if result['Significant_Bonferroni']:
                        direction = ">" if result['Mean_Diff'] > 0 else "<"
                        print(f"    {result['Agent_1']} {direction} {result['Agent_2']}: "
                              f"p_corrected={result['P_Value_Bonferroni']:.6f}")

    # Save results
    if correction_results:
        all_results = []
        for metric_name, metric_results in correction_results.items():
            all_results.extend(metric_results)

        df = pd.DataFrame(all_results)
        csv_filename = f"{save_path}/cross_dataset_bonferroni_correction.csv"
        df.to_csv(csv_filename, index=False)
        print(f"\n📄 Detailed results saved: {csv_filename}")

        with open(f"{save_path}/cross_dataset_bonferroni_correction_results.pkl", "wb") as f:
            pickle.dump(correction_results, f)

    print(f"\n Cross-dataset multiple comparison correction completed!")
    return correction_results


if __name__ == "__main__":
    test_unified_ecia()



import numpy as np
import matplotlib.pyplot as plt
import os
from tqdm import tqdm

# =====================
# 1. SIMPLE EMOTION COLLECTOR
# =====================

def collect_ecia_emotions_simple(env_class, n_trials=200, n_runs=50, env_name="Environment"):
    """Simple emotion collection for ECIA_Full"""

    print(f" Collecting ECIA_Full emotions for {env_name} ({n_runs} runs)")

    # ECIA_Full parameters
    ecia_params = {
        "n_actions": 5,
        "epsilon": 0.03,
        "eta": 0.55,
        "xi": 0.001,
        "memory_threshold": 0.015,
        "memory_influence": 0.3,
        "memory_similarity_threshold": 0.035,
        "top_k": 3,
        "alpha": 0.22,
        "window_size": 30,
        "memory_size": 15,
        "emotion_decay": 0.96,
        "min_eta": 0.095,
    }

    # Store emotion data for all runs
    all_emotion_data = []  # [run][trial][emotion_idx]

    for run in tqdm(range(n_runs), desc=f"{env_name}"):
        # Initialize environment and agent
        env = env_class(random_state=run)
        agent = ECIA(random_state=run + 1000, **ecia_params)

        env.reset()
        agent.reset()

        # Store emotions for this run
        run_emotions = []  # [trial][emotion_idx]

        for trial in range(n_trials):
            action = agent.select_action()
            reward, context = env.step(action)

            # Capture emotion state
            emotion_state = agent.emotion.copy()  # 8 emotions
            run_emotions.append(emotion_state)

            # Update agent
            agent.update(context, action, reward)

        all_emotion_data.append(run_emotions)

    return np.array(all_emotion_data)  # Shape: [n_runs, n_trials, 8]

def run_complete_meta_analysis():
    """Run complete meta-analysis for all agents and environments"""

    print("=" * 80)
    print("META-ANALYSIS STUDY")
    print("12 Master Seeds × 300 Runs = 3600 Total Experiments per Agent")
    print("=" * 80)

    # Optimized agent parameters
    optimized_params = {
        "n_actions": 5,
        "epsilon": 0.03,
        "eta": 0.55,
        "xi": 0.001,
        "memory_threshold": 0.015,
        "memory_influence": 0.3,
        "memory_similarity_threshold": 0.035,
        "top_k": 3,
        "alpha": 0.22,
        "window_size": 30,
        "memory_size": 15,
        "emotion_decay": 0.96,
        "min_eta": 0.095,
    }

    # Agent configurations for meta-analysis
    agents = {
        # Baseline algorithms
        "EpsilonGreedy": (EpsilonGreedyAgent, {"n_actions": 5, "epsilon": 0.1}),
        "UCB": (UCBAgent, {"n_actions": 5, "c": 0.5}),
        "TS" : (ThompsonSamplingAgent, {"n_actions":5}),
        # ECIA Full
        "ECIA_Full": (ECIA, optimized_params),
        "ECIA_NoEmotion": (ECIA_NoEmotion, optimized_params),
        "ECIA_NoMemory": (ECIA_NoMemory, optimized_params),
        "ECIA_NoDopamine": (ECIA_NoDopamine, optimized_params),
        "ECIA_NoDop_NoMem": (ECIA_NoDopamine_NoMemory, optimized_params),
        "ECIA_NoDop_NoEmo": (ECIA_NoDopamine_NoEmotion, optimized_params),
        "ECIA_NoMem_NoEmo": (ECIA_NoMemory_NoEmotion, optimized_params),
        "ECIA_NoAll": (ECIA_NoAll_Components, optimized_params)
    }


    # Environment configurations
    environments = {
        "EnvA": EnvironmentA,
        "EnvB": EnvironmentB,
        "EnvC": EnvironmentC,
    }

    # Store all meta-analysis results
    complete_meta_results = {}

    # Run meta-analysis for each environment
    for env_name, env_class in environments.items():
        print(f"\n ENVIRONMENT {env_name} Meta-Analysis")
        n_trials = 200


        print(f" Configuration: {len(MANAGER.SEEDS)} seeds × {MANAGER.N_RUNS_PER_SEED} runs = {len(MANAGER.SEEDS) * MANAGER.N_RUNS_PER_SEED} experiments")
        print(f"Expected change points: {get_environment_change_points(env_name)}")
        print("-" * 60)

        complete_meta_results[env_name] = {}

        # Run meta-analysis for each agent in this environment
        for agent_name, (agent_class, agent_kwargs) in agents.items():
            print(f"\n Agent: {agent_name}")

            meta_result = run_meta_analysis_for_agent(
                env_class, agent_class, agent_kwargs,
                n_trials=n_trials, env_name=env_name
            )

            complete_meta_results[env_name][agent_name] = meta_result

            # Print comprehensive meta-statistics
            meta_stats = meta_result['meta_statistics']
            print(f"   Meta-Analysis Results:")
            print(f"    Overall Performance:")
            print(f"      Mean ± Std: {meta_stats['meta_mean']:.4f} ± {meta_stats['meta_std']:.4f}")
            print(f"      95% CI: [{meta_stats['ci_lower']:.4f}, {meta_stats['ci_upper']:.4f}]")
            print(f"    Recovery Analysis:")
            print(f"      Recovery Rate: {meta_stats['meta_recovery_rate_mean']:.4f} ± {meta_stats['meta_recovery_rate_std']:.4f}")
            print(f"      Recovery Time: {meta_stats['meta_recovery_time_mean']:.2f} ± {meta_stats['meta_recovery_time_std']:.2f} trials")
            print(f"    Experiment Details:")
            print(f"      Total experiments: {meta_stats['total_experiments']}")
            print(f"      Master seeds used: {meta_stats['n_master_seeds']}")
            print(f"      Success rate: {meta_stats['meta_success_rate']:.1%}")

    # Save complete results
    save_results_safely(complete_meta_results, "content/Results/complete_results_full.pkl")

    # Save lightweight version
    lightweight_results = create_lightweight_results(complete_meta_results)
    save_results_safely(lightweight_results, "content/Results/complete_results.pkl")

    print(f" Lightweight results saved (original: ~1GB → compressed: ~10-50MB)")

    # Save configuration
    MANAGER.save_configuration()

    print(f"\n Complete Meta-Analysis finished!")
    print(f" Results saved in: content/Results/meta_analysis/")

    return complete_meta_results

def save_results_safely(results, filepath):
    """Safe pickle saving with backup"""
    import tempfile
    import shutil

    # Create temporary file first
    temp_filepath = filepath + ".tmp"

    try:
        with open(temp_filepath, "wb") as f:
            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)

        # If successful, replace original file
        shutil.move(temp_filepath, filepath)
        print(f" Results safely saved: {filepath}")

    except Exception as e:
        print(f" Failed to save {filepath}: {e}")
        if os.path.exists(temp_filepath):
            os.remove(temp_filepath)

def create_lightweight_results(complete_meta_results):
    """Create lightweight version of results for saving"""

    lightweight_results = {}

    for env_name, env_results in complete_meta_results.items():
        lightweight_results[env_name] = {}

        for agent_name, agent_result in env_results.items():
            if agent_result['meta_statistics']['n_master_seeds'] > 0:

                lightweight_agent_result = {
                    'meta_statistics': agent_result['meta_statistics'],  
                    'seed_level_summary': {}  
                }


                for seed_key, seed_result in agent_result['individual_seeds'].items():
                    if seed_result['success_rate'] > 0:
                        lightweight_agent_result['seed_level_summary'][seed_key] = {
                            'master_seed': seed_result['master_seed'],
                            'mean_reward': seed_result['mean_reward'],
                            'std_reward': seed_result['std_reward'],
                            'success_rate': seed_result['success_rate'],
                            'n_experiments': seed_result['n_experiments']
                        }

                lightweight_results[env_name][agent_name] = lightweight_agent_result

    return lightweight_results



#4. META-ANALYSIS EXPERIMENT EXECUTION
# =====================

def run_single_experiment(env_class, agent_class, agent_kwargs,
                                   n_trials=200, env_name="Unknown",
                                   experiment_seed=42):
    """Run a single experiment with specific seed"""

    try:
        # Initialize environment with seed
        env = env_class(random_state=experiment_seed)

        # Initialize agent with seed
        agent_kwargs_with_seed = agent_kwargs.copy()
        agent_kwargs_with_seed['random_state'] = experiment_seed + 1
        agent = agent_class(**agent_kwargs_with_seed)

        env.reset()
        agent.reset()

        rewards = []
        actions = []

        for t in range(n_trials):
            action = agent.select_action()
            reward, context = env.step(action)

            # ECIA - context save
            if hasattr(agent, 'context'):  # if ECIA
                agent.update(context, action, reward)
            else:  # other agent
                agent.update(action, reward)

            rewards.append(reward)
            actions.append(action)

        return {
            "rewards": np.array(rewards),
            "actions": np.array(actions),
            "mean_reward": np.mean(rewards),
            "success": True,
            "experiment_seed": experiment_seed,
            "env_instance": env
        }

    except Exception as e:
        print(f"    Experiment error: {e}")
        return {
            "rewards": np.zeros(n_trials),
            "actions": np.zeros(n_trials),
            "mean_reward": 0.0,
            "success": False,
            "experiment_seed": experiment_seed,
            "env_instance": None
        }

def run_master_seed_experiments(master_seed, env_class, agent_class,
                                         agent_kwargs, n_trials=200, env_name="Environment"):
    """Run experiments for a single master seed (environment-specific)"""

    # Get environment-specific settings
    seeds = MANAGER.SEEDS
    n_runs = MANAGER.N_RUNS_PER_SEED
    print(f" Master Seed {master_seed}: Running {n_runs} experiments...")

    # Generate experiment seeds with env_name
    experiment_seeds = MANAGER.generate_experiment_seeds(master_seed, env_name)

    all_rewards = []
    all_actions = []
    all_env_instances = []
    success_count = 0

    for run_id, experiment_seed in enumerate(tqdm(experiment_seeds, desc=f"Seed-{master_seed}")):
        result = run_single_experiment(
            env_class, agent_class, agent_kwargs,
            n_trials, env_name, experiment_seed
        )

        if result["success"]:
            all_rewards.append(result["rewards"])
            all_actions.append(result["actions"])
            success_count += 1

            if "env_instance" in result:
                all_env_instances.append(result["env_instance"])

    master_seed_result = {
        "master_seed": master_seed,
        "rewards": np.array(all_rewards) if success_count > 0 else np.zeros((1, n_trials)),
        "actions": np.array(all_actions) if success_count > 0 else np.zeros((1, n_trials)),
        "mean_reward": np.mean([np.mean(r) for r in all_rewards]) if success_count > 0 else 0.0,
        "std_reward": np.std([np.mean(r) for r in all_rewards]) if success_count > 0 else 0.0,
        "success_rate": success_count / len(experiment_seeds),
        "env_instances": all_env_instances,
        "n_experiments": len(experiment_seeds)
    }

    print(f"  ✅ Master Seed {master_seed}: Success rate {master_seed_result['success_rate']:.1%}, "
          f"Mean reward {master_seed_result['mean_reward']:.4f} ± {master_seed_result['std_reward']:.4f}")

    return master_seed_result



def run_meta_analysis_for_agent(env_class, agent_class, agent_kwargs,
                                         n_trials=200, env_name="Environment"):
    """Run complete meta-analysis for one agent with comprehensive analysis"""

    seeds = MANAGER.SEEDS

    meta_results = {}

    for master_seed in seeds:  
        master_seed_result = run_master_seed_experiments(
            master_seed, env_class, agent_class, agent_kwargs, n_trials, env_name
        )
        meta_results[f"seed_{master_seed}"] = master_seed_result


    # Calculate meta-statistics across all seeds
    all_mean_rewards = []
    all_std_rewards = []
    all_success_rates = []
    all_recovery_rates = []
    all_recovery_times = []

    for seed_result in meta_results.values():
        if seed_result['success_rate'] > 0:
            rewards = seed_result['rewards']
            env_instances = seed_result.get('env_instances', None)

            # Basic statistics
            all_mean_rewards.append(seed_result['mean_reward'])
            all_std_rewards.append(seed_result['std_reward'])
            all_success_rates.append(seed_result['success_rate'])

            # Comprehensive analysis for this master seed
            try:
                # Recovery rate calculation
                recovery_rates = compute_unified_recovery_rate(rewards, env_name, env_instances)
                if len(recovery_rates) > 0:
                    all_recovery_rates.append(np.mean(recovery_rates))

                # Recovery time measurement
                recovery_times = measure_unified_recovery_time(rewards, env_name, env_instances)
                if len(recovery_times) > 0:
                    all_recovery_times.append(np.mean(recovery_times))


            except Exception as e:
                print(f"    Warning: Comprehensive analysis failed for master seed {seed_result['master_seed']}: {e}")

    if all_mean_rewards:
        meta_statistics = {
            # Basic meta-statistics
            "meta_mean": np.mean(all_mean_rewards),
            "meta_std": np.std(all_mean_rewards),
            "meta_sem": np.std(all_mean_rewards) / np.sqrt(len(all_mean_rewards)),
            "meta_success_rate": np.mean(all_success_rates),
            "n_master_seeds": len(all_mean_rewards),
            "total_experiments": sum(seed_result['n_experiments'] for seed_result in meta_results.values()),

            # Comprehensive meta-statistics
            "meta_recovery_rate_mean": np.mean(all_recovery_rates) if all_recovery_rates else 0.0,
            "meta_recovery_rate_std": np.std(all_recovery_rates) if all_recovery_rates else 0.0,
            "meta_recovery_time_mean": np.mean(all_recovery_times) if all_recovery_times else 0.0,
            "meta_recovery_time_std": np.std(all_recovery_times) if all_recovery_times else 0.0
        }

        # Calculate confidence intervals
        if len(all_mean_rewards) > 1:
            alpha = 0.05  # 95% confidence interval
            t_critical = stats.t.ppf(1 - alpha/2, len(all_mean_rewards) - 1)

            # Basic CI
            margin_error = t_critical * meta_statistics['meta_sem']
            meta_statistics['ci_lower'] = meta_statistics['meta_mean'] - margin_error
            meta_statistics['ci_upper'] = meta_statistics['meta_mean'] + margin_error

            # Recovery rate CI
            if all_recovery_rates:
                recovery_sem = np.std(all_recovery_rates) / np.sqrt(len(all_recovery_rates))
                recovery_margin = t_critical * recovery_sem
                meta_statistics['recovery_rate_ci_lower'] = meta_statistics['meta_recovery_rate_mean'] - recovery_margin
                meta_statistics['recovery_rate_ci_upper'] = meta_statistics['meta_recovery_rate_mean'] + recovery_margin

            # Recovery time CI
            if all_recovery_times:
                time_sem = np.std(all_recovery_times) / np.sqrt(len(all_recovery_times))
                time_margin = t_critical * time_sem
                meta_statistics['recovery_time_ci_lower'] = meta_statistics['meta_recovery_time_mean'] - time_margin
                meta_statistics['recovery_time_ci_upper'] = meta_statistics['meta_recovery_time_mean'] + time_margin
        else:
            meta_statistics.update({
                'ci_lower': meta_statistics['meta_mean'],
                'ci_upper': meta_statistics['meta_mean'],
                'recovery_rate_ci_lower': meta_statistics['meta_recovery_rate_mean'],
                'recovery_rate_ci_upper': meta_statistics['meta_recovery_rate_mean'],
                'recovery_time_ci_lower': meta_statistics['meta_recovery_time_mean'],
                'recovery_time_ci_upper': meta_statistics['meta_recovery_time_mean']
            })
    else:
        meta_statistics = {
            "meta_mean": 0.0, "meta_std": 0.0, "meta_sem": 0.0,
            "meta_success_rate": 0.0, "n_master_seeds": 0, "total_experiments": 0,
            "ci_lower": 0.0, "ci_upper": 0.0,
            "meta_recovery_rate_mean": 0.0, "meta_recovery_rate_std": 0.0,
            "meta_recovery_time_mean": 0.0, "meta_recovery_time_std": 0.0,
            "recovery_rate_ci_lower": 0.0, "recovery_rate_ci_upper": 0.0,
            "recovery_time_ci_lower": 0.0, "recovery_time_ci_upper": 0.0
        }

    complete_result = {
        "individual_seeds": meta_results,
        "meta_statistics": meta_statistics,
        "seeds": seeds,
        "env_name": env_name,
        "comprehensive_analysis_included": True
    }

    return complete_result


# =====================
# 6. META-ANALYSIS STATISTICAL COMPARISON
# =====================

def perform_meta_statistical_comparison(complete_meta_results):
    """Perform statistical comparison across meta-analysis results"""

    print("\n META-ANALYSIS STATISTICAL COMPARISON")
    print("=" * 60)

    meta_comparison_results = {}

    for env_name, env_results in complete_meta_results.items():
        print(f"\n Environment: {env_name}")
        print("-" * 40)

        # Extract meta-means for all agents
        agent_meta_data = {}
        for agent_name, agent_result in env_results.items():
            meta_stats = agent_result['meta_statistics']
            if meta_stats['n_master_seeds'] > 0:
                # Collect individual seed results for statistical testing
                seed_means = []
                for seed_key, seed_result in agent_result['individual_seeds'].items():
                    if seed_result['success_rate'] > 0:
                        seed_means.append(seed_result['mean_reward'])

                agent_meta_data[agent_name] = {
                    'seed_means': seed_means,
                    'meta_mean': meta_stats['meta_mean'],
                    'meta_std': meta_stats['meta_std'],
                    'meta_sem': meta_stats['meta_sem'],
                    'n_seeds': len(seed_means)
                }

        # Perform pairwise comparisons
        env_comparisons = {}
        agent_names = list(agent_meta_data.keys())

        for i in range(len(agent_names)):
            for j in range(i+1, len(agent_names)):
                agent1, agent2 = agent_names[i], agent_names[j]

                data1 = agent_meta_data[agent1]['seed_means']
                data2 = agent_meta_data[agent2]['seed_means']

                if len(data1) >= 3 and len(data2) >= 3:
                    # Perform appropriate statistical test
                    try:
                        # Step 1: Normality test (Shapiro-Wilk)
                        _, p_norm1 = stats.shapiro(data1)
                        _, p_norm2 = stats.shapiro(data2)

                        if p_norm1 > 0.05 and p_norm2 > 0.05:
                            # Both normal - Step 2: Equal variance test (Levene's)
                            _, p_levene = stats.levene(data1, data2)

                            if p_levene > 0.05:
                                # Equal variances - use Student's t-test
                                stat, p_val = stats.ttest_ind(data1, data2, equal_var=True)
                                test_used = "Student's t-test"
                            else:
                                # Unequal variances - use Welch's t-test
                                stat, p_val = stats.ttest_ind(data1, data2, equal_var=False)
                                test_used = "Welch's t-test"
                        else:
                            # Non-normal - use Mann-Whitney U
                            stat, p_val = stats.mannwhitneyu(data1, data2, alternative='two-sided')
                            test_used = "Mann-Whitney U test"


                        # Calculate effect size (Cohen's d)
                        pooled_std = np.sqrt(((len(data1) - 1) * np.var(data1) +
                                            (len(data2) - 1) * np.var(data2)) /
                                           (len(data1) + len(data2) - 2))

                        if pooled_std > 0:
                            cohens_d = (np.mean(data1) - np.mean(data2)) / pooled_std
                        else:
                            cohens_d = 0

                        comparison_key = f"{agent1}_vs_{agent2}"
                        env_comparisons[comparison_key] = {
                            'mean_diff': np.mean(data1) - np.mean(data2),
                            'p_value': p_val,
                            'cohens_d': cohens_d,
                            'test_used': test_used,
                            'significant': p_val < 0.05,
                            'effect_size_interpretation': interpret_cohens_d(cohens_d)
                        }

                        # Print significant results
                        if p_val < 0.05:
                            direction = ">" if np.mean(data1) > np.mean(data2) else "<"
                            print(f"  {agent1} {direction} {agent2}:")
                            print(f"    p-value: {p_val:.4f} ({test_used})")
                            print(f"    Effect size: {cohens_d:.3f} ({interpret_cohens_d(cohens_d)})")

                    except Exception as e:
                        print(f"    Statistical test failed for {agent1} vs {agent2}: {e}")

        meta_comparison_results[env_name] = env_comparisons

    return meta_comparison_results


def interpret_cohens_d(d):
    """Interpret Cohen's d effect size"""
    abs_d = abs(d)
    if abs_d < 0.2:
        return "negligible"
    elif abs_d < 0.5:
        return "small"
    elif abs_d < 0.8:
        return "medium"
    else:
        return "large"


# =====================
# 7. VISUALIZATION AND REPORTING
# =====================

def save_csv_data(complete_meta_results, save_path="content/Results/meta_analysis"):
    """Save comprehensive analysis data to CSV with statistical measures"""

    import pandas as pd
    import os

    os.makedirs(save_path, exist_ok=True)

    # Environment-level summary with full statistics
    for env_name, env_results in complete_meta_results.items():
        print(f"\n Creating enhanced CSV for {env_name}")

        # Agent-level summary data
        agent_summary_data = []

        # Individual experiment-level data
        detailed_data = []

        for agent_name, agent_result in env_results.items():
            if agent_result['meta_statistics']['n_master_seeds'] > 0:
                meta_stats = agent_result['meta_statistics']

                # Agent summary with full statistics
                agent_summary_data.append({
                    'Environment': env_name,
                    'Agent': agent_name,
                    'Agent_Type': 'BASELINE' if agent_name in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',

                    # Core Performance Metrics
                    'Meta_Mean': meta_stats['meta_mean'],
                    'Meta_Std': meta_stats['meta_std'],
                    'Meta_SEM': meta_stats['meta_sem'],
                    'CI_Lower_95': meta_stats.get('ci_lower', 0),
                    'CI_Upper_95': meta_stats.get('ci_upper', 0),
                    'CI_Width': meta_stats.get('ci_upper', 0) - meta_stats.get('ci_lower', 0),

                    # Recovery Metrics
                    'Recovery_Rate_Mean': meta_stats.get('meta_recovery_rate_mean', 0),
                    'Recovery_Rate_Std': meta_stats.get('meta_recovery_rate_std', 0),
                    'Recovery_Rate_CI_Lower': meta_stats.get('recovery_rate_ci_lower', 0),
                    'Recovery_Rate_CI_Upper': meta_stats.get('recovery_rate_ci_upper', 0),

                    'Recovery_Time_Mean': meta_stats.get('meta_recovery_time_mean', 0),
                    'Recovery_Time_Std': meta_stats.get('meta_recovery_time_std', 0),
                    'Recovery_Time_CI_Lower': meta_stats.get('recovery_time_ci_lower', 0),
                    'Recovery_Time_CI_Upper': meta_stats.get('recovery_time_ci_upper', 0),

                    # Experimental Details
                    'N_Master_Seeds': meta_stats['n_master_seeds'],
                    'Total_Experiments': meta_stats['total_experiments'],
                    'Success_Rate': meta_stats['meta_success_rate'],

                    # Derived Statistics
                    'Coefficient_of_Variation': meta_stats['meta_std'] / meta_stats['meta_mean'] if meta_stats['meta_mean'] != 0 else 0,
                    'Performance_Reliability': 1 - (meta_stats['meta_std'] / meta_stats['meta_mean']) if meta_stats['meta_mean'] != 0 else 0
                })

                # Collect all individual experiment data with seed information
                for seed_key, seed_result in agent_result['individual_seeds'].items():
                    if seed_result['success_rate'] > 0:
                        rewards = seed_result['rewards']  # Shape: (300, 200)
                        env_instances = seed_result.get('env_instances', None)

                        # Each of 300 runs for this master seed
                        for run_idx in range(rewards.shape[0]):
                            run_rewards = rewards[run_idx]
                            run_mean = np.mean(run_rewards)

                            # Calculate recovery metrics for this run
                            run_rewards_reshaped = run_rewards.reshape(1, -1)
                            recovery_rates = compute_unified_recovery_rate(run_rewards_reshaped, env_name, env_instances)
                            recovery_times = measure_unified_recovery_time(run_rewards_reshaped, env_name, env_instances)

                            detailed_data.append({
                                'Environment': env_name,
                                'Agent': agent_name,
                                'Agent_Type': 'BASELINE' if agent_name in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',
                                'Master_Seed': seed_result['master_seed'],
                                'Run_Index': run_idx,
                                'Global_Experiment_Index': len(detailed_data),  # Unique experiment ID

                                # Performance Metrics
                                'Mean_Reward': run_mean,
                                'Recovery_Rate': np.mean(recovery_rates) if len(recovery_rates) > 0 else 0,
                                'Recovery_Time': np.mean(recovery_times) if len(recovery_times) > 0 else 0,

                                # Additional Run Statistics
                                'Reward_Std': np.std(run_rewards),
                                'Reward_Min': np.min(run_rewards),
                                'Reward_Max': np.max(run_rewards),
                                'Reward_Median': np.median(run_rewards),
                                'Reward_Q25': np.percentile(run_rewards, 25),
                                'Reward_Q75': np.percentile(run_rewards, 75),

                                # Trial-wise Performance Indicators
                                'Early_Performance': np.mean(run_rewards[:50]),  # First 50 trials
                                'Late_Performance': np.mean(run_rewards[-50:]),  # Last 50 trials
                                'Performance_Trend': np.polyfit(range(len(run_rewards)), run_rewards, 1)[0],  # Linear trend
                            })

        # Save agent summary CSV
        summary_df = pd.DataFrame(agent_summary_data)
        summary_csv = f"{save_path}/agent_summary_statistics_{env_name}.csv"
        summary_df.to_csv(summary_csv, index=False)
        print(f"   Agent summary saved: {summary_csv} ({len(summary_df)} agents)")

        # Save detailed experiment CSV
        detailed_df = pd.DataFrame(detailed_data)
        detailed_csv = f"{save_path}/detailed_experiments_{env_name}.csv"
        detailed_df.to_csv(detailed_csv, index=False)
        print(f"   Detailed data saved: {detailed_csv} ({len(detailed_df)} experiments)")


def plot_emotion_trajectories_simple(emotion_data, env_name, save_path="emotion_plots"):
    """Plot 8 emotions in 4x2 grid (vertical layout) showing average trajectory over trials"""

    os.makedirs(save_path, exist_ok=True)

    emotion_names = [
        "Fear", "Joy", "Hope", "Sadness",
        "Curiosity", "Anger", "Pride", "Shame"
    ]

    # Calculate average emotion trajectories across runs
    avg_emotions = np.mean(emotion_data, axis=0)  # Shape: [n_trials, 8]
    std_emotions = np.std(emotion_data, axis=0)   # Shape: [n_trials, 8]

    n_trials = avg_emotions.shape[0]
    trials = np.arange(n_trials)

    # Create 4x2 subplot (4 rows, 2 columns) - CHANGED FROM 2x4
    fig, axes = plt.subplots(4, 2, figsize=(12, 16))  # CHANGED: taller figure
    axes = axes.flatten()

    colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4',
              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']

    for i in range(8):
        ax = axes[i]
        emotion_name = emotion_names[i]

        # Plot mean trajectory
        ax.plot(trials, avg_emotions[:, i], color=colors[i], linewidth=2,
                label=f'Mean {emotion_name}')

        # Plot confidence interval (mean ± std)
        ax.fill_between(trials,
                       avg_emotions[:, i] - std_emotions[:, i],
                       avg_emotions[:, i] + std_emotions[:, i],
                       color=colors[i], alpha=0.3)

        ax.set_title(f'{emotion_name}', fontweight='bold', fontsize=12)
        ax.set_xlabel('Trial')
        ax.set_ylabel('Emotion Intensity')
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1)

        # Add some stats
        mean_intensity = np.mean(avg_emotions[:, i])
        ax.text(0.02, 0.98, f'Avg: {mean_intensity:.3f}',
                transform=ax.transAxes, fontsize=9,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8),
                verticalalignment='top')

    plt.suptitle(f'ECIA_Full Emotion Trajectories - {env_name}\n'
                 f'Average across {emotion_data.shape[0]} runs',
                 fontsize=14, fontweight='bold')
    plt.tight_layout()

    # Save plot
    plt.savefig(f"{save_path}/emotion_trajectories_{env_name}.png", dpi=300, bbox_inches='tight')
    plt.close()

    print(f" Emotion plot saved: {save_path}/emotion_trajectories_{env_name}.png")

def plot_emotion_comparison_all_envs(data_path="content/Results/simple_emotion_analysis"):
    """Create comparison plot showing all environments together - 4x2 layout"""

    environments = ["EnvA", "EnvB", "EnvC"]
    emotion_names = [
        "Fear", "Joy", "Hope", "Sadness",
        "Curiosity", "Anger", "Pride", "Shame"
    ]

    # Load all data
    all_env_data = {}
    for env_name in environments:
        data_file = f"{data_path}/emotion_data_{env_name}.npy"
        if os.path.exists(data_file):
            emotion_data = np.load(data_file)
            avg_emotions = np.mean(emotion_data, axis=0)  # Average across runs
            all_env_data[env_name] = avg_emotions

    if not all_env_data:
        print(" No emotion data found for comparison plot")
        return

    # Create comparison plot - CHANGED TO 4x2
    fig, axes = plt.subplots(4, 2, figsize=(12, 16))  # CHANGED: 4 rows, 2 columns
    axes = axes.flatten()

    colors = {'EnvA': '#1f77b4', 'EnvB': '#ff7f0e', 'EnvC': '#2ca02c'}

    for i in range(8):
        ax = axes[i]
        emotion_name = emotion_names[i]

        for env_name, avg_emotions in all_env_data.items():
            trials = np.arange(len(avg_emotions))
            ax.plot(trials, avg_emotions[:, i],
                   color=colors[env_name], linewidth=2,
                   label=env_name, alpha=0.8)

        ax.set_title(f'{emotion_name}', fontweight='bold', fontsize=12)
        ax.set_xlabel('Trial')
        ax.set_ylabel('Emotion Intensity')
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1)

        if i == 0:  # Add legend to first subplot
            ax.legend(loc='upper right')

    plt.suptitle('ECIA_Full Emotion Trajectories - Environment Comparison',
                 fontsize=16, fontweight='bold')
    plt.tight_layout()

    # Save comparison plot
    plt.savefig(f"{data_path}/emotion_comparison_all_environments.png",
                dpi=300, bbox_inches='tight')
    plt.close()

    print(f" Comparison plot saved: {data_path}/emotion_comparison_all_environments.png")


def save_enhanced_cross_dataset_csv_data(complete_results, save_path="content/Results/cross_dataset_study"):
    """Save comprehensive cross-dataset study data to CSV with statistical measures"""

    import pandas as pd
    import os

    os.makedirs(save_path, exist_ok=True)

    # Agent-level summary data
    agent_summary_data = []

    # Individual experiment-level data
    detailed_data = []

    for agent_name, agent_result in complete_results.items():
        meta_stats = agent_result['meta_statistics']

        # Agent summary with full statistics
        agent_summary_data.append({
            'Agent': agent_name,
            'Agent_Type': 'BASELINE' if agent_name in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',

            # Core Performance Metrics
            'Overall_Performance_Mean': meta_stats.get('overall_performance_mean', 0),
            'Overall_Performance_Std': meta_stats.get('overall_performance_std', 0),

            # Recovery Metrics with CI
            'Recovery_Rate_Mean': meta_stats.get('recovery_rate_mean', 0),
            'Recovery_Rate_Std': meta_stats.get('recovery_rate_std', 0),

            'Recovery_Time_Mean': meta_stats.get('recovery_time_mean', 0),
            'Recovery_Time_Std': meta_stats.get('recovery_time_std', 0),

            # Experimental Details
            'N_Master_Seeds': meta_stats.get('n_master_seeds', 0),
            'Total_Experiments': meta_stats.get('n_total_experiments', 0),

            # Derived Statistics
            'Performance_CV': meta_stats.get('overall_performance_std', 0) / meta_stats.get('overall_performance_mean', 1),
            'Recovery_Efficiency': meta_stats.get('recovery_rate_mean', 0) / max(meta_stats.get('recovery_time_mean', 1), 1)
        })

        # Collect individual experiment data
        for seed_key, seed_result in agent_result['individual_seeds'].items():
            if seed_result['n_experiments'] > 0:
                # Each experiment within this seed
                overall_perfs = seed_result['overall_performances']
                recovery_rates = seed_result['recovery_rates']
                recovery_times = seed_result['recovery_times']

                for exp_idx in range(len(overall_perfs)):
                    detailed_data.append({
                        'Agent': agent_name,
                        'Agent_Type': 'BASELINE' if agent_name in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',
                        'Master_Seed': seed_result['master_seed'],
                        'Experiment_Index': exp_idx,
                        'Global_Experiment_Index': len(detailed_data),

                        # Core Metrics
                        'Overall_Performance': overall_perfs[exp_idx] if exp_idx < len(overall_perfs) else 0,
                        'Recovery_Rate': recovery_rates[exp_idx] if exp_idx < len(recovery_rates) else 0,
                        'Recovery_Time': recovery_times[exp_idx] if exp_idx < len(recovery_times) else 0,

                        # Performance Categories
                        'Performance_Category': 'High' if (exp_idx < len(overall_perfs) and overall_perfs[exp_idx] > 0.7) else
                                              'Medium' if (exp_idx < len(overall_perfs) and overall_perfs[exp_idx] > 0.5) else 'Low',
                        'Recovery_Category': 'Fast' if (exp_idx < len(recovery_times) and recovery_times[exp_idx] < 20) else
                                           'Medium' if (exp_idx < len(recovery_times) and recovery_times[exp_idx] < 40) else 'Slow'
                    })

    # Calculate confidence intervals for agent summaries
    for i, agent_data in enumerate(agent_summary_data):
        agent_name = agent_data['Agent']
        if agent_name in complete_results:
            # Collect all experiment data for CI calculation
            all_overall_perfs = []
            all_recovery_rates = []
            all_recovery_times = []

            for seed_result in complete_results[agent_name]['individual_seeds'].values():
                if seed_result['n_experiments'] > 0:
                    all_overall_perfs.extend(seed_result['overall_performances'])
                    all_recovery_rates.extend(seed_result['recovery_rates'])
                    all_recovery_times.extend(seed_result['recovery_times'])

            # Calculate 95% CI
            if len(all_overall_perfs) > 1:
                alpha = 0.05
                t_critical = stats.t.ppf(1 - alpha/2, len(all_overall_perfs) - 1)

                # Overall Performance CI
                perf_sem = np.std(all_overall_perfs) / np.sqrt(len(all_overall_perfs))
                perf_margin = t_critical * perf_sem
                agent_summary_data[i]['Overall_Performance_CI_Lower'] = np.mean(all_overall_perfs) - perf_margin
                agent_summary_data[i]['Overall_Performance_CI_Upper'] = np.mean(all_overall_perfs) + perf_margin
                agent_summary_data[i]['Overall_Performance_SEM'] = perf_sem

                # Recovery Rate CI
                if all_recovery_rates:
                    recovery_sem = np.std(all_recovery_rates) / np.sqrt(len(all_recovery_rates))
                    recovery_margin = t_critical * recovery_sem
                    agent_summary_data[i]['Recovery_Rate_CI_Lower'] = np.mean(all_recovery_rates) - recovery_margin
                    agent_summary_data[i]['Recovery_Rate_CI_Upper'] = np.mean(all_recovery_rates) + recovery_margin
                    agent_summary_data[i]['Recovery_Rate_SEM'] = recovery_sem

                # Recovery Time CI
                if all_recovery_times:
                    time_sem = np.std(all_recovery_times) / np.sqrt(len(all_recovery_times))
                    time_margin = t_critical * time_sem
                    agent_summary_data[i]['Recovery_Time_CI_Lower'] = np.mean(all_recovery_times) - time_margin
                    agent_summary_data[i]['Recovery_Time_CI_Upper'] = np.mean(all_recovery_times) + time_margin
                    agent_summary_data[i]['Recovery_Time_SEM'] = time_sem

    # Save agent summary CSV
    summary_df = pd.DataFrame(agent_summary_data)
    summary_csv = f"{save_path}/cross_dataset_agent_summary_statistics.csv"
    summary_df.to_csv(summary_csv, index=False)
    print(f"  📄 Agent summary saved: {summary_csv} ({len(summary_df)} agents)")

    # Save detailed experiment CSV
    detailed_df = pd.DataFrame(detailed_data)
    detailed_csv = f"{save_path}/cross_dataset_detailed_experiments.csv"
    detailed_df.to_csv(detailed_csv, index=False)
    print(f"   Detailed data saved: {detailed_csv} ({len(detailed_df)} experiments)")


def create_statistical_comparison_csv(complete_meta_results, meta_comparisons, save_path="content/Results/meta_analysis"):
    """Create CSV with statistical comparison results"""

    import pandas as pd
    import os

    os.makedirs(save_path, exist_ok=True)

    comparison_data = []

    for env_name, env_comparisons in meta_comparisons.items():
        for comparison_name, comparison_result in env_comparisons.items():
            agent1, agent2 = comparison_name.split('_vs_')

            comparison_data.append({
                'Environment': env_name,
                'Agent_1': agent1,
                'Agent_2': agent2,
                'Agent_1_Type': 'BASELINE' if agent1 in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',
                'Agent_2_Type': 'BASELINE' if agent2 in ["EpsilonGreedy", "UCB", "TS"] else 'ECIA',

                # Comparison Results
                'Mean_Difference': comparison_result['mean_diff'],
                'P_Value': comparison_result['p_value'],
                'Cohens_D': comparison_result['cohens_d'],
                'Effect_Size_Interpretation': comparison_result['effect_size_interpretation'],
                'Test_Used': comparison_result['test_used'],
                'Significant': comparison_result['significant'],

                # Performance Direction
                'Better_Agent': agent1 if comparison_result['mean_diff'] > 0 else agent2,
                'Performance_Advantage': abs(comparison_result['mean_diff']),
                'Effect_Size_Category': 'Large' if abs(comparison_result['cohens_d']) >= 0.8 else
                                       'Medium' if abs(comparison_result['cohens_d']) >= 0.5 else
                                       'Small' if abs(comparison_result['cohens_d']) >= 0.2 else 'Negligible'
            })

    # Save comparison results
    comparison_df = pd.DataFrame(comparison_data)
    comparison_csv = f"{save_path}/statistical_comparisons.csv"
    comparison_df.to_csv(comparison_csv, index=False)
    print(f" Statistical comparisons saved: {comparison_csv}")

    return comparison_df



def generate_cross_dataset_unified_report(complete_results, save_path="content/Results/cross_dataset_study"):
    """Generate comprehensive cross-dataset study report with unified metrics"""

    os.makedirs(save_path, exist_ok=True)

    report_lines = []
    report_lines.append("CROSS-DATASET STUDY ANALYSIS REPORT (UNIFIED METRICS)")
    report_lines.append("=" * 70)
    report_lines.append("Study Design: EnvA,B,C Sequential Training → RandomShift Testing")
    report_lines.append(f"Total Experiments: 3 master seeds × 300 runs = 900 per agent")
    report_lines.append("Metrics: Unified performance indicators consistent with single analysis")
    report_lines.append("")

    # Executive Summary
    report_lines.append("EXECUTIVE SUMMARY")
    report_lines.append("-" * 30)

    # Calculate overall statistics using unified metrics
    all_overall_perfs = []
    all_recovery_rates = []
    all_recovery_times = []
    agent_names = list(complete_results.keys())

    for agent_result in complete_results.values():
        meta_stats = agent_result['meta_statistics']
        all_overall_perfs.append(meta_stats.get('overall_performance_mean', 0))
        all_recovery_rates.append(meta_stats.get('recovery_rate_mean', 0))
        all_recovery_times.append(meta_stats.get('recovery_time_mean', 0))

    report_lines.append(f"• Total Agents Evaluated: {len(agent_names)}")
    report_lines.append(f"• Average Overall Performance: {np.mean(all_overall_perfs):.4f} ± {np.std(all_overall_perfs):.4f}")
    report_lines.append(f"• Average Recovery Rate: {np.mean(all_recovery_rates):.4f} ± {np.std(all_recovery_rates):.4f}")
    report_lines.append(f"• Average Recovery Time: {np.mean(all_recovery_times):.2f} ± {np.std(all_recovery_times):.2f} trials")
    report_lines.append("")

    # Agent Rankings by Overall Performance
    report_lines.append("AGENT RANKINGS BY OVERALL PERFORMANCE (UNIFIED METRICS)")
    report_lines.append("-" * 55)

    agent_performance = []
    for agent_name, agent_result in complete_results.items():
        meta_stats = agent_result['meta_statistics']
        agent_performance.append((agent_name, meta_stats.get('overall_performance_mean', 0), meta_stats))

    agent_performance.sort(key=lambda x: x[1], reverse=True)

    for rank, (agent_name, overall_perf, meta_stats) in enumerate(agent_performance, 1):
        agent_type = "BASELINE" if agent_name in ["EpsilonGreedy", "UCB", "TS"] else "ECIA"

        report_lines.append(
            f"  {rank:2d}. [{agent_type:8s}] {agent_name:15s} | "
            f"Overall: {overall_perf:.4f} | "
            f"Recovery: {meta_stats.get('recovery_rate_mean', 0):.3f} | "
        )

    report_lines.append("")

    # Detailed Performance Metrics
    report_lines.append("DETAILED PERFORMANCE METRICS")
    report_lines.append("-" * 35)

    for agent_name, agent_result in complete_results.items():
        meta_stats = agent_result['meta_statistics']
        report_lines.append(f"\n{agent_name}:")
        report_lines.append(f"  • Overall Performance: {meta_stats.get('overall_performance_mean', 0):.4f} ± {meta_stats.get('overall_performance_std', 0):.4f}")
        report_lines.append(f"  • Recovery Rate: {meta_stats.get('recovery_rate_mean', 0):.4f} ± {meta_stats.get('recovery_rate_std', 0):.4f}")
        report_lines.append(f"  • Recovery Time: {meta_stats.get('recovery_time_mean', 0):.2f} ± {meta_stats.get('recovery_time_std', 0):.2f} trials")

    report_lines.append("")

    # Key Findings
    report_lines.append("KEY FINDINGS")
    report_lines.append("-" * 20)

    # Best performer
    if agent_performance:
        best_agent, best_score, _ = agent_performance[0]
        report_lines.append(f"• Best Overall Performance: {best_agent} (Score: {best_score:.4f})")

        # ECIA vs Baseline comparison using unified metrics
        ecia_overall_perfs = []
        baseline_overall_perfs = []
        ecia_recovery_rates = []
        baseline_recovery_rates = []

        for agent_name, agent_result in complete_results.items():
            meta_stats = agent_result['meta_statistics']

            if "ECIA" in agent_name:
                ecia_overall_perfs.append(meta_stats.get('overall_performance_mean', 0))
                ecia_recovery_rates.append(meta_stats.get('recovery_rate_mean', 0))
            elif agent_name in ["EpsilonGreedy", "UCB", "TS"]:
                baseline_overall_perfs.append(meta_stats.get('overall_performance_mean', 0))
                baseline_recovery_rates.append(meta_stats.get('recovery_rate_mean', 0))

        if ecia_overall_perfs and baseline_overall_perfs:
            report_lines.append(f"• ECIA vs Baseline Overall Performance:")
            report_lines.append(f"  - ECIA Average: {np.mean(ecia_overall_perfs):.4f} ± {np.std(ecia_overall_perfs):.4f}")
            report_lines.append(f"  - Baseline Average: {np.mean(baseline_overall_perfs):.4f} ± {np.std(baseline_overall_perfs):.4f}")

            report_lines.append(f"• ECIA vs Baseline Recovery Rate:")
            report_lines.append(f"  - ECIA Average: {np.mean(ecia_recovery_rates):.4f} ± {np.std(ecia_recovery_rates):.4f}")
            report_lines.append(f"  - Baseline Average: {np.mean(baseline_recovery_rates):.4f} ± {np.std(baseline_recovery_rates):.4f}")


    report_lines.append("")

    # Methodology
    report_lines.append("METHODOLOGY")
    report_lines.append("-" * 20)
    report_lines.append("• Training Phase: Sequential learning on EnvA (200 trials) → EnvB (200 trials) → EnvC (200 trials)")
    report_lines.append("• Testing Phase: Continued learning on RandomShiftEnvironment (600 trials)")
    report_lines.append("• Unified Metrics: Same calculation methods as single environment analysis")
    report_lines.append("  - Overall Performance: Mean reward across test phase")
    report_lines.append("  - Recovery Rate: Performance recovery after environment changes")
    report_lines.append("  - Recovery Time: Trials needed to recover from performance drops")
    report_lines.append("• Seeds: 3 master seeds (42, 123, 456) × 300 runs each")

    # Save report
    with open(f"{save_path}/cross_dataset_study_report_unified.txt", "w") as f:
        f.write("\n".join(report_lines))

    print(f" Cross-dataset study unified report saved: {save_path}/cross_dataset_study_report_unified.txt")
    return "\n".join(report_lines)


# =====================
# 8. COMPREHENSIVE ANALYSIS FUNCTIONS (INTEGRATED)
# =====================

def get_environment_change_points(env_name, env_instance=None):
    """Get change points for each environment"""
    if env_name == "EnvA":
        return [100]
    elif env_name == "EnvB":
        return [40, 80, 120, 160]
    elif env_name == "EnvC":
        if env_instance and hasattr(env_instance, 'change_points'):
            return env_instance.change_points
        else:
            return []
    else:
        return []

def get_environment_optimal_rewards(env_name, env_instance=None):
    """Get optimal rewards for each segment"""
    if env_name == "EnvA":
        return [0.8, 0.9]
    elif env_name == "EnvB":
        return [0.95, 0.95, 0.95, 0.95, 0.95]
    elif env_name == "EnvC":
        if env_instance and hasattr(env_instance, 'optimal_rewards'):
            return env_instance.optimal_rewards
        else:
            return []
    else:
        return []

def calculate_noise_adjusted_parameters(env_name):
    """Calculate parameters adjusted for noise level"""
    noise_levels = {
        "EnvA": {"sigma": 0.15, "noise_level": "medium"},
        "EnvB": {"sigma": 0.05, "noise_level": "low"},
        "EnvC": {"sigma": 0.15, "noise_level": "medium"}
    }

    noise_info = noise_levels.get(env_name, {"noise_level": "medium"})

    if noise_info["noise_level"] == "low":
        threshold_ratio = 0.90
        stability_window = 3
        min_stability_trials = 2
    elif noise_info["noise_level"] == "medium":
        threshold_ratio = 0.85
        stability_window = 5
        min_stability_trials = 3
    else:
        threshold_ratio = 0.80
        stability_window = 7
        min_stability_trials = 4

    return {
        "threshold_ratio": threshold_ratio,
        "stability_window": stability_window,
        "min_stability_trials": min_stability_trials
    }

def detect_randomshift_changes_from_rewards(rewards, window_size=20):
    change_points = []


    for i in range(window_size, len(rewards) - window_size):
        before_window = rewards[i-window_size:i]
        after_window = rewards[i:i+window_size]

        # Calculate mean difference
        mean_diff = abs(np.mean(after_window) - np.mean(before_window))

        # Lower threshold for better detection
        if mean_diff > 0.10:  # Reduced from 0.15 to 0.10
            change_points.append(i)

    # Remove too close change points
    filtered_points = []
    for point in change_points:
        if not filtered_points or point - filtered_points[-1] > 25:  # Reduced from 30 to 25
            filtered_points.append(point)

    return filtered_points

def compute_unified_recovery_rate(rewards, env_name, env_instances=None, analysis_window=30):

    env_params = calculate_noise_adjusted_parameters(env_name)
    all_recovery_rates = []

    for run_idx in range(rewards.shape[0]):
        run_rewards = rewards[run_idx]

        # RandomShift의 경우 보상에서 직접 변화점 감지
        if env_name == "RandomShift":
            change_points = detect_randomshift_changes_from_rewards(run_rewards)
            if not change_points:
                continue
            optimal_rewards = [0.8] * len(change_points)
        else:
            if env_name == "EnvC":
                if env_instances is not None and len(env_instances) > run_idx:
                    actual_env = env_instances[run_idx]
                    change_points = actual_env.change_points if hasattr(actual_env, 'change_points') else get_environment_change_points(env_name)
                    optimal_rewards = actual_env.optimal_rewards if hasattr(actual_env, 'optimal_rewards') else get_environment_optimal_rewards(env_name)
                else:
                    change_points = get_environment_change_points(env_name)
                    optimal_rewards = get_environment_optimal_rewards(env_name)
            else:
                change_points = get_environment_change_points(env_name)
                optimal_rewards = get_environment_optimal_rewards(env_name)

        run_recovery_rates = []

        for change_idx, change_point in enumerate(change_points):
            if change_point >= len(run_rewards) - analysis_window:
                continue

            post_start = change_point
            post_end = min(change_point + analysis_window, len(run_rewards))

            if post_end <= post_start:
                continue

            segment_optimal = optimal_rewards[min(change_idx, len(optimal_rewards) - 1)]
            post_change_performance = np.mean(run_rewards[post_start:post_end])
            recovery_rate = post_change_performance / segment_optimal


            run_recovery_rates.append(recovery_rate)

        if run_recovery_rates:
            avg_recovery_rate = np.mean(run_recovery_rates)
            all_recovery_rates.append(avg_recovery_rate)

    return np.array(all_recovery_rates)

def measure_unified_recovery_time(rewards, env_name, env_instances=None, analysis_window=50):

    env_params = calculate_noise_adjusted_parameters(env_name)
    all_recovery_times = []

    for run_idx in range(rewards.shape[0]):
        run_rewards = rewards[run_idx]

        # RandomShift의 경우 보상에서 직접 변화점 감지
        if env_name == "RandomShift":
            change_points = detect_randomshift_changes_from_rewards(run_rewards)
            if not change_points:
                continue
            optimal_rewards = [0.8] * len(change_points)
        else:
            if env_name == "EnvC":
                if env_instances is not None and len(env_instances) > run_idx:
                    actual_env = env_instances[run_idx]
                    change_points = actual_env.change_points if hasattr(actual_env, 'change_points') else get_environment_change_points(env_name)
                    optimal_rewards = actual_env.optimal_rewards if hasattr(actual_env, 'optimal_rewards') else get_environment_optimal_rewards(env_name)
                else:
                    change_points = get_environment_change_points(env_name)
                    optimal_rewards = get_environment_optimal_rewards(env_name)
            else:
                change_points = get_environment_change_points(env_name)
                optimal_rewards = get_environment_optimal_rewards(env_name)

        run_recovery_times = []

        for change_idx, change_point in enumerate(change_points):
            if change_point >= len(run_rewards) - 10:
                continue

            segment_optimal = optimal_rewards[min(change_idx, len(optimal_rewards) - 1)]
            threshold = segment_optimal * env_params["threshold_ratio"]

            post_change_start = change_point
            post_change_end = min(change_point + analysis_window, len(run_rewards))
            post_change_rewards = run_rewards[post_change_start:post_change_end]

            recovery_time = len(post_change_rewards)
            stability_window = env_params["stability_window"]
            min_trials = env_params["min_stability_trials"]

            for i in range(min_trials, len(post_change_rewards) - stability_window + 1):
                window = post_change_rewards[i:i + stability_window]
                if np.mean(window) >= threshold:
                    recovery_time = i + stability_window // 2
                    break

            run_recovery_times.append(recovery_time)

        if run_recovery_times:
            avg_recovery_time = np.mean(run_recovery_times)
            all_recovery_times.append(avg_recovery_time)

    return np.array(all_recovery_times)


def calculate_cross_dataset_unified_metrics(training_result, test_result, env_name="RandomShift"):

    # Get test rewards and environment instance
    test_rewards = test_result['test_rewards'].reshape(1, -1)  # Shape: (1, n_trials)
    env_instances = [test_result['environment_instance']] if test_result.get('environment_instance') else None


    try:
        # Recovery rate calculation
        recovery_rates = compute_unified_recovery_rate(test_rewards, env_name, env_instances)
        recovery_rate_mean = np.mean(recovery_rates) if len(recovery_rates) > 0 else 0.0
        recovery_rate_std = np.std(recovery_rates) if len(recovery_rates) > 0 else 0.0

        # Recovery time measurement
        recovery_times = measure_unified_recovery_time(test_rewards, env_name, env_instances)
        recovery_time_mean = np.mean(recovery_times) if len(recovery_times) > 0 else 0.0
        recovery_time_std = np.std(recovery_times) if len(recovery_times) > 0 else 0.0

    except Exception as e:
        import traceback
        traceback.print_exc()

        recovery_rate_mean = recovery_rate_std = 0.0
        recovery_time_mean = recovery_time_std = 0.0
    return {
        'overall_performance': test_result['test_performance'],
        'recovery_rate_mean': recovery_rate_mean,
        'recovery_rate_std': recovery_rate_std,
        'recovery_time_mean': recovery_time_mean,
        'recovery_time_std': recovery_time_std
    }
# =====================
# CROSS-DATASET & EMOTION TRAJECTORY
# =====================

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import os
import pickle
from tqdm import tqdm
from collections import defaultdict, deque
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

def train_agent_on_ABC_environments(agent_class, agent_kwargs, experiment_seed=42):
    """Train agent on EnvA, B, C sequentially without reset"""

    # Initialize agent with seed
    agent_kwargs_with_seed = agent_kwargs.copy()
    agent_kwargs_with_seed['random_state'] = experiment_seed + 1
    agent = agent_class(**agent_kwargs_with_seed)

    environments = [
        (EnvironmentA, "EnvA"),
        (EnvironmentB, "EnvB"),
        (EnvironmentC, "EnvC")
    ]

    all_rewards = []
    all_environments_performance = {}
    all_env_instances = []

    for env_class, env_name in environments:

        # Initialize environment
        env = env_class(random_state=experiment_seed + hash(env_name) % 1000)
        env.reset()

        env_rewards = []

        # Train for 200 trials
        for t in range(200):
            action = agent.select_action()
            reward, context = env.step(action)

            # Update agent (cumulative learning)
            if hasattr(agent, 'context'):  # ECIA family
                agent.update(context, action, reward)
            else:  # Baseline agents
                agent.update(action, reward)

            env_rewards.append(reward)

        all_rewards.extend(env_rewards)
        all_environments_performance[env_name] = {
            'rewards': np.array(env_rewards),
            'mean_performance': np.mean(env_rewards)
        }
        all_env_instances.append((env, env_name))

    return {
        'trained_agent': agent,
        'training_rewards': np.array(all_rewards),
        'environments_performance': all_environments_performance,
        'total_training_performance': np.mean(all_rewards),
        'env_instances': all_env_instances
    }

def test_agent_on_randomshift(trained_agent, experiment_seed=42, test_trials=100):
    """Test trained agent on RandomShiftEnvironment with unified metrics"""

    # Initialize test environment
    env = RandomShiftEnvironment(
        total_trials=test_trials,
        random_state=experiment_seed + 999
    )
    env.reset()

    test_rewards = []

    for t in range(test_trials):
        action = trained_agent.select_action()
        reward, context = env.step(action)

        # Continue learning in new environment
        if hasattr(trained_agent, 'context'):  # ECIA family
            trained_agent.update(context, action, reward)
        else:  # Baseline agents
            trained_agent.update(action, reward)

        test_rewards.append(reward)

    return {
        'test_rewards': np.array(test_rewards),
        'test_performance': np.mean(test_rewards),
        'environment_instance': env
    }

def calculate_cross_dataset_unified_metrics(training_result, test_result, env_name="RandomShift"):
    """Calculate unified metrics for cross-dataset results using single analysis methods"""

     # Get test rewards and environment instance
    test_rewards = test_result['test_rewards'].reshape(1, -1)  # Shape: (1, n_trials)
    env_instances = [test_result['environment_instance']] if test_result.get('environment_instance') else None

    try:
        # Recovery rate calculation
        recovery_rates = compute_unified_recovery_rate(test_rewards, env_name, env_instances)
        recovery_rate_mean = np.mean(recovery_rates) if len(recovery_rates) > 0 else 0.0
        recovery_rate_std = np.std(recovery_rates) if len(recovery_rates) > 0 else 0.0

        # Recovery time measurement
        recovery_times = measure_unified_recovery_time(test_rewards, env_name, env_instances)
        recovery_time_mean = np.mean(recovery_times) if len(recovery_times) > 0 else 0.0
        recovery_time_std = np.std(recovery_times) if len(recovery_times) > 0 else 0.0


    except Exception as e:
        print(f"Warning: Unified metrics calculation failed: {e}")
        recovery_rate_mean = recovery_rate_std = 0.0
        recovery_time_mean = recovery_time_std = 0.0

    return {
        'overall_performance': test_result['test_performance'],
        'recovery_rate_mean': recovery_rate_mean,
        'recovery_rate_std': recovery_rate_std,
        'recovery_time_mean': recovery_time_mean,
        'recovery_time_std': recovery_time_std
    }

def run_single_cross_dataset_experiment(agent_class, agent_kwargs, experiment_seed=42):
    """Run single complete cross-dataset experiment with unified metrics"""

    try:
        # Phase 1: Train on EnvA, B, C
        training_result = train_agent_on_ABC_environments(
            agent_class, agent_kwargs, experiment_seed
        )

        # Phase 2: Test on RandomShift (with continued learning)
        test_result = test_agent_on_randomshift(
            training_result['trained_agent'], experiment_seed, test_trials=600
        )

        # Phase 3: Calculate unified metrics
        unified_metrics = calculate_cross_dataset_unified_metrics(
            training_result, test_result, "RandomShift"
        )

        return {
            'training_result': training_result,
            'test_result': test_result,
            'unified_metrics': unified_metrics,
            'experiment_seed': experiment_seed,
            'success': True
        }

    except Exception as e:
        print(f"    Experiment failed: {e}")
        return {
            'training_result': None,
            'test_result': None,
            'unified_metrics': None,
            'experiment_seed': experiment_seed,
            'success': False
        }

def run_cross_dataset_meta_analysis_for_agent(agent_class, agent_kwargs, agent_name="Agent"):
    """Run cross-dataset meta-analysis for one agent with unified metrics"""

    # Cross-dataset specific seeds and runs
    cross_dataset_seeds = [34, 55, 89]
    n_runs_per_seed = 300

    print(f"🔄 Cross-Dataset Study: {agent_name}")

    meta_results = {}

    for master_seed in cross_dataset_seeds:
        print(f"  🌀 Master Seed {master_seed}: Running {n_runs_per_seed} experiments...")

        # Generate experiment seeds
        np.random.seed(master_seed)
        experiment_seeds = np.random.randint(0, 1000000, size=n_runs_per_seed)

        seed_results = []

        for experiment_seed in tqdm(experiment_seeds, desc=f"Seed-{master_seed}"):
            result = run_single_cross_dataset_experiment(
                agent_class, agent_kwargs, experiment_seed
            )

            if result['success']:
                seed_results.append(result)

        # Aggregate results for this master seed using unified metrics
        if seed_results:
            # Collect unified metrics
            overall_performances = [r['unified_metrics']['overall_performance'] for r in seed_results]
            recovery_rates = [r['unified_metrics']['recovery_rate_mean'] for r in seed_results]
            recovery_times = [r['unified_metrics']['recovery_time_mean'] for r in seed_results]

            # Traditional metrics for comparison
            training_performances = [r['training_result']['total_training_performance'] for r in seed_results]
            test_performances = [r['test_result']['test_performance'] for r in seed_results]

            meta_results[f"seed_{master_seed}"] = {
                'master_seed': master_seed,
                'n_experiments': len(seed_results),

                # Unified metrics (primary)
                'overall_performances': overall_performances,
                'recovery_rates': recovery_rates,
                'recovery_times': recovery_times,

                # Aggregated unified metrics
                'mean_overall_performance': np.mean(overall_performances),
                'std_overall_performance': np.std(overall_performances),
                'mean_recovery_rate': np.mean(recovery_rates),
                'std_recovery_rate': np.std(recovery_rates),
                'mean_recovery_time': np.mean(recovery_times),
                'std_recovery_time': np.std(recovery_times)
            }

        print(f"   Master Seed {master_seed}: {len(seed_results)} successful experiments")

    # Calculate overall meta-statistics using unified metrics
    all_overall_perfs = []
    all_recovery_rates = []
    all_recovery_times = []

    for seed_result in meta_results.values():
        # Unified metrics
        all_overall_perfs.extend(seed_result['overall_performances'])
        all_recovery_rates.extend(seed_result['recovery_rates'])
        all_recovery_times.extend(seed_result['recovery_times'])


    meta_statistics = {
        # Primary unified metrics
        'overall_performance_mean': np.mean(all_overall_perfs) if all_overall_perfs else 0,
        'overall_performance_std': np.std(all_overall_perfs) if all_overall_perfs else 0,
        'recovery_rate_mean': np.mean(all_recovery_rates) if all_recovery_rates else 0,
        'recovery_rate_std': np.std(all_recovery_rates) if all_recovery_rates else 0,
        'recovery_time_mean': np.mean(all_recovery_times) if all_recovery_times else 0,
        'recovery_time_std': np.std(all_recovery_times) if all_recovery_times else 0,

        # Experiment details
        'n_total_experiments': sum(r['n_experiments'] for r in meta_results.values()),
        'n_master_seeds': len(meta_results)
    }

    return {
        'individual_seeds': meta_results,
        'meta_statistics': meta_statistics,
        'agent_name': agent_name
    }

def run_simple_emotion_analysis():
    """Run simple emotion analysis for all environments"""

    print(" SIMPLE EMOTION TRAJECTORY ANALYSIS")
    print("=" * 50)
    print(" Focus: 8 emotion plots showing trial-by-trial average")
    print(" Agent: ECIA_Full only")
    print(" Environments: EnvA, EnvB, EnvC")
    print("=" * 50)

    environments = {
        "EnvA": EnvironmentA,
        "EnvB": EnvironmentB,
        "EnvC": EnvironmentC
    }

    save_path = "content/Results/simple_emotion_analysis"
    os.makedirs(save_path, exist_ok=True)

    for env_name, env_class in environments.items():
        print(f"\n Processing {env_name}...")

        # Collect emotion data
        emotion_data = collect_ecia_emotions_simple(
            env_class, n_trials=200, n_runs=50, env_name=env_name
        )

        # Create plots
        plot_emotion_trajectories_simple(emotion_data, env_name, save_path)

        # Save raw data
        np.save(f"{save_path}/emotion_data_{env_name}.npy", emotion_data)
        print(f" Data saved: {save_path}/emotion_data_{env_name}.npy")

    print(f"\n Simple emotion analysis completed!")
    print(f" Results saved in: {save_path}/")
    print(f" Generated files:")
    for env_name in environments.keys():
        print(f"   - emotion_trajectories_{env_name}.png")
        print(f"   - emotion_data_{env_name}.npy")

def run_complete_cross_dataset_study():
    """Run complete cross-dataset study for all agents with unified metrics"""

    print("=" * 80)
    print("CROSS-DATASET STUDY: EnvA,B,C → RandomShift (Unified Metrics)")
    print("3 Master Seeds × 300 Runs × 600 Test Trials")
    print("=" * 80)

    # Agent configurations
    optimized_params = {
        "n_actions": 5,
        "epsilon": 0.03,
        "eta": 0.55,
        "xi": 0.001,
        "memory_threshold": 0.015,
        "memory_influence": 0.3,
        "memory_similarity_threshold": 0.035,
        "top_k": 3,
        "alpha": 0.22,
        "window_size": 30,
        "memory_size": 15,
        "emotion_decay": 0.96,
        "min_eta": 0.095,
    }

    agents = {
        "ECIA_Full": (ECIA, optimized_params),
        "EpsilonGreedy": (EpsilonGreedyAgent, {"n_actions": 5, "epsilon": 0.1}),
        "UCB": (UCBAgent, {"n_actions": 5, "c": 0.5}),
        "TS": (ThompsonSamplingAgent, {"n_actions": 5})}

    complete_results = {}

    for agent_name, (agent_class, agent_kwargs) in agents.items():
        print(f"\n Agent: {agent_name}")

        result = run_cross_dataset_meta_analysis_for_agent(
            agent_class, agent_kwargs, agent_name
        )

        complete_results[agent_name] = result

        # Print summary with unified metrics
        meta_stats = result['meta_statistics']
        print(f"   Results Summary (Unified Metrics):")
        print(f"    Overall Performance: {meta_stats['overall_performance_mean']:.4f} ± {meta_stats['overall_performance_std']:.4f}")
        print(f"    Recovery Rate: {meta_stats['recovery_rate_mean']:.4f} ± {meta_stats['recovery_rate_std']:.4f}")
        print(f"    Recovery Time: {meta_stats['recovery_time_mean']:.2f} ± {meta_stats['recovery_time_std']:.2f} trials")
        print(f"    Total Experiments: {meta_stats['n_total_experiments']}")

    # Save results
    os.makedirs("content/Results/cross_dataset_study", exist_ok=True)
    with open("content/Results/cross_dataset_study/complete_cross_dataset_results_unified.pkl", "wb") as f:
        pickle.dump(complete_results, f)

    print(f"\n Complete Cross-Dataset Study finished!")
    print(f" Results saved in: content/Results/cross_dataset_study/")

    return complete_results



# =====================
# 10. MAIN EXECUTION FUNCTION
# =====================
# MENU SYSTEM
# =====================
def run_selected_analysis(choice):
    """Run selected analysis with enhanced CSV output only (no plots except emotions)"""

    if choice == 1:
        print("\n🔍 META-ANALYSIS STUDY (CSV ENHANCED)")
        print(" Statistical robustness evaluation on EnvA, EnvB, EnvC")
        print("=" * 60)

        try:
            results = run_complete_meta_analysis()

            if results:
                print("\n Creating Emotion Trajectory Plots Only...")
                try:
                    run_simple_emotion_analysis()
                    print("    Emotion plots created")
                except Exception as e:
                    print(f"    Emotion analysis failed: {e}")

                print("\n Creating Enhanced CSV Data...")
                try:
                    save_csv_data(results)
                    print("    Enhanced CSV data created")
                except Exception as e:
                    print(f"    Enhanced CSV creation failed: {e}")

                print("\n Performing Statistical Analysis...")
                try:
                    meta_comparisons = perform_meta_statistical_comparison(results)
                    create_statistical_comparison_csv(results, meta_comparisons)
                    perform_multiple_comparison_correction(results)
                    print("    Statistical analysis completed")
                except Exception as e:
                    print(f"    Statistical analysis failed: {e}")

                print("\n Generating Report...")
                generate_meta_analysis_report(results, meta_comparisons)

            print(f"\n META-ANALYSIS FINISHED!")

        except Exception as e:
            print(f" meta-analysis failed: {e}")
            import traceback
            traceback.print_exc()

    elif choice == 2:
        print("\n CROSS-DATASET STUDY (CSV ENHANCED)")
        print(" Adaptation evaluation: EnvA,B,C → RandomShift")
        print("=" * 60)

        try:
            cross_dataset_results = run_complete_cross_dataset_study()

            if cross_dataset_results:
                print("\n Creating Enhanced CSV Data...")
                try:
                    save_enhanced_cross_dataset_csv_data(cross_dataset_results)
                    print("    Enhanced CSV data created")
                except Exception as e:
                    print(f"    Enhanced CSV creation failed: {e}")

                print("\n Performing Statistical Analysis...")
                try:
                    perform_multiple_comparison_correction_cross_dataset(cross_dataset_results)
                    print("    Statistical analysis completed")
                except Exception as e:
                    print(f"    Statistical analysis failed: {e}")

                print("\n Generating Report...")
                generate_cross_dataset_unified_report(cross_dataset_results)

            print(f"\n CROSS-DATASET STUDY FINISHED!")

        except Exception as e:
            print(f" Cross-dataset study failed: {e}")
            import traceback
            traceback.print_exc()

    elif choice == 3:
        print("\n COMPLETE COMPREHENSIVE STUDY (CSV ENHANCED)")
        print(" Running both studies with enhanced CSV output")
        print("=" * 80)

        # Run both studies
        run_selected_analysis(1)  # EnvA-C
        run_selected_analysis(2)  # Cross-dataset

        print(f"\n COMPLETE COMPREHENSIVE STUDY FINISHED!")

# Updated main execution
def main_analysis_menu():
    """Main menu for CSV-enhanced analysis (no plots except emotions)"""

    print(" COMPREHENSIVE ECIA EVALUATION SYSTEM (CSV ENHANCED)")
    print("=" * 60)
    print("Features:")
    print("• Enhanced CSV output with 95% CI, SEM, STD")
    print("• Statistical comparison tables")
    print("• Emotion trajectory plots only")
    print("• Comprehensive reports")
    print("=" * 60)
    print("Select analysis to run:")
    print("1. Meta-Analysis (EnvA, B, C)")
    print("2. Cross-Dataset Study (A,B,C → RandomShift)")
    print("3. Both studies (Complete Evaluation)")
    print("=" * 60)

    while True:
        try:
            choice = int(input("Enter choice (1/2/3): ").strip())
            if choice in [1, 2, 3]:
                return choice
            else:
                print("Invalid choice. Please enter 1, 2, or 3.")
        except ValueError:
            print("Invalid input. Please enter a number.")

def create_combined_analysis_report(results, transfer_results,
                                  save_path="content/Results/combined_analysis"):
    """Create combined analysis report"""

    os.makedirs(save_path, exist_ok=True)

    report_lines = []
    report_lines.append("COMBINED META-ANALYSIS & CROSS-DATASET TRANSFER REPORT")
    report_lines.append("=" * 80)
    report_lines.append("")

    report_lines.append("STUDY OVERVIEW")
    report_lines.append("-" * 30)
    report_lines.append("This report combines results from two comprehensive studies:")
    report_lines.append("1. Meta-Analysis: Statistical robustness evaluation")
    report_lines.append("2. Cross-Dataset Transfer: Generalization capability assessment")
    report_lines.append("")

    # ECIA Performance Summary
    report_lines.append("ECIA PERFORMANCE SUMMARY")
    report_lines.append("-" * 35)

    # Extract ECIA_Full performance from both studies
    report_lines.append("ECIA_Full Performance Across Studies:")

    # results
    if results:
        report_lines.append("\nMeta-Analysis Results:")
        for env_name, env_results in results.items():
            if 'ECIA_Full' in env_results:
                meta_stats = env_results['ECIA_Full']['meta_statistics']
                report_lines.append(f"  • {env_name}: {meta_stats['meta_mean']:.4f} ± {meta_stats['meta_std']:.4f}")

    # Transfer results
    if transfer_results:
        report_lines.append("\nCross-Dataset Transfer Results:")
        for config_name, config_results in transfer_results.items():
            if 'ECIA_Full' in config_results:
                meta_stats = config_results['ECIA_Full']['meta_statistics']
                report_lines.append(f"  • {config_name}: {meta_stats['overall_performance_mean']:.4f} ± {meta_stats['overall_performance_std']:.4f}")

    report_lines.append("")

    # Key Findings
    report_lines.append("KEY FINDINGS")
    report_lines.append("-" * 20)
    report_lines.append("• ECIA demonstrates superior performance in controlled environments")
    report_lines.append("• Cross-dataset adaptation capabilities vary by environment characteristics")
    report_lines.append("• Emotion trajectory analysis reveals adaptive decision-making patterns")
    report_lines.append("• Memory and emotion systems contribute significantly to robustness")

    # Save combined report
    with open(f"{save_path}/combined_analysis_report.txt", "w") as f:
        f.write("\n".join(report_lines))

    print(f" Combined analysis report saved: {save_path}/combined_analysis_report.txt")


def generate_meta_analysis_report(complete_meta_results, meta_comparisons,
                                           save_path="content/Results/meta_analysis"):
    """Generate comprehensive report of meta-analysis"""

    report_lines = []
    report_lines.append("BASED META-ANALYSIS REPORT")
    report_lines.append("=" * 60)
    report_lines.append(f"Master Seeds Used: {MANAGER.SEEDS}")
    report_lines.append(f"Runs per Master Seed: {MANAGER.N_RUNS_PER_SEED}")
    report_lines.append(f"Total Experiments per Agent: {MANAGER.TOTAL_EXPERIMENTS}")
    report_lines.append("")

    # Summary for each environment
    for env_name, env_results in complete_meta_results.items():
        report_lines.append(f"ENVIRONMENT: {env_name}")
        report_lines.append("-" * 40)

        # Sort agents by meta-mean performance
        agent_performance = []
        for agent_name, agent_result in env_results.items():
            meta_stats = agent_result['meta_statistics']
            if meta_stats['n_master_seeds'] > 0:
                agent_performance.append((agent_name, meta_stats['meta_mean'], meta_stats))

        agent_performance.sort(key=lambda x: x[1], reverse=True)

        # Report rankings
        for rank, (agent_name, meta_mean, meta_stats) in enumerate(agent_performance, 1):
            agent_type = "BASELINE" if agent_name in ["EpsilonGreedy", "UCB"] else "ECIA"
            report_lines.append(
                f"  {rank:2d}. [{agent_type:8s}] {agent_name:15s} | "
                f"Meta-Mean: {meta_stats['meta_mean']:.4f} ± {meta_stats['meta_std']:.4f} | "
                f"95% CI: [{meta_stats['ci_lower']:.4f}, {meta_stats['ci_upper']:.4f}]"
            )

        report_lines.append("")

        # Statistical comparisons
        if env_name in meta_comparisons:
            significant_comparisons = [
                (comp_name, comp_data) for comp_name, comp_data in meta_comparisons[env_name].items()
                if comp_data['significant']
            ]

            if significant_comparisons:
                report_lines.append("  Statistically Significant Differences:")
                for comp_name, comp_data in significant_comparisons:
                    agent1, agent2 = comp_name.split('_vs_')
                    direction = ">" if comp_data['mean_diff'] > 0 else "<"
                    report_lines.append(
                        f"    {agent1} {direction} {agent2}: p={comp_data['p_value']:.4f}, "
                        f"d={comp_data['cohens_d']:.3f} ({comp_data['effect_size_interpretation']})"
                    )
            else:
                report_lines.append("  No statistically significant differences found.")

        report_lines.append("")

    # Methodology section
    report_lines.append("METHODOLOGY")
    report_lines.append("-" * 40)
    report_lines.append("• sequence master seeds ensure natural variation")
    report_lines.append("• Each master seed generates 800 deterministic experiment seeds")
    report_lines.append("• Meta-analysis aggregates results across 8 master seeds")
    report_lines.append("• Statistical testing with appropriate parametric/non-parametric tests")
    report_lines.append("• Effect sizes calculated using Cohen's d")
    report_lines.append("• 95% confidence intervals using t-distribution")

    # Save report
    with open(f"{save_path}/meta_analysis_report.txt", "w") as f:
        f.write("\n".join(report_lines))

    print(f" meta-analysis report saved: {save_path}/meta_analysis_report.txt")

    return "\n".join(report_lines)


# =====================
# 8. COMPREHENSIVE ANALYSIS FUNCTIONS (INTEGRATED)
# =====================

def get_environment_change_points(env_name, env_instance=None):
    """Get change points for each environment"""
    if env_name == "EnvA":
        return [100]
    elif env_name == "EnvB":
        return [40, 80, 120, 160]
    elif env_name == "EnvC":
        if env_instance and hasattr(env_instance, 'change_points'):
            return env_instance.change_points
        else:
            return []
    else:
        return []

def get_environment_optimal_rewards(env_name, env_instance=None):
    """Get optimal rewards for each segment"""
    if env_name == "EnvA":
        return [0.8, 0.9]
    elif env_name == "EnvB":
        return [0.95, 0.95, 0.95, 0.95, 0.95]
    elif env_name == "EnvC":
        if env_instance and hasattr(env_instance, 'optimal_rewards'):
            return env_instance.optimal_rewards
        else:
            return []
    else:
        return []

def calculate_noise_adjusted_parameters(env_name):
    """Calculate parameters adjusted for noise level"""
    noise_levels = {
        "EnvA": {"sigma": 0.15, "noise_level": "medium"},
        "EnvB": {"sigma": 0.05, "noise_level": "low"},
        "EnvC": {"sigma": 0.15, "noise_level": "medium"}
    }

    noise_info = noise_levels.get(env_name, {"noise_level": "medium"})

    if noise_info["noise_level"] == "low":
        threshold_ratio = 0.90
        stability_window = 3
        min_stability_trials = 2
    elif noise_info["noise_level"] == "medium":
        threshold_ratio = 0.85
        stability_window = 5
        min_stability_trials = 3
    else:
        threshold_ratio = 0.80
        stability_window = 7
        min_stability_trials = 4

    return {
        "threshold_ratio": threshold_ratio,
        "stability_window": stability_window,
        "min_stability_trials": min_stability_trials
    }



if __name__ == "__main__":
    choice = main_analysis_menu()
    run_selected_analysis(choice)
